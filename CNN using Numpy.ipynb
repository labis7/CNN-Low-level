{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron, MNIST\n",
    "---\n",
    "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data\n",
    "2. Define a neural network\n",
    "3. Train the model\n",
    "4. Evaluate the performance of our trained model on a test dataset!\n",
    "\n",
    "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Architecture ######\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 800, \"output_dim\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 128, \"output_dim\": 10, \"activation\": \"sigmoid\"} #Or relu again like the original example\n",
    "]#No Dropout...yet\n",
    "\n",
    "\n",
    "######  Init Layers  ######\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "\n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(image, params, s): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] #5x5\n",
    "    f_num = f.shape[0]\n",
    "    \n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] * f[z, :, :, :]) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "\n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X,filter_params ,params_values, nn_architecture):\n",
    "    \n",
    "    ######################## Forward Propagation Convolution Part  ##########################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    \n",
    "    params = [f1, b1]\n",
    "    conv1 = conv(X, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "    conv1[conv1<=0] = 0 #Relu\n",
    "    \n",
    "    params = [f2, b2]\n",
    "    conv2 = conv(conv1, params, 1)\n",
    "    conv2[conv2<=0] = 0 #Relu\n",
    "    \n",
    "    pl = maxpool(conv2, 2, 2) #pool_f = 2 , pool_s = 2\n",
    "    \n",
    "    #packet\n",
    "    conv_mem = [X, conv1, conv2, pl]\n",
    "    \n",
    "    num_c, f_dim, _ = pl.shape\n",
    "    fc1 = pl.reshape(num_c*f_dim*f_dim, 1) #Flattening\n",
    "\n",
    "    \n",
    "    ######################## Forward Propagation FC Part  ##########################\n",
    "    \n",
    "    \n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = fc1\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, conv_mem, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = []\n",
    "    #print(len(targets))\n",
    "    for i in range(len(targets)):\n",
    "        out.append(logs[i][targets[i]])\n",
    "    out = np.array(out)\n",
    "    \n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  BACK PROPAGATION  #######\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    \n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr \n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture):\n",
    "    \n",
    "    \n",
    "    ################# Backwardpropagation for FC Part  #######################\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    #print(Y.shape)\n",
    "    m = Y.shape[1]     # 1 sample each time\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        \n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    ################# Backwardpropagation for Conv Part  #######################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    [image_in, conv1, conv2, pl]= conv_mem\n",
    "    #dA_prev\n",
    "    #Find dimensions of pooled image\n",
    "    #dim = int(np.sqrt(dA_prev.shape(0)/f2.shape(0))) #sqrt(800/8)=10 ==> 8*10*10\n",
    "    dpool = dA_prev.reshape(pl.shape) #, 1) \n",
    "    dconv2 = maxpoolBackward(dpool, conv2)  # , pool_f, pool_s)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    conv_s = 1\n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) #\n",
    "    dconv1[conv1<=0] = 0\n",
    "    \n",
    "    _, df1, db1 = convolutionBackward(dconv1, image_in, f1, conv_s)\n",
    "    \n",
    "    conv_grads = [df1, df2, db1, db2] \n",
    "    \n",
    "    return conv_grads, grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE ######\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    ####### Building index labels from One-Hot matrix #######\n",
    "    temp = []\n",
    "    #train_labels = train_labels.sum(1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if(Y[i][j]==1):\n",
    "                temp.append(j)\n",
    "    #########################################################\n",
    "    \n",
    "    ##filter params\n",
    "    num_f1 = 8\n",
    "    num_f2 = 8\n",
    "    f_dim = 5\n",
    "    f1 = (num_f1, 1, f_dim, f_dim )\n",
    "    f2 = (num_f2, num_f1, f_dim, f_dim )\n",
    "    #To make for a smoother training process, we initialize each filter with a mean of 0 and a standard deviation of 1\n",
    "    scale = 1.0\n",
    "    stddev = scale/np.sqrt(np.prod(f1))\n",
    "    f1 = np.random.normal(loc = 0, scale = stddev, size = f1)\n",
    "    stddev = scale/np.sqrt(np.prod(f2))\n",
    "    f2 = np.random.normal(loc = 0, scale = stddev, size = f2)\n",
    "    b1 = np.zeros((f1.shape[0],1))\n",
    "    b2 = np.zeros((f2.shape[0],1))\n",
    "    #Packing Conv params\n",
    "    filter_params = []\n",
    "    filter_params = (f1, f2, b1, b2)\n",
    "\n",
    "    \n",
    "    \n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 6000\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            \n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            \n",
    "            df1 = np.zeros(f1.shape)\n",
    "            df2 = np.zeros(f2.shape)\n",
    "            db1 = np.zeros(b1.shape)\n",
    "            db2 = np.zeros(b2.shape)\n",
    "            dW1 = np.zeros(params_values['W1'].shape)\n",
    "            dW2 = np.zeros(params_values['W2'].shape)\n",
    "            dwb1 = np.zeros(params_values['b1'].shape)\n",
    "            dwb2 = np.zeros(params_values['b2'].shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y = Y[c:(c + batch)]\n",
    "            Ys  = temp[c:(c + batch)] #shape (m,1), NOT one-hot\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            for i in range(batch):\n",
    "                \n",
    "                \n",
    "                # 1 image per time...for now\n",
    "                Y_hat, conv_mem, cashe = full_forward_propagation(X[i], filter_params, params_values, nn_architecture)\n",
    "            \n",
    "            \n",
    "                Ys = (np.array(temp[i])).reshape(1,1) #just 1 label        \n",
    "                Yh = np.array(Y_hat.T)\n",
    "\n",
    "                ############### LogSoftMax  #################\n",
    "                #x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "                num = np.exp(Yh)\n",
    "                den = np.sum(np.exp(Yh), axis = 1)\n",
    "\n",
    "                for i in range(Yh.shape[0]): #1  , just 1 per time\n",
    "                    #for j in range(Yh.shape[1]): #10\n",
    "                    Yh[i][:] = np.log(num[i][:] / den[i])  \n",
    "                #############################################\n",
    "            \n",
    "                #print(Yh.shape)\n",
    "                #print(len(Ys))                \n",
    "                cost += NLLLoss(Yh, np.array(Ys))  #(we need to send image(samples,classes), labels(NOT in one-hot) - Future optimizations needed)\n",
    "                #print(\"Cost: {%f}\" %(cost))\n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y[i])\n",
    "\n",
    "                conv_grads, grads_values = full_backward_propagation(Y_hat, Y[i].reshape(Y[i].shape[0],1), conv_mem, cashe, filter_params, params_values, nn_architecture)\n",
    "                [df1_, df2_, db1_, db2_] = conv_grads\n",
    "\n",
    "                df1 += df1_\n",
    "                df2 += df2_\n",
    "                db1 += db1_\n",
    "                db2 += db2_\n",
    "\n",
    "                dW1 +=  grads_values['dW1']\n",
    "                dW2 +=  grads_values['dW2']\n",
    "                dwb1 += grads_values['db1']\n",
    "                dwb2 += grads_values['db2']\n",
    "                \n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            f1 -= learning_rate * (df1/batch)\n",
    "            f2 -= learning_rate * (df2/batch)\n",
    "            b1 -= learning_rate * (db1/batch)\n",
    "            b2 -= learning_rate * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1/batch\n",
    "            grads_values['dW2'] = dW2/batch\n",
    "            grads_values['db1'] = dwb1/batch\n",
    "            grads_values['db2'] = dwb2/batch\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            print(\"Accuracy : {:.2f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.5f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1, 5, 5)\n",
      "(8, 1)\n",
      "(8, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-b79b7af080e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_ARCHITECTURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-172-3b619e89a6f9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate, verbose, callback)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mget_accuracy_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mconv_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_backward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcashe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mdf1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-168-e3008ab42c39>\u001b[0m in \u001b[0;36mfull_backward_propagation\u001b[1;34m(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mconv_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mdconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutionBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_s\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mdconv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-165-6d7435cddcf8>\u001b[0m in \u001b[0;36mconvolutionBackward\u001b[1;34m(dconv_prev, conv_in, filt, s)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_w\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m#each entry of the dconv_prev will try to affect the idxs from which was made of.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mdfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mdconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#, axis =1) ## AXIS?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, NN_ARCHITECTURE, 2, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.33000%\n"
     ]
    }
   ],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
