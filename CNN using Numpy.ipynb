{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron, MNIST\n",
    "---\n",
    "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data\n",
    "2. Define a neural network\n",
    "3. Train the model\n",
    "4. Evaluate the performance of our trained model on a test dataset!\n",
    "\n",
    "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Architecture ######\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 800, \"output_dim\": 512, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 512, \"output_dim\": 10, \"activation\": \"sigmoid\"} #Or relu again like the original example\n",
    "]#No Dropout...yet\n",
    "\n",
    "\n",
    "######  Init Layers  ######\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "\n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(image, params, s): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] #5x5\n",
    "    f_num = f.shape[0]\n",
    "    \n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] * f[z, :, :, :]) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "\n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X,filter_params ,params_values, nn_architecture, dropout):\n",
    "    \n",
    "    ######################## Forward Propagation Convolution Part  ##########################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    \n",
    "    params = [f1, b1]\n",
    "    conv1 = conv(X, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "    conv1[conv1<=0] = 0 #Relu\n",
    "    \n",
    "    params = [f2, b2]\n",
    "    conv2 = conv(conv1, params, 1)\n",
    "    conv2[conv2<=0] = 0 #Relu\n",
    "    \n",
    "    pl = maxpool(conv2, 2, 2) #pool_f = 2 , pool_s = 2\n",
    "    \n",
    "    #packet\n",
    "    conv_mem = [X, conv1, conv2, pl]\n",
    "    \n",
    "    num_c, f_dim, _ = pl.shape\n",
    "    fc1 = pl.reshape(num_c*f_dim*f_dim, 1) #Flattening\n",
    "\n",
    "    \n",
    "    ######################## Forward Propagation FC Part  ##########################\n",
    "    \n",
    "    \n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0 \n",
    "    A_curr = fc1\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        if ((layer_idx == 1)and(dropout)):\n",
    "                ## Dropout ##\n",
    "                d = (np.random.rand(W_curr.shape[0],W_curr.shape[1])<0.5)\n",
    "                d = d*1 #Bool --> int(0s and 1s)\n",
    "                W_curr = d*W_curr\n",
    "                #############\n",
    "            \n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, conv_mem, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = []\n",
    "    #print(len(targets))\n",
    "    for i in range(len(targets)):\n",
    "        out.append(logs[i][targets[i]])\n",
    "    out = np.array(out)\n",
    "    \n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  BACK PROPAGATION  #######\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # calculation of the activation function derivative\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    \n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr \n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture):\n",
    "    \n",
    "    \n",
    "    ################# Backwardpropagation for FC Part  #######################\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    #print(Y.shape)\n",
    "    m = Y.shape[1]     # 1 sample each time\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = Y_hat - Y#- (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        \n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    ################# Backwardpropagation for Conv Part  #######################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    [image_in, conv1, conv2, pl]= conv_mem\n",
    "    #dA_prev\n",
    "    #Find dimensions of pooled image\n",
    "    #dim = int(np.sqrt(dA_prev.shape(0)/f2.shape(0))) #sqrt(800/8)=10 ==> 8*10*10\n",
    "    dpool = dA_prev.reshape(pl.shape) #, 1) \n",
    "    dconv2 = maxpoolBackward(dpool, conv2)  # , pool_f, pool_s)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    conv_s = 1\n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) #\n",
    "    dconv1[conv1<=0] = 0\n",
    "    \n",
    "    _, df1, db1 = convolutionBackward(dconv1, image_in, f1, conv_s)\n",
    "    \n",
    "    conv_grads = [df1, df2, db1, db2] \n",
    "    \n",
    "    return conv_grads, grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE ######\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    \n",
    "    ####### Building index labels from One-Hot matrix #######\n",
    "    temp = []\n",
    "    #train_labels = train_labels.sum(1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if(Y[i][j]==1):\n",
    "                temp.append(j)\n",
    "    #########################################################\n",
    "    \n",
    "    ##filter params\n",
    "    num_f1 = 8\n",
    "    num_f2 = 8\n",
    "    f_dim = 5\n",
    "    f1 = (num_f1, 1, f_dim, f_dim )\n",
    "    f2 = (num_f2, num_f1, f_dim, f_dim )\n",
    "    #To make for a smoother training process, we initialize each filter with a mean of 0 and a standard deviation of 1\n",
    "    scale = 1.0\n",
    "    #stddev = scale/np.sqrt(np.prod(f1))\n",
    "    trim = 0.1\n",
    "    f1 = np.random.randn( num_f1, 1, f_dim, f_dim) *trim\n",
    "    stddev = scale/np.sqrt(np.prod(f2))\n",
    "    f2 = np.random.randn(num_f2, num_f1, f_dim, f_dim ) *trim\n",
    "    b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "    b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "    #Packing Conv params\n",
    "    filter_params = []\n",
    "    filter_params = (f1, f2, b1, b2)\n",
    "\n",
    "    \n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 12\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            \n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            \n",
    "            df1 = np.zeros(f1.shape)\n",
    "            df2 = np.zeros(f2.shape)\n",
    "            db1 = np.zeros(b1.shape)\n",
    "            db2 = np.zeros(b2.shape)\n",
    "            dW1 = np.zeros(params_values['W1'].shape)\n",
    "            dW2 = np.zeros(params_values['W2'].shape)\n",
    "            dB1 = np.zeros(params_values['b1'].shape)\n",
    "            dB2 = np.zeros(params_values['b2'].shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            Ys  = np.array(temp[c:(c + batch)]).reshape(batch,1) #shape (m,1), NOT one-hot\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                \n",
    "                # 1 image per time...for now\n",
    "                Y_hat, conv_mem, cashe = full_forward_propagation(X_t[b], filter_params, params_values, nn_architecture, dropout)\n",
    "            \n",
    "            \n",
    "                       \n",
    "                Yh = np.array(Y_hat.T)\n",
    "\n",
    "                ############### LogSoftMax  #################\n",
    "                #x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "                num = np.exp(Yh)\n",
    "                den = np.sum(np.exp(Yh), axis = 1)\n",
    "\n",
    "                for i in range(Yh.shape[0]): #1  , just 1 per time\n",
    "                    #for j in range(Yh.shape[1]): #10\n",
    "                    Yh[i][:] = np.log(num[i][:] / den[i])  \n",
    "                #############################################\n",
    "            \n",
    "                #print(Yh.shape)\n",
    "                #print(len(Ys))                \n",
    "                cost += NLLLoss(Yh, np.array(Ys[b]))  #(we need to send image(samples,classes), labels(NOT in one-hot) - Future optimizations needed)\n",
    "                #print(\"Cost: {%f}\" %(cost))\n",
    "                \n",
    "                \n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b].reshape(10,1))\n",
    "                \n",
    "                conv_grads, grads_values = full_backward_propagation(Y_hat, Y_t[b].reshape(Y_t[b].shape[0],1), conv_mem, cashe, filter_params, params_values, nn_architecture)\n",
    "                [df1_, df2_, db1_, db2_] = conv_grads\n",
    "\n",
    "                df1 += df1_\n",
    "                df2 += df2_\n",
    "                db1 += db1_\n",
    "                db2 += db2_\n",
    "\n",
    "                dW1 +=  grads_values['dW1']\n",
    "                dW2 +=  grads_values['dW2']\n",
    "                dB1 += grads_values['db1']\n",
    "                dB2 += grads_values['db2']\n",
    "                \n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            print(c)\n",
    "            \n",
    "\n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "0\n",
      "Cost : 2.33194\n",
      "Accuracy : 0.00000%\n",
      "12\n",
      "Cost : 2.31712\n",
      "Accuracy : 0.00000%\n",
      "24\n",
      "Cost : 2.30183\n",
      "Accuracy : 0.00000%\n",
      "36\n",
      "Cost : 2.30091\n",
      "Accuracy : 0.00000%\n",
      "48\n",
      "Cost : 2.28860\n",
      "Accuracy : 0.00000%\n",
      "60\n",
      "Cost : 2.26492\n",
      "Accuracy : 0.00000%\n",
      "72\n",
      "Cost : 2.28164\n",
      "Accuracy : 0.00000%\n",
      "84\n",
      "Cost : 2.30385\n",
      "Accuracy : 0.00000%\n",
      "96\n",
      "Cost : 2.24933\n",
      "Accuracy : 0.00000%\n",
      "108\n",
      "Cost : 2.27358\n",
      "Accuracy : 0.00000%\n",
      "120\n",
      "Cost : 2.26518\n",
      "Accuracy : 8.33333%\n",
      "132\n",
      "Cost : 2.29471\n",
      "Accuracy : 0.00000%\n",
      "144\n",
      "Cost : 2.29439\n",
      "Accuracy : 0.00000%\n",
      "156\n",
      "Cost : 2.28338\n",
      "Accuracy : 0.00000%\n",
      "168\n",
      "Cost : 2.25080\n",
      "Accuracy : 0.00000%\n",
      "180\n",
      "Cost : 2.28764\n",
      "Accuracy : 0.00000%\n",
      "192\n",
      "Cost : 2.24895\n",
      "Accuracy : 0.00000%\n",
      "204\n",
      "Cost : 2.18171\n",
      "Accuracy : 8.33333%\n",
      "216\n",
      "Cost : 2.29403\n",
      "Accuracy : 0.00000%\n",
      "228\n",
      "Cost : 2.22957\n",
      "Accuracy : 8.33333%\n",
      "240\n",
      "Cost : 2.22705\n",
      "Accuracy : 8.33333%\n",
      "252\n",
      "Cost : 2.20576\n",
      "Accuracy : 8.33333%\n",
      "264\n",
      "Cost : 2.24490\n",
      "Accuracy : 8.33333%\n",
      "276\n",
      "Cost : 2.22385\n",
      "Accuracy : 8.33333%\n",
      "288\n",
      "Cost : 2.21398\n",
      "Accuracy : 8.33333%\n",
      "300\n",
      "Cost : 2.22582\n",
      "Accuracy : 8.33333%\n",
      "312\n",
      "Cost : 2.20439\n",
      "Accuracy : 8.33333%\n",
      "324\n",
      "Cost : 2.23682\n",
      "Accuracy : 0.00000%\n",
      "336\n",
      "Cost : 2.19876\n",
      "Accuracy : 0.00000%\n",
      "348\n",
      "Cost : 2.16373\n",
      "Accuracy : 8.33333%\n",
      "360\n",
      "Cost : 2.20164\n",
      "Accuracy : 8.33333%\n",
      "372\n",
      "Cost : 2.14733\n",
      "Accuracy : 8.33333%\n",
      "384\n",
      "Cost : 2.22390\n",
      "Accuracy : 0.00000%\n",
      "396\n",
      "Cost : 2.20054\n",
      "Accuracy : 8.33333%\n",
      "408\n",
      "Cost : 2.17894\n",
      "Accuracy : 16.66667%\n",
      "420\n",
      "Cost : 2.22457\n",
      "Accuracy : 0.00000%\n",
      "432\n",
      "Cost : 2.16310\n",
      "Accuracy : 8.33333%\n",
      "444\n",
      "Cost : 2.15971\n",
      "Accuracy : 33.33333%\n",
      "456\n",
      "Cost : 2.13978\n",
      "Accuracy : 16.66667%\n",
      "468\n",
      "Cost : 2.20208\n",
      "Accuracy : 8.33333%\n",
      "480\n",
      "Cost : 2.23621\n",
      "Accuracy : 8.33333%\n",
      "492\n",
      "Cost : 2.27559\n",
      "Accuracy : 8.33333%\n",
      "504\n",
      "Cost : 2.16385\n",
      "Accuracy : 25.00000%\n",
      "516\n",
      "Cost : 2.13414\n",
      "Accuracy : 16.66667%\n",
      "528\n",
      "Cost : 2.24163\n",
      "Accuracy : 8.33333%\n",
      "540\n",
      "Cost : 2.26661\n",
      "Accuracy : 0.00000%\n",
      "552\n",
      "Cost : 2.15321\n",
      "Accuracy : 25.00000%\n",
      "564\n",
      "Cost : 2.19033\n",
      "Accuracy : 16.66667%\n",
      "576\n",
      "Cost : 2.19549\n",
      "Accuracy : 0.00000%\n",
      "588\n",
      "Cost : 2.20252\n",
      "Accuracy : 8.33333%\n",
      "600\n",
      "Cost : 2.19323\n",
      "Accuracy : 16.66667%\n",
      "612\n",
      "Cost : 2.21881\n",
      "Accuracy : 0.00000%\n",
      "624\n",
      "Cost : 2.23700\n",
      "Accuracy : 0.00000%\n",
      "636\n",
      "Cost : 2.12774\n",
      "Accuracy : 8.33333%\n",
      "648\n",
      "Cost : 2.15657\n",
      "Accuracy : 25.00000%\n",
      "660\n",
      "Cost : 2.19848\n",
      "Accuracy : 8.33333%\n",
      "672\n",
      "Cost : 2.14095\n",
      "Accuracy : 25.00000%\n",
      "684\n",
      "Cost : 2.17181\n",
      "Accuracy : 16.66667%\n",
      "696\n",
      "Cost : 2.15245\n",
      "Accuracy : 8.33333%\n",
      "708\n",
      "Cost : 2.12820\n",
      "Accuracy : 25.00000%\n",
      "720\n",
      "Cost : 2.21564\n",
      "Accuracy : 8.33333%\n",
      "732\n",
      "Cost : 2.20765\n",
      "Accuracy : 8.33333%\n",
      "744\n",
      "Cost : 2.16823\n",
      "Accuracy : 16.66667%\n",
      "756\n",
      "Cost : 2.23436\n",
      "Accuracy : 8.33333%\n",
      "768\n",
      "Cost : 2.18382\n",
      "Accuracy : 16.66667%\n",
      "780\n",
      "Cost : 2.09465\n",
      "Accuracy : 33.33333%\n",
      "792\n",
      "Cost : 2.14713\n",
      "Accuracy : 8.33333%\n",
      "804\n",
      "Cost : 2.16687\n",
      "Accuracy : 8.33333%\n",
      "816\n",
      "Cost : 2.08463\n",
      "Accuracy : 25.00000%\n",
      "828\n",
      "Cost : 2.09368\n",
      "Accuracy : 33.33333%\n",
      "840\n",
      "Cost : 2.20716\n",
      "Accuracy : 8.33333%\n",
      "852\n",
      "Cost : 2.17173\n",
      "Accuracy : 8.33333%\n",
      "864\n",
      "Cost : 2.09860\n",
      "Accuracy : 16.66667%\n",
      "876\n",
      "Cost : 2.18138\n",
      "Accuracy : 16.66667%\n",
      "888\n",
      "Cost : 2.08128\n",
      "Accuracy : 33.33333%\n",
      "900\n",
      "Cost : 2.06241\n",
      "Accuracy : 41.66667%\n",
      "912\n",
      "Cost : 2.11668\n",
      "Accuracy : 33.33333%\n",
      "924\n",
      "Cost : 2.13755\n",
      "Accuracy : 25.00000%\n",
      "936\n",
      "Cost : 2.13860\n",
      "Accuracy : 16.66667%\n",
      "948\n",
      "Cost : 2.06237\n",
      "Accuracy : 25.00000%\n",
      "960\n",
      "Cost : 2.00472\n",
      "Accuracy : 50.00000%\n",
      "972\n",
      "Cost : 2.15593\n",
      "Accuracy : 8.33333%\n",
      "984\n",
      "Cost : 2.06861\n",
      "Accuracy : 25.00000%\n",
      "996\n",
      "Cost : 2.09798\n",
      "Accuracy : 16.66667%\n",
      "1008\n",
      "Cost : 2.07218\n",
      "Accuracy : 16.66667%\n",
      "1020\n",
      "Cost : 2.11085\n",
      "Accuracy : 25.00000%\n",
      "1032\n",
      "Cost : 2.10862\n",
      "Accuracy : 25.00000%\n",
      "1044\n",
      "Cost : 2.02400\n",
      "Accuracy : 41.66667%\n",
      "1056\n",
      "Cost : 2.16822\n",
      "Accuracy : 16.66667%\n",
      "1068\n",
      "Cost : 2.10323\n",
      "Accuracy : 25.00000%\n",
      "1080\n",
      "Cost : 2.01148\n",
      "Accuracy : 50.00000%\n",
      "1092\n",
      "Cost : 2.09084\n",
      "Accuracy : 25.00000%\n",
      "1104\n",
      "Cost : 2.10990\n",
      "Accuracy : 25.00000%\n",
      "1116\n",
      "Cost : 2.10545\n",
      "Accuracy : 33.33333%\n",
      "1128\n",
      "Cost : 2.15687\n",
      "Accuracy : 16.66667%\n",
      "1140\n",
      "Cost : 2.14602\n",
      "Accuracy : 8.33333%\n",
      "1152\n",
      "Cost : 2.03251\n",
      "Accuracy : 33.33333%\n",
      "1164\n",
      "Cost : 2.05944\n",
      "Accuracy : 50.00000%\n",
      "1176\n",
      "Cost : 2.04162\n",
      "Accuracy : 25.00000%\n",
      "1188\n",
      "Cost : 2.06055\n",
      "Accuracy : 25.00000%\n",
      "1200\n",
      "Cost : 2.01823\n",
      "Accuracy : 33.33333%\n",
      "1212\n",
      "Cost : 2.13230\n",
      "Accuracy : 25.00000%\n",
      "1224\n",
      "Cost : 2.01715\n",
      "Accuracy : 33.33333%\n",
      "1236\n",
      "Cost : 2.16970\n",
      "Accuracy : 16.66667%\n",
      "1248\n",
      "Cost : 1.98127\n",
      "Accuracy : 33.33333%\n",
      "1260\n",
      "Cost : 2.15571\n",
      "Accuracy : 16.66667%\n",
      "1272\n",
      "Cost : 2.10327\n",
      "Accuracy : 33.33333%\n",
      "1284\n",
      "Cost : 2.12779\n",
      "Accuracy : 16.66667%\n",
      "1296\n",
      "Cost : 2.07251\n",
      "Accuracy : 16.66667%\n",
      "1308\n",
      "Cost : 2.11140\n",
      "Accuracy : 8.33333%\n",
      "1320\n",
      "Cost : 1.98679\n",
      "Accuracy : 50.00000%\n",
      "1332\n",
      "Cost : 2.07531\n",
      "Accuracy : 16.66667%\n",
      "1344\n",
      "Cost : 2.06140\n",
      "Accuracy : 25.00000%\n",
      "1356\n",
      "Cost : 2.16549\n",
      "Accuracy : 8.33333%\n",
      "1368\n",
      "Cost : 1.96030\n",
      "Accuracy : 33.33333%\n",
      "1380\n",
      "Cost : 2.08260\n",
      "Accuracy : 16.66667%\n",
      "1392\n",
      "Cost : 1.95297\n",
      "Accuracy : 50.00000%\n",
      "1404\n",
      "Cost : 2.03353\n",
      "Accuracy : 58.33333%\n",
      "1416\n",
      "Cost : 1.95867\n",
      "Accuracy : 50.00000%\n",
      "1428\n",
      "Cost : 2.06944\n",
      "Accuracy : 25.00000%\n",
      "1440\n",
      "Cost : 1.90201\n",
      "Accuracy : 58.33333%\n",
      "1452\n",
      "Cost : 2.02765\n",
      "Accuracy : 33.33333%\n",
      "1464\n",
      "Cost : 2.05605\n",
      "Accuracy : 33.33333%\n",
      "1476\n",
      "Cost : 2.01799\n",
      "Accuracy : 41.66667%\n",
      "1488\n",
      "Cost : 1.97157\n",
      "Accuracy : 41.66667%\n",
      "1500\n",
      "Cost : 1.98829\n",
      "Accuracy : 50.00000%\n",
      "1512\n",
      "Cost : 2.00703\n",
      "Accuracy : 50.00000%\n",
      "1524\n",
      "Cost : 1.99391\n",
      "Accuracy : 33.33333%\n",
      "1536\n",
      "Cost : 2.08050\n",
      "Accuracy : 33.33333%\n",
      "1548\n",
      "Cost : 2.07194\n",
      "Accuracy : 25.00000%\n",
      "1560\n",
      "Cost : 1.90367\n",
      "Accuracy : 66.66667%\n",
      "1572\n",
      "Cost : 2.02619\n",
      "Accuracy : 33.33333%\n",
      "1584\n",
      "Cost : 1.99372\n",
      "Accuracy : 33.33333%\n",
      "1596\n",
      "Cost : 1.91160\n",
      "Accuracy : 58.33333%\n",
      "1608\n",
      "Cost : 1.98502\n",
      "Accuracy : 50.00000%\n",
      "1620\n",
      "Cost : 1.93290\n",
      "Accuracy : 58.33333%\n",
      "1632\n",
      "Cost : 1.95797\n",
      "Accuracy : 41.66667%\n",
      "1644\n",
      "Cost : 1.83840\n",
      "Accuracy : 66.66667%\n",
      "1656\n",
      "Cost : 1.95944\n",
      "Accuracy : 50.00000%\n",
      "1668\n",
      "Cost : 1.85961\n",
      "Accuracy : 66.66667%\n",
      "1680\n",
      "Cost : 1.93916\n",
      "Accuracy : 50.00000%\n",
      "1692\n",
      "Cost : 1.80060\n",
      "Accuracy : 75.00000%\n",
      "1704\n",
      "Cost : 1.94039\n",
      "Accuracy : 58.33333%\n",
      "1716\n",
      "Cost : 1.86942\n",
      "Accuracy : 58.33333%\n",
      "1728\n",
      "Cost : 1.80693\n",
      "Accuracy : 83.33333%\n",
      "1740\n",
      "Cost : 1.98747\n",
      "Accuracy : 41.66667%\n",
      "1752\n",
      "Cost : 2.02457\n",
      "Accuracy : 33.33333%\n",
      "1764\n",
      "Cost : 1.84692\n",
      "Accuracy : 75.00000%\n",
      "1776\n",
      "Cost : 2.07625\n",
      "Accuracy : 33.33333%\n",
      "1788\n",
      "Cost : 1.97052\n",
      "Accuracy : 50.00000%\n",
      "1800\n",
      "Cost : 1.92692\n",
      "Accuracy : 50.00000%\n",
      "1812\n",
      "Cost : 2.05370\n",
      "Accuracy : 33.33333%\n",
      "1824\n",
      "Cost : 1.92304\n",
      "Accuracy : 50.00000%\n",
      "1836\n",
      "Cost : 1.91687\n",
      "Accuracy : 58.33333%\n",
      "1848\n",
      "Cost : 1.93254\n",
      "Accuracy : 50.00000%\n",
      "1860\n",
      "Cost : 1.96886\n",
      "Accuracy : 41.66667%\n",
      "1872\n",
      "Cost : 2.00937\n",
      "Accuracy : 41.66667%\n",
      "1884\n",
      "Cost : 2.01425\n",
      "Accuracy : 25.00000%\n",
      "1896\n",
      "Cost : 1.95675\n",
      "Accuracy : 50.00000%\n",
      "1908\n",
      "Cost : 1.97648\n",
      "Accuracy : 50.00000%\n",
      "1920\n",
      "Cost : 1.96702\n",
      "Accuracy : 41.66667%\n",
      "1932\n",
      "Cost : 1.97283\n",
      "Accuracy : 50.00000%\n",
      "1944\n",
      "Cost : 2.04989\n",
      "Accuracy : 33.33333%\n",
      "1956\n",
      "Cost : 2.06095\n",
      "Accuracy : 33.33333%\n",
      "1968\n",
      "Cost : 2.07973\n",
      "Accuracy : 25.00000%\n",
      "1980\n",
      "Cost : 1.99678\n",
      "Accuracy : 33.33333%\n",
      "1992\n",
      "Cost : 2.10232\n",
      "Accuracy : 16.66667%\n",
      "2004\n",
      "Cost : 1.98317\n",
      "Accuracy : 58.33333%\n",
      "2016\n",
      "Cost : 2.01545\n",
      "Accuracy : 25.00000%\n",
      "2028\n",
      "Cost : 2.05189\n",
      "Accuracy : 25.00000%\n",
      "2040\n",
      "Cost : 1.99279\n",
      "Accuracy : 41.66667%\n",
      "2052\n",
      "Cost : 1.97931\n",
      "Accuracy : 33.33333%\n",
      "2064\n",
      "Cost : 2.15979\n",
      "Accuracy : 16.66667%\n",
      "2076\n",
      "Cost : 1.94152\n",
      "Accuracy : 50.00000%\n",
      "2088\n",
      "Cost : 1.95746\n",
      "Accuracy : 41.66667%\n",
      "2100\n",
      "Cost : 1.88482\n",
      "Accuracy : 50.00000%\n",
      "2112\n",
      "Cost : 1.88298\n",
      "Accuracy : 50.00000%\n",
      "2124\n",
      "Cost : 2.00489\n",
      "Accuracy : 41.66667%\n",
      "2136\n",
      "Cost : 1.92924\n",
      "Accuracy : 50.00000%\n",
      "2148\n",
      "Cost : 1.84247\n",
      "Accuracy : 75.00000%\n",
      "2160\n",
      "Cost : 1.89763\n",
      "Accuracy : 58.33333%\n",
      "2172\n",
      "Cost : 1.83701\n",
      "Accuracy : 58.33333%\n",
      "2184\n",
      "Cost : 1.98942\n",
      "Accuracy : 58.33333%\n",
      "2196\n",
      "Cost : 1.95107\n",
      "Accuracy : 50.00000%\n",
      "2208\n",
      "Cost : 2.08177\n",
      "Accuracy : 25.00000%\n",
      "2220\n",
      "Cost : 1.83257\n",
      "Accuracy : 75.00000%\n",
      "2232\n",
      "Cost : 1.87413\n",
      "Accuracy : 58.33333%\n",
      "2244\n",
      "Cost : 1.94660\n",
      "Accuracy : 50.00000%\n",
      "2256\n",
      "Cost : 1.91881\n",
      "Accuracy : 50.00000%\n",
      "2268\n",
      "Cost : 1.98892\n",
      "Accuracy : 50.00000%\n",
      "2280\n",
      "Cost : 2.03880\n",
      "Accuracy : 16.66667%\n",
      "2292\n",
      "Cost : 1.88108\n",
      "Accuracy : 66.66667%\n",
      "2304\n",
      "Cost : 1.98268\n",
      "Accuracy : 41.66667%\n",
      "2316\n",
      "Cost : 1.99184\n",
      "Accuracy : 41.66667%\n",
      "2328\n",
      "Cost : 1.96908\n",
      "Accuracy : 41.66667%\n",
      "2340\n",
      "Cost : 2.03345\n",
      "Accuracy : 33.33333%\n",
      "2352\n",
      "Cost : 2.01766\n",
      "Accuracy : 41.66667%\n",
      "2364\n",
      "Cost : 2.01783\n",
      "Accuracy : 25.00000%\n",
      "2376\n",
      "Cost : 2.11426\n",
      "Accuracy : 25.00000%\n",
      "2388\n",
      "Cost : 1.94995\n",
      "Accuracy : 41.66667%\n",
      "2400\n",
      "Cost : 2.02889\n",
      "Accuracy : 33.33333%\n",
      "2412\n",
      "Cost : 2.05976\n",
      "Accuracy : 33.33333%\n",
      "2424\n",
      "Cost : 2.03049\n",
      "Accuracy : 33.33333%\n",
      "2436\n",
      "Cost : 2.03378\n",
      "Accuracy : 33.33333%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2448\n",
      "Cost : 2.02653\n",
      "Accuracy : 25.00000%\n",
      "2460\n",
      "Cost : 1.93662\n",
      "Accuracy : 33.33333%\n",
      "2472\n",
      "Cost : 1.97468\n",
      "Accuracy : 50.00000%\n",
      "2484\n",
      "Cost : 1.87226\n",
      "Accuracy : 66.66667%\n",
      "2496\n",
      "Cost : 1.85995\n",
      "Accuracy : 66.66667%\n",
      "2508\n",
      "Cost : 1.96327\n",
      "Accuracy : 41.66667%\n",
      "2520\n",
      "Cost : 1.90053\n",
      "Accuracy : 66.66667%\n",
      "2532\n",
      "Cost : 1.88664\n",
      "Accuracy : 41.66667%\n",
      "2544\n",
      "Cost : 2.14152\n",
      "Accuracy : 25.00000%\n",
      "2556\n",
      "Cost : 1.95513\n",
      "Accuracy : 50.00000%\n",
      "2568\n",
      "Cost : 1.93596\n",
      "Accuracy : 50.00000%\n",
      "2580\n",
      "Cost : 1.88369\n",
      "Accuracy : 58.33333%\n",
      "2592\n",
      "Cost : 1.92467\n",
      "Accuracy : 50.00000%\n",
      "2604\n",
      "Cost : 1.91836\n",
      "Accuracy : 50.00000%\n",
      "2616\n",
      "Cost : 1.86715\n",
      "Accuracy : 75.00000%\n",
      "2628\n",
      "Cost : 1.84034\n",
      "Accuracy : 66.66667%\n",
      "2640\n",
      "Cost : 1.83350\n",
      "Accuracy : 66.66667%\n",
      "2652\n",
      "Cost : 1.89933\n",
      "Accuracy : 75.00000%\n",
      "2664\n",
      "Cost : 1.92966\n",
      "Accuracy : 41.66667%\n",
      "2676\n",
      "Cost : 2.06111\n",
      "Accuracy : 25.00000%\n",
      "2688\n",
      "Cost : 1.92810\n",
      "Accuracy : 58.33333%\n",
      "2700\n",
      "Cost : 1.84986\n",
      "Accuracy : 58.33333%\n",
      "2712\n",
      "Cost : 2.00372\n",
      "Accuracy : 41.66667%\n",
      "2724\n",
      "Cost : 1.93902\n",
      "Accuracy : 41.66667%\n",
      "2736\n",
      "Cost : 1.92667\n",
      "Accuracy : 58.33333%\n",
      "2748\n",
      "Cost : 1.95425\n",
      "Accuracy : 58.33333%\n",
      "2760\n",
      "Cost : 2.08300\n",
      "Accuracy : 33.33333%\n",
      "2772\n",
      "Cost : 1.94685\n",
      "Accuracy : 41.66667%\n",
      "2784\n",
      "Cost : 1.87116\n",
      "Accuracy : 66.66667%\n",
      "2796\n",
      "Cost : 2.02511\n",
      "Accuracy : 41.66667%\n",
      "2808\n",
      "Cost : 1.93363\n",
      "Accuracy : 50.00000%\n",
      "2820\n",
      "Cost : 1.87352\n",
      "Accuracy : 50.00000%\n",
      "2832\n",
      "Cost : 2.00303\n",
      "Accuracy : 33.33333%\n",
      "2844\n",
      "Cost : 1.87446\n",
      "Accuracy : 66.66667%\n",
      "2856\n",
      "Cost : 1.91540\n",
      "Accuracy : 58.33333%\n",
      "2868\n",
      "Cost : 1.81572\n",
      "Accuracy : 66.66667%\n",
      "2880\n",
      "Cost : 1.88592\n",
      "Accuracy : 58.33333%\n",
      "2892\n",
      "Cost : 2.00571\n",
      "Accuracy : 50.00000%\n",
      "2904\n",
      "Cost : 1.94606\n",
      "Accuracy : 50.00000%\n",
      "2916\n",
      "Cost : 1.94456\n",
      "Accuracy : 58.33333%\n",
      "2928\n",
      "Cost : 1.93425\n",
      "Accuracy : 66.66667%\n",
      "2940\n",
      "Cost : 1.99167\n",
      "Accuracy : 41.66667%\n",
      "2952\n",
      "Cost : 2.01873\n",
      "Accuracy : 41.66667%\n",
      "2964\n",
      "Cost : 1.97055\n",
      "Accuracy : 50.00000%\n",
      "2976\n",
      "Cost : 1.82151\n",
      "Accuracy : 66.66667%\n",
      "2988\n",
      "Cost : 1.87676\n",
      "Accuracy : 58.33333%\n",
      "3000\n",
      "Cost : 1.90125\n",
      "Accuracy : 58.33333%\n",
      "3012\n",
      "Cost : 1.94194\n",
      "Accuracy : 41.66667%\n",
      "3024\n",
      "Cost : 1.98949\n",
      "Accuracy : 33.33333%\n",
      "3036\n",
      "Cost : 2.00171\n",
      "Accuracy : 33.33333%\n",
      "3048\n",
      "Cost : 1.98670\n",
      "Accuracy : 58.33333%\n",
      "3060\n",
      "Cost : 2.01816\n",
      "Accuracy : 50.00000%\n",
      "3072\n",
      "Cost : 2.00172\n",
      "Accuracy : 50.00000%\n",
      "3084\n",
      "Cost : 1.97477\n",
      "Accuracy : 33.33333%\n",
      "3096\n",
      "Cost : 1.98436\n",
      "Accuracy : 25.00000%\n",
      "3108\n",
      "Cost : 1.92160\n",
      "Accuracy : 50.00000%\n",
      "3120\n",
      "Cost : 1.87020\n",
      "Accuracy : 75.00000%\n",
      "3132\n",
      "Cost : 1.82786\n",
      "Accuracy : 66.66667%\n",
      "3144\n",
      "Cost : 1.96106\n",
      "Accuracy : 41.66667%\n",
      "3156\n",
      "Cost : 1.89963\n",
      "Accuracy : 58.33333%\n",
      "3168\n",
      "Cost : 1.89447\n",
      "Accuracy : 50.00000%\n",
      "3180\n",
      "Cost : 1.89497\n",
      "Accuracy : 41.66667%\n",
      "3192\n",
      "Cost : 1.94535\n",
      "Accuracy : 41.66667%\n",
      "3204\n",
      "Cost : 1.82543\n",
      "Accuracy : 75.00000%\n",
      "3216\n",
      "Cost : 2.00726\n",
      "Accuracy : 33.33333%\n",
      "3228\n",
      "Cost : 1.79154\n",
      "Accuracy : 75.00000%\n",
      "3240\n",
      "Cost : 1.81409\n",
      "Accuracy : 58.33333%\n",
      "3252\n",
      "Cost : 1.83130\n",
      "Accuracy : 75.00000%\n",
      "3264\n",
      "Cost : 1.88633\n",
      "Accuracy : 50.00000%\n",
      "3276\n",
      "Cost : 1.78989\n",
      "Accuracy : 66.66667%\n",
      "3288\n",
      "Cost : 1.93861\n",
      "Accuracy : 41.66667%\n",
      "3300\n",
      "Cost : 1.80958\n",
      "Accuracy : 66.66667%\n",
      "3312\n",
      "Cost : 1.93541\n",
      "Accuracy : 58.33333%\n",
      "3324\n",
      "Cost : 1.87119\n",
      "Accuracy : 58.33333%\n",
      "3336\n",
      "Cost : 1.79056\n",
      "Accuracy : 75.00000%\n",
      "3348\n",
      "Cost : 2.08489\n",
      "Accuracy : 16.66667%\n",
      "3360\n",
      "Cost : 1.91601\n",
      "Accuracy : 50.00000%\n",
      "3372\n",
      "Cost : 1.92422\n",
      "Accuracy : 50.00000%\n",
      "3384\n",
      "Cost : 1.86052\n",
      "Accuracy : 58.33333%\n",
      "3396\n",
      "Cost : 1.88107\n",
      "Accuracy : 66.66667%\n",
      "3408\n",
      "Cost : 1.92079\n",
      "Accuracy : 50.00000%\n",
      "3420\n",
      "Cost : 1.73703\n",
      "Accuracy : 75.00000%\n",
      "3432\n",
      "Cost : 1.84654\n",
      "Accuracy : 58.33333%\n",
      "3444\n",
      "Cost : 1.76309\n",
      "Accuracy : 83.33333%\n",
      "3456\n",
      "Cost : 1.80588\n",
      "Accuracy : 66.66667%\n",
      "3468\n",
      "Cost : 1.72418\n",
      "Accuracy : 75.00000%\n",
      "3480\n",
      "Cost : 1.77414\n",
      "Accuracy : 83.33333%\n",
      "3492\n",
      "Cost : 1.98968\n",
      "Accuracy : 33.33333%\n",
      "3504\n",
      "Cost : 1.94056\n",
      "Accuracy : 41.66667%\n",
      "3516\n",
      "Cost : 1.75699\n",
      "Accuracy : 83.33333%\n",
      "3528\n",
      "Cost : 1.79563\n",
      "Accuracy : 83.33333%\n",
      "3540\n",
      "Cost : 1.88924\n",
      "Accuracy : 41.66667%\n",
      "3552\n",
      "Cost : 1.84888\n",
      "Accuracy : 58.33333%\n",
      "3564\n",
      "Cost : 1.78592\n",
      "Accuracy : 66.66667%\n",
      "3576\n",
      "Cost : 1.87982\n",
      "Accuracy : 50.00000%\n",
      "3588\n",
      "Cost : 1.88360\n",
      "Accuracy : 50.00000%\n",
      "3600\n",
      "Cost : 1.77986\n",
      "Accuracy : 75.00000%\n",
      "3612\n",
      "Cost : 1.72122\n",
      "Accuracy : 75.00000%\n",
      "3624\n",
      "Cost : 1.92550\n",
      "Accuracy : 41.66667%\n",
      "3636\n",
      "Cost : 1.86416\n",
      "Accuracy : 50.00000%\n",
      "3648\n",
      "Cost : 1.68579\n",
      "Accuracy : 83.33333%\n",
      "3660\n",
      "Cost : 1.84621\n",
      "Accuracy : 58.33333%\n",
      "3672\n",
      "Cost : 1.83403\n",
      "Accuracy : 75.00000%\n",
      "3684\n",
      "Cost : 1.98929\n",
      "Accuracy : 33.33333%\n",
      "3696\n",
      "Cost : 1.75835\n",
      "Accuracy : 66.66667%\n",
      "3708\n",
      "Cost : 1.85730\n",
      "Accuracy : 66.66667%\n",
      "3720\n",
      "Cost : 1.79750\n",
      "Accuracy : 75.00000%\n",
      "3732\n",
      "Cost : 1.80455\n",
      "Accuracy : 66.66667%\n",
      "3744\n",
      "Cost : 1.84893\n",
      "Accuracy : 41.66667%\n",
      "3756\n",
      "Cost : 1.95146\n",
      "Accuracy : 33.33333%\n",
      "3768\n",
      "Cost : 1.81434\n",
      "Accuracy : 66.66667%\n",
      "3780\n",
      "Cost : 1.77234\n",
      "Accuracy : 75.00000%\n",
      "3792\n",
      "Cost : 1.97053\n",
      "Accuracy : 50.00000%\n",
      "3804\n",
      "Cost : 1.90474\n",
      "Accuracy : 50.00000%\n",
      "3816\n",
      "Cost : 1.88820\n",
      "Accuracy : 58.33333%\n",
      "3828\n",
      "Cost : 1.86575\n",
      "Accuracy : 66.66667%\n",
      "3840\n",
      "Cost : 1.83388\n",
      "Accuracy : 50.00000%\n",
      "3852\n",
      "Cost : 1.87015\n",
      "Accuracy : 50.00000%\n",
      "3864\n",
      "Cost : 2.03087\n",
      "Accuracy : 25.00000%\n",
      "3876\n",
      "Cost : 1.91506\n",
      "Accuracy : 50.00000%\n",
      "3888\n",
      "Cost : 1.72052\n",
      "Accuracy : 91.66667%\n",
      "3900\n",
      "Cost : 1.80672\n",
      "Accuracy : 75.00000%\n",
      "3912\n",
      "Cost : 1.96306\n",
      "Accuracy : 41.66667%\n",
      "3924\n",
      "Cost : 1.86713\n",
      "Accuracy : 66.66667%\n",
      "3936\n",
      "Cost : 1.96291\n",
      "Accuracy : 25.00000%\n",
      "3948\n",
      "Cost : 1.91531\n",
      "Accuracy : 58.33333%\n",
      "3960\n",
      "Cost : 1.90124\n",
      "Accuracy : 66.66667%\n",
      "3972\n",
      "Cost : 1.98466\n",
      "Accuracy : 41.66667%\n",
      "3984\n",
      "Cost : 1.67146\n",
      "Accuracy : 83.33333%\n",
      "3996\n",
      "Cost : 1.82959\n",
      "Accuracy : 58.33333%\n",
      "4008\n",
      "Cost : 1.83215\n",
      "Accuracy : 58.33333%\n",
      "4020\n",
      "Cost : 1.78239\n",
      "Accuracy : 66.66667%\n",
      "4032\n",
      "Cost : 1.81725\n",
      "Accuracy : 66.66667%\n",
      "4044\n",
      "Cost : 1.78844\n",
      "Accuracy : 75.00000%\n",
      "4056\n",
      "Cost : 1.92885\n",
      "Accuracy : 41.66667%\n",
      "4068\n",
      "Cost : 1.79523\n",
      "Accuracy : 66.66667%\n",
      "4080\n",
      "Cost : 1.82617\n",
      "Accuracy : 66.66667%\n",
      "4092\n",
      "Cost : 1.74304\n",
      "Accuracy : 75.00000%\n",
      "4104\n",
      "Cost : 1.89092\n",
      "Accuracy : 41.66667%\n",
      "4116\n",
      "Cost : 1.81309\n",
      "Accuracy : 58.33333%\n",
      "4128\n",
      "Cost : 1.87099\n",
      "Accuracy : 50.00000%\n",
      "4140\n",
      "Cost : 1.85225\n",
      "Accuracy : 58.33333%\n",
      "4152\n",
      "Cost : 1.80688\n",
      "Accuracy : 66.66667%\n",
      "4164\n",
      "Cost : 1.77978\n",
      "Accuracy : 58.33333%\n",
      "4176\n",
      "Cost : 1.90221\n",
      "Accuracy : 41.66667%\n",
      "4188\n",
      "Cost : 1.53736\n",
      "Accuracy : 83.33333%\n",
      "4200\n",
      "Cost : 1.77277\n",
      "Accuracy : 75.00000%\n",
      "4212\n",
      "Cost : 1.71994\n",
      "Accuracy : 66.66667%\n",
      "4224\n",
      "Cost : 1.78619\n",
      "Accuracy : 58.33333%\n",
      "4236\n",
      "Cost : 1.65402\n",
      "Accuracy : 83.33333%\n",
      "4248\n",
      "Cost : 1.73964\n",
      "Accuracy : 75.00000%\n",
      "4260\n",
      "Cost : 1.76409\n",
      "Accuracy : 66.66667%\n",
      "4272\n",
      "Cost : 1.99485\n",
      "Accuracy : 41.66667%\n",
      "4284\n",
      "Cost : 1.76153\n",
      "Accuracy : 75.00000%\n",
      "4296\n",
      "Cost : 1.75323\n",
      "Accuracy : 75.00000%\n",
      "4308\n",
      "Cost : 1.91764\n",
      "Accuracy : 50.00000%\n",
      "4320\n",
      "Cost : 1.85397\n",
      "Accuracy : 58.33333%\n",
      "4332\n",
      "Cost : 1.88165\n",
      "Accuracy : 58.33333%\n",
      "4344\n",
      "Cost : 1.74681\n",
      "Accuracy : 66.66667%\n",
      "4356\n",
      "Cost : 1.76471\n",
      "Accuracy : 75.00000%\n",
      "4368\n",
      "Cost : 1.96163\n",
      "Accuracy : 50.00000%\n",
      "4380\n",
      "Cost : 1.90449\n",
      "Accuracy : 41.66667%\n",
      "4392\n",
      "Cost : 1.86232\n",
      "Accuracy : 50.00000%\n",
      "4404\n",
      "Cost : 1.76826\n",
      "Accuracy : 75.00000%\n",
      "4416\n",
      "Cost : 1.77887\n",
      "Accuracy : 83.33333%\n",
      "4428\n",
      "Cost : 1.77533\n",
      "Accuracy : 66.66667%\n",
      "4440\n",
      "Cost : 1.70931\n",
      "Accuracy : 75.00000%\n",
      "4452\n",
      "Cost : 1.84615\n",
      "Accuracy : 58.33333%\n",
      "4464\n",
      "Cost : 1.71001\n",
      "Accuracy : 83.33333%\n",
      "4476\n",
      "Cost : 1.76888\n",
      "Accuracy : 66.66667%\n",
      "4488\n",
      "Cost : 1.80397\n",
      "Accuracy : 58.33333%\n",
      "4500\n",
      "Cost : 1.84737\n",
      "Accuracy : 50.00000%\n",
      "4512\n",
      "Cost : 1.80845\n",
      "Accuracy : 58.33333%\n",
      "4524\n",
      "Cost : 1.77451\n",
      "Accuracy : 66.66667%\n",
      "4536\n",
      "Cost : 1.83183\n",
      "Accuracy : 50.00000%\n",
      "4548\n",
      "Cost : 1.67259\n",
      "Accuracy : 75.00000%\n",
      "4560\n",
      "Cost : 1.77133\n",
      "Accuracy : 66.66667%\n",
      "4572\n",
      "Cost : 1.68216\n",
      "Accuracy : 91.66667%\n",
      "4584\n",
      "Cost : 1.68544\n",
      "Accuracy : 83.33333%\n",
      "4596\n",
      "Cost : 1.82464\n",
      "Accuracy : 58.33333%\n",
      "4608\n",
      "Cost : 1.63633\n",
      "Accuracy : 83.33333%\n",
      "4620\n",
      "Cost : 1.63553\n",
      "Accuracy : 91.66667%\n",
      "4632\n",
      "Cost : 1.81459\n",
      "Accuracy : 66.66667%\n",
      "4644\n",
      "Cost : 1.83234\n",
      "Accuracy : 58.33333%\n",
      "4656\n",
      "Cost : 1.83314\n",
      "Accuracy : 66.66667%\n",
      "4668\n",
      "Cost : 1.76130\n",
      "Accuracy : 50.00000%\n",
      "4680\n",
      "Cost : 1.93473\n",
      "Accuracy : 50.00000%\n",
      "4692\n",
      "Cost : 1.84449\n",
      "Accuracy : 58.33333%\n",
      "4704\n",
      "Cost : 1.76735\n",
      "Accuracy : 66.66667%\n",
      "4716\n",
      "Cost : 1.88135\n",
      "Accuracy : 58.33333%\n",
      "4728\n",
      "Cost : 1.68980\n",
      "Accuracy : 83.33333%\n",
      "4740\n",
      "Cost : 1.74853\n",
      "Accuracy : 66.66667%\n",
      "4752\n",
      "Cost : 1.89671\n",
      "Accuracy : 50.00000%\n",
      "4764\n",
      "Cost : 1.79652\n",
      "Accuracy : 58.33333%\n",
      "4776\n",
      "Cost : 1.79547\n",
      "Accuracy : 66.66667%\n",
      "4788\n",
      "Cost : 1.73353\n",
      "Accuracy : 75.00000%\n",
      "4800\n",
      "Cost : 1.76741\n",
      "Accuracy : 75.00000%\n",
      "4812\n",
      "Cost : 1.83364\n",
      "Accuracy : 58.33333%\n",
      "4824\n",
      "Cost : 1.84798\n",
      "Accuracy : 66.66667%\n",
      "4836\n",
      "Cost : 1.83558\n",
      "Accuracy : 41.66667%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4848\n",
      "Cost : 1.82503\n",
      "Accuracy : 58.33333%\n",
      "4860\n",
      "Cost : 1.72900\n",
      "Accuracy : 75.00000%\n",
      "4872\n",
      "Cost : 1.71206\n",
      "Accuracy : 83.33333%\n",
      "4884\n",
      "Cost : 1.73144\n",
      "Accuracy : 75.00000%\n",
      "4896\n",
      "Cost : 1.66758\n",
      "Accuracy : 91.66667%\n",
      "4908\n",
      "Cost : 1.81331\n",
      "Accuracy : 66.66667%\n",
      "4920\n",
      "Cost : 1.66478\n",
      "Accuracy : 83.33333%\n",
      "4932\n",
      "Cost : 1.78501\n",
      "Accuracy : 58.33333%\n",
      "4944\n",
      "Cost : 1.78172\n",
      "Accuracy : 58.33333%\n",
      "4956\n",
      "Cost : 1.84054\n",
      "Accuracy : 66.66667%\n",
      "4968\n",
      "Cost : 1.75377\n",
      "Accuracy : 75.00000%\n",
      "4980\n",
      "Cost : 1.77204\n",
      "Accuracy : 75.00000%\n",
      "4992\n",
      "Cost : 1.78741\n",
      "Accuracy : 66.66667%\n",
      "5004\n",
      "Cost : 1.73328\n",
      "Accuracy : 75.00000%\n",
      "5016\n",
      "Cost : 1.69641\n",
      "Accuracy : 75.00000%\n",
      "5028\n",
      "Cost : 1.93555\n",
      "Accuracy : 33.33333%\n",
      "5040\n",
      "Cost : 1.83865\n",
      "Accuracy : 58.33333%\n",
      "5052\n",
      "Cost : 1.83707\n",
      "Accuracy : 50.00000%\n",
      "5064\n",
      "Cost : 1.76971\n",
      "Accuracy : 58.33333%\n",
      "5076\n",
      "Cost : 1.91231\n",
      "Accuracy : 58.33333%\n",
      "5088\n",
      "Cost : 1.68355\n",
      "Accuracy : 66.66667%\n",
      "5100\n",
      "Cost : 1.68760\n",
      "Accuracy : 83.33333%\n",
      "5112\n",
      "Cost : 1.56928\n",
      "Accuracy : 91.66667%\n",
      "5124\n",
      "Cost : 1.73000\n",
      "Accuracy : 66.66667%\n",
      "5136\n",
      "Cost : 1.75162\n",
      "Accuracy : 50.00000%\n",
      "5148\n",
      "Cost : 1.93294\n",
      "Accuracy : 41.66667%\n",
      "5160\n",
      "Cost : 1.95848\n",
      "Accuracy : 41.66667%\n",
      "5172\n",
      "Cost : 1.79060\n",
      "Accuracy : 58.33333%\n",
      "5184\n",
      "Cost : 1.79880\n",
      "Accuracy : 58.33333%\n",
      "5196\n",
      "Cost : 1.82064\n",
      "Accuracy : 50.00000%\n",
      "5208\n",
      "Cost : 1.85486\n",
      "Accuracy : 58.33333%\n",
      "5220\n",
      "Cost : 1.76478\n",
      "Accuracy : 66.66667%\n",
      "5232\n",
      "Cost : 1.88540\n",
      "Accuracy : 50.00000%\n",
      "5244\n",
      "Cost : 1.77986\n",
      "Accuracy : 58.33333%\n",
      "5256\n",
      "Cost : 1.72605\n",
      "Accuracy : 75.00000%\n",
      "5268\n",
      "Cost : 1.66530\n",
      "Accuracy : 75.00000%\n",
      "5280\n",
      "Cost : 1.81667\n",
      "Accuracy : 50.00000%\n",
      "5292\n",
      "Cost : 1.92106\n",
      "Accuracy : 50.00000%\n",
      "5304\n",
      "Cost : 1.84609\n",
      "Accuracy : 50.00000%\n",
      "5316\n",
      "Cost : 1.70703\n",
      "Accuracy : 75.00000%\n",
      "5328\n",
      "Cost : 1.70828\n",
      "Accuracy : 75.00000%\n",
      "5340\n",
      "Cost : 1.67744\n",
      "Accuracy : 75.00000%\n",
      "5352\n",
      "Cost : 1.65146\n",
      "Accuracy : 83.33333%\n",
      "5364\n",
      "Cost : 1.74798\n",
      "Accuracy : 66.66667%\n",
      "5376\n",
      "Cost : 1.72226\n",
      "Accuracy : 66.66667%\n",
      "5388\n",
      "Cost : 1.72243\n",
      "Accuracy : 66.66667%\n",
      "5400\n",
      "Cost : 1.73167\n",
      "Accuracy : 75.00000%\n",
      "5412\n",
      "Cost : 1.73379\n",
      "Accuracy : 66.66667%\n",
      "5424\n",
      "Cost : 1.73962\n",
      "Accuracy : 66.66667%\n",
      "5436\n",
      "Cost : 1.72188\n",
      "Accuracy : 66.66667%\n",
      "5448\n",
      "Cost : 1.81307\n",
      "Accuracy : 50.00000%\n",
      "5460\n",
      "Cost : 1.71271\n",
      "Accuracy : 66.66667%\n",
      "5472\n",
      "Cost : 1.70843\n",
      "Accuracy : 66.66667%\n",
      "5484\n",
      "Cost : 1.55386\n",
      "Accuracy : 91.66667%\n",
      "5496\n",
      "Cost : 1.66895\n",
      "Accuracy : 75.00000%\n",
      "5508\n",
      "Cost : 1.62574\n",
      "Accuracy : 75.00000%\n",
      "5520\n",
      "Cost : 1.65097\n",
      "Accuracy : 83.33333%\n",
      "5532\n",
      "Cost : 1.87478\n",
      "Accuracy : 58.33333%\n",
      "5544\n",
      "Cost : 1.74634\n",
      "Accuracy : 66.66667%\n",
      "5556\n",
      "Cost : 1.73969\n",
      "Accuracy : 66.66667%\n",
      "5568\n",
      "Cost : 1.76685\n",
      "Accuracy : 50.00000%\n",
      "5580\n",
      "Cost : 1.63461\n",
      "Accuracy : 83.33333%\n",
      "5592\n",
      "Cost : 1.82567\n",
      "Accuracy : 58.33333%\n",
      "5604\n",
      "Cost : 1.76266\n",
      "Accuracy : 75.00000%\n",
      "5616\n",
      "Cost : 1.85741\n",
      "Accuracy : 58.33333%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-74fe5753bead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_ARCHITECTURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.05 stable LR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-165-bddd2212742c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose, callback)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m# 1 image per time...for now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-ea3937c6eb2d>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[1;34m(X, filter_params, params_values, nn_architecture, dropout)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#conv1 shape = (num_channels, h, w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mconv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m#Relu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-6d7435cddcf8>\u001b[0m in \u001b[0;36mconv\u001b[1;34m(image, params, s)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_h\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_w\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mnp_o\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp_o\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\deep-learning\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1778\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1779\u001b[0m     \"\"\"\n\u001b[0;32m   1780\u001b[0m     \u001b[0mSum\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, NN_ARCHITECTURE, 2, 0.05, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
