{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron, MNIST\n",
    "---\n",
    "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data\n",
    "2. Define a neural network\n",
    "3. Train the model\n",
    "4. Evaluate the performance of our trained model on a test dataset!\n",
    "\n",
    "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Architecture ######\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 800, \"output_dim\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 128, \"output_dim\": 10, \"activation\": \"sigmoid\"} #Or relu again like the original example\n",
    "]#No Dropout...yet\n",
    "\n",
    "\n",
    "######  Init Layers  ######\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "\n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(image, params, s): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] #5x5\n",
    "    f_num = f.shape[0]\n",
    "    \n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] * f[z, :, :, :]) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "\n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X,filter_params ,params_values, nn_architecture, dropout):\n",
    "    \n",
    "    ######################## Forward Propagation Convolution Part  ##########################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    \n",
    "    params = [f1, b1]\n",
    "    conv1 = conv(X, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "    conv1[conv1<=0] = 0 #Relu\n",
    "    \n",
    "    params = [f2, b2]\n",
    "    conv2 = conv(conv1, params, 1)\n",
    "    conv2[conv2<=0] = 0 #Relu\n",
    "    \n",
    "    pl = maxpool(conv2, 2, 2) #pool_f = 2 , pool_s = 2\n",
    "    \n",
    "    #packet\n",
    "    conv_mem = [X, conv1, conv2, pl]\n",
    "    \n",
    "    num_c, f_dim, _ = pl.shape\n",
    "    fc1 = pl.reshape(num_c*f_dim*f_dim, 1) #Flattening\n",
    "\n",
    "    \n",
    "    ######################## Forward Propagation FC Part  ##########################\n",
    "    \n",
    "    \n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = fc1\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        if ((layer_idx == 1)and(dropout)):\n",
    "                ## Dropout ##\n",
    "                d = (np.random.rand(W_curr.shape[0],W_curr.shape[1])<0.2)\n",
    "                d = d*1 #Bool --> int(0s and 1s)\n",
    "                W_curr = d*W_curr\n",
    "                #############\n",
    "            \n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, conv_mem, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = []\n",
    "    #print(len(targets))\n",
    "    for i in range(len(targets)):\n",
    "        out.append(logs[i][targets[i]])\n",
    "    out = np.array(out)\n",
    "    \n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  BACK PROPAGATION  #######\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # calculation of the activation function derivative\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    \n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr \n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture):\n",
    "    \n",
    "    \n",
    "    ################# Backwardpropagation for FC Part  #######################\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    #print(Y.shape)\n",
    "    m = Y.shape[1]     # 1 sample each time\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = Y_hat - Y#- (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        \n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    ################# Backwardpropagation for Conv Part  #######################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    [image_in, conv1, conv2, pl]= conv_mem\n",
    "    #dA_prev\n",
    "    #Find dimensions of pooled image\n",
    "    #dim = int(np.sqrt(dA_prev.shape(0)/f2.shape(0))) #sqrt(800/8)=10 ==> 8*10*10\n",
    "    dpool = dA_prev.reshape(pl.shape) #, 1) \n",
    "    dconv2 = maxpoolBackward(dpool, conv2)  # , pool_f, pool_s)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    conv_s = 1\n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) #\n",
    "    dconv1[conv1<=0] = 0\n",
    "    \n",
    "    _, df1, db1 = convolutionBackward(dconv1, image_in, f1, conv_s)\n",
    "    \n",
    "    conv_grads = [df1, df2, db1, db2] \n",
    "    \n",
    "    return conv_grads, grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE ######\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    \n",
    "    ####### Building index labels from One-Hot matrix #######\n",
    "    temp = []\n",
    "    #train_labels = train_labels.sum(1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if(Y[i][j]==1):\n",
    "                temp.append(j)\n",
    "    #########################################################\n",
    "    \n",
    "    ##filter params\n",
    "    num_f1 = 8\n",
    "    num_f2 = 8\n",
    "    f_dim = 5\n",
    "    f1 = (num_f1, 1, f_dim, f_dim )\n",
    "    f2 = (num_f2, num_f1, f_dim, f_dim )\n",
    "    #To make for a smoother training process, we initialize each filter with a mean of 0 and a standard deviation of 1\n",
    "    scale = 1.0\n",
    "    #stddev = scale/np.sqrt(np.prod(f1))\n",
    "    trim = 0.1\n",
    "    f1 = np.random.randn( num_f1, 1, f_dim, f_dim) *trim\n",
    "    stddev = scale/np.sqrt(np.prod(f2))\n",
    "    f2 = np.random.randn(num_f2, num_f1, f_dim, f_dim ) *trim\n",
    "    b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "    b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "    #Packing Conv params\n",
    "    filter_params = []\n",
    "    filter_params = (f1, f2, b1, b2)\n",
    "\n",
    "    \n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 10\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            \n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            \n",
    "            df1 = np.zeros(f1.shape)\n",
    "            df2 = np.zeros(f2.shape)\n",
    "            db1 = np.zeros(b1.shape)\n",
    "            db2 = np.zeros(b2.shape)\n",
    "            dW1 = np.zeros(params_values['W1'].shape)\n",
    "            dW2 = np.zeros(params_values['W2'].shape)\n",
    "            dB1 = np.zeros(params_values['b1'].shape)\n",
    "            dB2 = np.zeros(params_values['b2'].shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            Ys  = np.array(temp[c:(c + batch)]).reshape(batch,1) #shape (m,1), NOT one-hot\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                \n",
    "                # 1 image per time...for now\n",
    "                Y_hat, conv_mem, cashe = full_forward_propagation(X_t[b], filter_params, params_values, nn_architecture, True)\n",
    "            \n",
    "            \n",
    "                       \n",
    "                Yh = np.array(Y_hat.T)\n",
    "\n",
    "                ############### LogSoftMax  #################\n",
    "                #x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "                num = np.exp(Yh)\n",
    "                den = np.sum(np.exp(Yh), axis = 1)\n",
    "\n",
    "                for i in range(Yh.shape[0]): #1  , just 1 per time\n",
    "                    #for j in range(Yh.shape[1]): #10\n",
    "                    Yh[i][:] = np.log(num[i][:] / den[i])  \n",
    "                #############################################\n",
    "            \n",
    "                #print(Yh.shape)\n",
    "                #print(len(Ys))                \n",
    "                cost += NLLLoss(Yh, np.array(Ys[b]))  #(we need to send image(samples,classes), labels(NOT in one-hot) - Future optimizations needed)\n",
    "                #print(\"Cost: {%f}\" %(cost))\n",
    "                \n",
    "                t = np.array(Ys[b])\n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b].reshape(10,1))\n",
    "                \n",
    "                conv_grads, grads_values = full_backward_propagation(Y_hat, Y_t[b].reshape(Y_t[b].shape[0],1), conv_mem, cashe, filter_params, params_values, nn_architecture)\n",
    "                [df1_, df2_, db1_, db2_] = conv_grads\n",
    "\n",
    "                df1 += df1_\n",
    "                df2 += df2_\n",
    "                db1 += db1_\n",
    "                db2 += db2_\n",
    "\n",
    "                dW1 +=  grads_values['dW1']\n",
    "                dW2 +=  grads_values['dW2']\n",
    "                dB1 += grads_values['db1']\n",
    "                dB2 += grads_values['db2']\n",
    "                \n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            print(c)\n",
    "            \n",
    "\n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1/(batch)\n",
    "            grads_values['dW2'] = dW2/(batch)\n",
    "            grads_values['db1'] = dB1/(batch)\n",
    "            grads_values['db2'] = dB2/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "0\n",
      "Cost : 2.27952\n",
      "Accuracy : 0.00000%\n",
      "10\n",
      "Cost : 2.33325\n",
      "Accuracy : 0.00000%\n",
      "20\n",
      "Cost : 2.31127\n",
      "Accuracy : 10.00000%\n",
      "30\n",
      "Cost : 2.35412\n",
      "Accuracy : 0.00000%\n",
      "40\n",
      "Cost : 2.31121\n",
      "Accuracy : 0.00000%\n",
      "50\n",
      "Cost : 2.29502\n",
      "Accuracy : 0.00000%\n",
      "60\n",
      "Cost : 2.28230\n",
      "Accuracy : 0.00000%\n",
      "70\n",
      "Cost : 2.28404\n",
      "Accuracy : 0.00000%\n",
      "80\n",
      "Cost : 2.31467\n",
      "Accuracy : 0.00000%\n",
      "90\n",
      "Cost : 2.29755\n",
      "Accuracy : 0.00000%\n",
      "100\n",
      "Cost : 2.29455\n",
      "Accuracy : 0.00000%\n",
      "110\n",
      "Cost : 2.27158\n",
      "Accuracy : 0.00000%\n",
      "120\n",
      "Cost : 2.27422\n",
      "Accuracy : 0.00000%\n",
      "130\n",
      "Cost : 2.30231\n",
      "Accuracy : 0.00000%\n",
      "140\n",
      "Cost : 2.30845\n",
      "Accuracy : 0.00000%\n",
      "150\n",
      "Cost : 2.31450\n",
      "Accuracy : 0.00000%\n",
      "160\n",
      "Cost : 2.30483\n",
      "Accuracy : 0.00000%\n",
      "170\n",
      "Cost : 2.31785\n",
      "Accuracy : 0.00000%\n",
      "180\n",
      "Cost : 2.29644\n",
      "Accuracy : 0.00000%\n",
      "190\n",
      "Cost : 2.30024\n",
      "Accuracy : 0.00000%\n",
      "200\n",
      "Cost : 2.28272\n",
      "Accuracy : 0.00000%\n",
      "210\n",
      "Cost : 2.28258\n",
      "Accuracy : 0.00000%\n",
      "220\n",
      "Cost : 2.28137\n",
      "Accuracy : 0.00000%\n",
      "230\n",
      "Cost : 2.30665\n",
      "Accuracy : 0.00000%\n",
      "240\n",
      "Cost : 2.28673\n",
      "Accuracy : 0.00000%\n",
      "250\n",
      "Cost : 2.29485\n",
      "Accuracy : 0.00000%\n",
      "260\n",
      "Cost : 2.30308\n",
      "Accuracy : 0.00000%\n",
      "270\n",
      "Cost : 2.28376\n",
      "Accuracy : 0.00000%\n",
      "280\n",
      "Cost : 2.29198\n",
      "Accuracy : 0.00000%\n",
      "290\n",
      "Cost : 2.30197\n",
      "Accuracy : 0.00000%\n",
      "300\n",
      "Cost : 2.30926\n",
      "Accuracy : 0.00000%\n",
      "310\n",
      "Cost : 2.29520\n",
      "Accuracy : 0.00000%\n",
      "320\n",
      "Cost : 2.29900\n",
      "Accuracy : 0.00000%\n",
      "330\n",
      "Cost : 2.30622\n",
      "Accuracy : 0.00000%\n",
      "340\n",
      "Cost : 2.30264\n",
      "Accuracy : 0.00000%\n",
      "350\n",
      "Cost : 2.28872\n",
      "Accuracy : 0.00000%\n",
      "360\n",
      "Cost : 2.29679\n",
      "Accuracy : 0.00000%\n",
      "370\n",
      "Cost : 2.28454\n",
      "Accuracy : 0.00000%\n",
      "380\n",
      "Cost : 2.26740\n",
      "Accuracy : 0.00000%\n",
      "390\n",
      "Cost : 2.28296\n",
      "Accuracy : 0.00000%\n",
      "400\n",
      "Cost : 2.26589\n",
      "Accuracy : 0.00000%\n",
      "410\n",
      "Cost : 2.30309\n",
      "Accuracy : 0.00000%\n",
      "420\n",
      "Cost : 2.31169\n",
      "Accuracy : 0.00000%\n",
      "430\n",
      "Cost : 2.29423\n",
      "Accuracy : 0.00000%\n",
      "440\n",
      "Cost : 2.28120\n",
      "Accuracy : 0.00000%\n",
      "450\n",
      "Cost : 2.25980\n",
      "Accuracy : 0.00000%\n",
      "460\n",
      "Cost : 2.27414\n",
      "Accuracy : 0.00000%\n",
      "470\n",
      "Cost : 2.27436\n",
      "Accuracy : 0.00000%\n",
      "480\n",
      "Cost : 2.29099\n",
      "Accuracy : 0.00000%\n",
      "490\n",
      "Cost : 2.29080\n",
      "Accuracy : 0.00000%\n",
      "500\n",
      "Cost : 2.27937\n",
      "Accuracy : 0.00000%\n",
      "510\n",
      "Cost : 2.28125\n",
      "Accuracy : 0.00000%\n",
      "520\n",
      "Cost : 2.30300\n",
      "Accuracy : 0.00000%\n",
      "530\n",
      "Cost : 2.29039\n",
      "Accuracy : 0.00000%\n",
      "540\n",
      "Cost : 2.31925\n",
      "Accuracy : 0.00000%\n",
      "550\n",
      "Cost : 2.29012\n",
      "Accuracy : 0.00000%\n",
      "560\n",
      "Cost : 2.32076\n",
      "Accuracy : 0.00000%\n",
      "570\n",
      "Cost : 2.27550\n",
      "Accuracy : 0.00000%\n",
      "580\n",
      "Cost : 2.29387\n",
      "Accuracy : 0.00000%\n",
      "590\n",
      "Cost : 2.29708\n",
      "Accuracy : 0.00000%\n",
      "600\n",
      "Cost : 2.26495\n",
      "Accuracy : 0.00000%\n",
      "610\n",
      "Cost : 2.28534\n",
      "Accuracy : 0.00000%\n",
      "620\n",
      "Cost : 2.31791\n",
      "Accuracy : 0.00000%\n",
      "630\n",
      "Cost : 2.26538\n",
      "Accuracy : 0.00000%\n",
      "640\n",
      "Cost : 2.29512\n",
      "Accuracy : 0.00000%\n",
      "650\n",
      "Cost : 2.29318\n",
      "Accuracy : 0.00000%\n",
      "660\n",
      "Cost : 2.28255\n",
      "Accuracy : 0.00000%\n",
      "670\n",
      "Cost : 2.27943\n",
      "Accuracy : 0.00000%\n",
      "680\n",
      "Cost : 2.29285\n",
      "Accuracy : 0.00000%\n",
      "690\n",
      "Cost : 2.28073\n",
      "Accuracy : 0.00000%\n",
      "700\n",
      "Cost : 2.29743\n",
      "Accuracy : 0.00000%\n",
      "710\n",
      "Cost : 2.26775\n",
      "Accuracy : 0.00000%\n",
      "720\n",
      "Cost : 2.32316\n",
      "Accuracy : 0.00000%\n",
      "730\n",
      "Cost : 2.28908\n",
      "Accuracy : 0.00000%\n",
      "740\n",
      "Cost : 2.28372\n",
      "Accuracy : 0.00000%\n",
      "750\n",
      "Cost : 2.31276\n",
      "Accuracy : 0.00000%\n",
      "760\n",
      "Cost : 2.30769\n",
      "Accuracy : 0.00000%\n",
      "770\n",
      "Cost : 2.28634\n",
      "Accuracy : 0.00000%\n",
      "780\n",
      "Cost : 2.26344\n",
      "Accuracy : 0.00000%\n",
      "790\n",
      "Cost : 2.26729\n",
      "Accuracy : 0.00000%\n",
      "800\n",
      "Cost : 2.29496\n",
      "Accuracy : 0.00000%\n",
      "810\n",
      "Cost : 2.28644\n",
      "Accuracy : 0.00000%\n",
      "820\n",
      "Cost : 2.26316\n",
      "Accuracy : 0.00000%\n",
      "830\n",
      "Cost : 2.28284\n",
      "Accuracy : 0.00000%\n",
      "840\n",
      "Cost : 2.31076\n",
      "Accuracy : 0.00000%\n",
      "850\n",
      "Cost : 2.29613\n",
      "Accuracy : 0.00000%\n",
      "860\n",
      "Cost : 2.30279\n",
      "Accuracy : 0.00000%\n",
      "870\n",
      "Cost : 2.26995\n",
      "Accuracy : 0.00000%\n",
      "880\n",
      "Cost : 2.29844\n",
      "Accuracy : 0.00000%\n",
      "890\n",
      "Cost : 2.29713\n",
      "Accuracy : 0.00000%\n",
      "900\n",
      "Cost : 2.27319\n",
      "Accuracy : 0.00000%\n",
      "910\n",
      "Cost : 2.26885\n",
      "Accuracy : 0.00000%\n",
      "920\n",
      "Cost : 2.27860\n",
      "Accuracy : 0.00000%\n",
      "930\n",
      "Cost : 2.29007\n",
      "Accuracy : 0.00000%\n",
      "940\n",
      "Cost : 2.27143\n",
      "Accuracy : 0.00000%\n",
      "950\n",
      "Cost : 2.22720\n",
      "Accuracy : 0.00000%\n",
      "960\n",
      "Cost : 2.24123\n",
      "Accuracy : 0.00000%\n",
      "970\n",
      "Cost : 2.27342\n",
      "Accuracy : 0.00000%\n",
      "980\n",
      "Cost : 2.26982\n",
      "Accuracy : 0.00000%\n",
      "990\n",
      "Cost : 2.27008\n",
      "Accuracy : 0.00000%\n",
      "1000\n",
      "Cost : 2.24223\n",
      "Accuracy : 0.00000%\n",
      "1010\n",
      "Cost : 2.22334\n",
      "Accuracy : 0.00000%\n",
      "1020\n",
      "Cost : 2.28837\n",
      "Accuracy : 0.00000%\n",
      "1030\n",
      "Cost : 2.30356\n",
      "Accuracy : 0.00000%\n",
      "1040\n",
      "Cost : 2.20622\n",
      "Accuracy : 0.00000%\n",
      "1050\n",
      "Cost : 2.25124\n",
      "Accuracy : 0.00000%\n",
      "1060\n",
      "Cost : 2.22444\n",
      "Accuracy : 10.00000%\n",
      "1070\n",
      "Cost : 2.29466\n",
      "Accuracy : 0.00000%\n",
      "1080\n",
      "Cost : 2.23421\n",
      "Accuracy : 0.00000%\n",
      "1090\n",
      "Cost : 2.24622\n",
      "Accuracy : 0.00000%\n",
      "1100\n",
      "Cost : 2.27285\n",
      "Accuracy : 0.00000%\n",
      "1110\n",
      "Cost : 2.24626\n",
      "Accuracy : 0.00000%\n",
      "1120\n",
      "Cost : 2.28907\n",
      "Accuracy : 0.00000%\n",
      "1130\n",
      "Cost : 2.25358\n",
      "Accuracy : 0.00000%\n",
      "1140\n",
      "Cost : 2.29628\n",
      "Accuracy : 0.00000%\n",
      "1150\n",
      "Cost : 2.25136\n",
      "Accuracy : 10.00000%\n",
      "1160\n",
      "Cost : 2.31323\n",
      "Accuracy : 0.00000%\n",
      "1170\n",
      "Cost : 2.27289\n",
      "Accuracy : 0.00000%\n",
      "1180\n",
      "Cost : 2.25172\n",
      "Accuracy : 10.00000%\n",
      "1190\n",
      "Cost : 2.23318\n",
      "Accuracy : 0.00000%\n",
      "1200\n",
      "Cost : 2.24588\n",
      "Accuracy : 0.00000%\n",
      "1210\n",
      "Cost : 2.21974\n",
      "Accuracy : 10.00000%\n",
      "1220\n",
      "Cost : 2.27481\n",
      "Accuracy : 0.00000%\n",
      "1230\n",
      "Cost : 2.24889\n",
      "Accuracy : 0.00000%\n",
      "1240\n",
      "Cost : 2.28275\n",
      "Accuracy : 0.00000%\n",
      "1250\n",
      "Cost : 2.23079\n",
      "Accuracy : 0.00000%\n",
      "1260\n",
      "Cost : 2.29572\n",
      "Accuracy : 0.00000%\n",
      "1270\n",
      "Cost : 2.24741\n",
      "Accuracy : 0.00000%\n",
      "1280\n",
      "Cost : 2.28009\n",
      "Accuracy : 0.00000%\n",
      "1290\n",
      "Cost : 2.28281\n",
      "Accuracy : 0.00000%\n",
      "1300\n",
      "Cost : 2.28320\n",
      "Accuracy : 0.00000%\n",
      "1310\n",
      "Cost : 2.28926\n",
      "Accuracy : 0.00000%\n",
      "1320\n",
      "Cost : 2.25403\n",
      "Accuracy : 0.00000%\n",
      "1330\n",
      "Cost : 2.27106\n",
      "Accuracy : 0.00000%\n",
      "1340\n",
      "Cost : 2.24459\n",
      "Accuracy : 0.00000%\n",
      "1350\n",
      "Cost : 2.26575\n",
      "Accuracy : 0.00000%\n",
      "1360\n",
      "Cost : 2.21544\n",
      "Accuracy : 10.00000%\n",
      "1370\n",
      "Cost : 2.19330\n",
      "Accuracy : 10.00000%\n",
      "1380\n",
      "Cost : 2.21924\n",
      "Accuracy : 10.00000%\n",
      "1390\n",
      "Cost : 2.25779\n",
      "Accuracy : 0.00000%\n",
      "1400\n",
      "Cost : 2.22453\n",
      "Accuracy : 0.00000%\n",
      "1410\n",
      "Cost : 2.28243\n",
      "Accuracy : 0.00000%\n",
      "1420\n",
      "Cost : 2.25439\n",
      "Accuracy : 0.00000%\n",
      "1430\n",
      "Cost : 2.28058\n",
      "Accuracy : 0.00000%\n",
      "1440\n",
      "Cost : 2.23734\n",
      "Accuracy : 0.00000%\n",
      "1450\n",
      "Cost : 2.22368\n",
      "Accuracy : 0.00000%\n",
      "1460\n",
      "Cost : 2.27034\n",
      "Accuracy : 0.00000%\n",
      "1470\n",
      "Cost : 2.24928\n",
      "Accuracy : 0.00000%\n",
      "1480\n",
      "Cost : 2.22188\n",
      "Accuracy : 10.00000%\n",
      "1490\n",
      "Cost : 2.20113\n",
      "Accuracy : 10.00000%\n",
      "1500\n",
      "Cost : 2.19690\n",
      "Accuracy : 20.00000%\n",
      "1510\n",
      "Cost : 2.24036\n",
      "Accuracy : 10.00000%\n",
      "1520\n",
      "Cost : 2.18280\n",
      "Accuracy : 0.00000%\n",
      "1530\n",
      "Cost : 2.21945\n",
      "Accuracy : 10.00000%\n",
      "1540\n",
      "Cost : 2.22009\n",
      "Accuracy : 0.00000%\n",
      "1550\n",
      "Cost : 2.30530\n",
      "Accuracy : 0.00000%\n",
      "1560\n",
      "Cost : 2.26757\n",
      "Accuracy : 0.00000%\n",
      "1570\n",
      "Cost : 2.25136\n",
      "Accuracy : 0.00000%\n",
      "1580\n",
      "Cost : 2.28698\n",
      "Accuracy : 0.00000%\n",
      "1590\n",
      "Cost : 2.23366\n",
      "Accuracy : 0.00000%\n",
      "1600\n",
      "Cost : 2.19433\n",
      "Accuracy : 0.00000%\n",
      "1610\n",
      "Cost : 2.28071\n",
      "Accuracy : 0.00000%\n",
      "1620\n",
      "Cost : 2.19232\n",
      "Accuracy : 0.00000%\n",
      "1630\n",
      "Cost : 2.29148\n",
      "Accuracy : 0.00000%\n",
      "1640\n",
      "Cost : 2.22031\n",
      "Accuracy : 0.00000%\n",
      "1650\n",
      "Cost : 2.21967\n",
      "Accuracy : 10.00000%\n",
      "1660\n",
      "Cost : 2.19293\n",
      "Accuracy : 10.00000%\n",
      "1670\n",
      "Cost : 2.22060\n",
      "Accuracy : 10.00000%\n",
      "1680\n",
      "Cost : 2.19835\n",
      "Accuracy : 0.00000%\n",
      "1690\n",
      "Cost : 2.18775\n",
      "Accuracy : 10.00000%\n",
      "1700\n",
      "Cost : 2.13551\n",
      "Accuracy : 20.00000%\n",
      "1710\n",
      "Cost : 2.13587\n",
      "Accuracy : 30.00000%\n",
      "1720\n",
      "Cost : 2.12412\n",
      "Accuracy : 30.00000%\n",
      "1730\n",
      "Cost : 2.19248\n",
      "Accuracy : 10.00000%\n",
      "1740\n",
      "Cost : 2.22097\n",
      "Accuracy : 0.00000%\n",
      "1750\n",
      "Cost : 2.27237\n",
      "Accuracy : 0.00000%\n",
      "1760\n",
      "Cost : 2.22120\n",
      "Accuracy : 10.00000%\n",
      "1770\n",
      "Cost : 2.19681\n",
      "Accuracy : 10.00000%\n",
      "1780\n",
      "Cost : 2.25390\n",
      "Accuracy : 0.00000%\n",
      "1790\n",
      "Cost : 2.20027\n",
      "Accuracy : 20.00000%\n",
      "1800\n",
      "Cost : 2.24493\n",
      "Accuracy : 0.00000%\n",
      "1810\n",
      "Cost : 2.21175\n",
      "Accuracy : 20.00000%\n",
      "1820\n",
      "Cost : 2.19317\n",
      "Accuracy : 10.00000%\n",
      "1830\n",
      "Cost : 2.12532\n",
      "Accuracy : 30.00000%\n",
      "1840\n",
      "Cost : 2.26709\n",
      "Accuracy : 0.00000%\n",
      "1850\n",
      "Cost : 2.17064\n",
      "Accuracy : 20.00000%\n",
      "1860\n",
      "Cost : 2.18694\n",
      "Accuracy : 10.00000%\n",
      "1870\n",
      "Cost : 2.20111\n",
      "Accuracy : 0.00000%\n",
      "1880\n",
      "Cost : 2.27795\n",
      "Accuracy : 0.00000%\n",
      "1890\n",
      "Cost : 2.21297\n",
      "Accuracy : 10.00000%\n",
      "1900\n",
      "Cost : 2.20127\n",
      "Accuracy : 10.00000%\n",
      "1910\n",
      "Cost : 2.16982\n",
      "Accuracy : 10.00000%\n",
      "1920\n",
      "Cost : 2.17494\n",
      "Accuracy : 20.00000%\n",
      "1930\n",
      "Cost : 2.17850\n",
      "Accuracy : 10.00000%\n",
      "1940\n",
      "Cost : 2.18307\n",
      "Accuracy : 20.00000%\n",
      "1950\n",
      "Cost : 2.23735\n",
      "Accuracy : 0.00000%\n",
      "1960\n",
      "Cost : 2.22474\n",
      "Accuracy : 0.00000%\n",
      "1970\n",
      "Cost : 2.23373\n",
      "Accuracy : 0.00000%\n",
      "1980\n",
      "Cost : 2.24062\n",
      "Accuracy : 10.00000%\n",
      "1990\n",
      "Cost : 2.21169\n",
      "Accuracy : 20.00000%\n",
      "2000\n",
      "Cost : 2.20732\n",
      "Accuracy : 10.00000%\n",
      "2010\n",
      "Cost : 2.22862\n",
      "Accuracy : 10.00000%\n",
      "2020\n",
      "Cost : 2.22766\n",
      "Accuracy : 0.00000%\n",
      "2030\n",
      "Cost : 2.22743\n",
      "Accuracy : 0.00000%\n",
      "2040\n",
      "Cost : 2.14075\n",
      "Accuracy : 20.00000%\n",
      "2050\n",
      "Cost : 2.16270\n",
      "Accuracy : 10.00000%\n",
      "2060\n",
      "Cost : 2.25215\n",
      "Accuracy : 0.00000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2070\n",
      "Cost : 2.19138\n",
      "Accuracy : 10.00000%\n",
      "2080\n",
      "Cost : 2.05387\n",
      "Accuracy : 30.00000%\n",
      "2090\n",
      "Cost : 2.22404\n",
      "Accuracy : 0.00000%\n",
      "2100\n",
      "Cost : 2.03047\n",
      "Accuracy : 30.00000%\n",
      "2110\n",
      "Cost : 2.18402\n",
      "Accuracy : 20.00000%\n",
      "2120\n",
      "Cost : 2.15239\n",
      "Accuracy : 20.00000%\n",
      "2130\n",
      "Cost : 2.18926\n",
      "Accuracy : 10.00000%\n",
      "2140\n",
      "Cost : 2.16251\n",
      "Accuracy : 10.00000%\n",
      "2150\n",
      "Cost : 2.13674\n",
      "Accuracy : 30.00000%\n",
      "2160\n",
      "Cost : 2.05028\n",
      "Accuracy : 30.00000%\n",
      "2170\n",
      "Cost : 2.17807\n",
      "Accuracy : 10.00000%\n",
      "2180\n",
      "Cost : 2.19630\n",
      "Accuracy : 10.00000%\n",
      "2190\n",
      "Cost : 2.12773\n",
      "Accuracy : 20.00000%\n",
      "2200\n",
      "Cost : 2.18842\n",
      "Accuracy : 10.00000%\n",
      "2210\n",
      "Cost : 2.27748\n",
      "Accuracy : 0.00000%\n",
      "2220\n",
      "Cost : 2.10612\n",
      "Accuracy : 20.00000%\n",
      "2230\n",
      "Cost : 2.24176\n",
      "Accuracy : 0.00000%\n",
      "2240\n",
      "Cost : 2.13930\n",
      "Accuracy : 20.00000%\n",
      "2250\n",
      "Cost : 2.11344\n",
      "Accuracy : 20.00000%\n",
      "2260\n",
      "Cost : 2.16322\n",
      "Accuracy : 20.00000%\n",
      "2270\n",
      "Cost : 2.25791\n",
      "Accuracy : 0.00000%\n",
      "2280\n",
      "Cost : 2.24012\n",
      "Accuracy : 0.00000%\n",
      "2290\n",
      "Cost : 2.16380\n",
      "Accuracy : 10.00000%\n",
      "2300\n",
      "Cost : 2.18653\n",
      "Accuracy : 10.00000%\n",
      "2310\n",
      "Cost : 2.15277\n",
      "Accuracy : 10.00000%\n",
      "2320\n",
      "Cost : 2.14859\n",
      "Accuracy : 20.00000%\n",
      "2330\n",
      "Cost : 2.13565\n",
      "Accuracy : 20.00000%\n",
      "2340\n",
      "Cost : 2.22469\n",
      "Accuracy : 10.00000%\n",
      "2350\n",
      "Cost : 2.15196\n",
      "Accuracy : 10.00000%\n",
      "2360\n",
      "Cost : 2.18147\n",
      "Accuracy : 20.00000%\n",
      "2370\n",
      "Cost : 2.12216\n",
      "Accuracy : 20.00000%\n",
      "2380\n",
      "Cost : 2.24298\n",
      "Accuracy : 0.00000%\n",
      "2390\n",
      "Cost : 2.09894\n",
      "Accuracy : 30.00000%\n",
      "2400\n",
      "Cost : 2.15189\n",
      "Accuracy : 20.00000%\n",
      "2410\n",
      "Cost : 2.10309\n",
      "Accuracy : 20.00000%\n",
      "2420\n",
      "Cost : 2.11085\n",
      "Accuracy : 20.00000%\n",
      "2430\n",
      "Cost : 2.15252\n",
      "Accuracy : 10.00000%\n",
      "2440\n",
      "Cost : 2.13396\n",
      "Accuracy : 30.00000%\n",
      "2450\n",
      "Cost : 2.14767\n",
      "Accuracy : 20.00000%\n",
      "2460\n",
      "Cost : 2.15708\n",
      "Accuracy : 20.00000%\n",
      "2470\n",
      "Cost : 2.18237\n",
      "Accuracy : 10.00000%\n",
      "2480\n",
      "Cost : 2.11736\n",
      "Accuracy : 20.00000%\n",
      "2490\n",
      "Cost : 2.01472\n",
      "Accuracy : 40.00000%\n",
      "2500\n",
      "Cost : 2.09297\n",
      "Accuracy : 30.00000%\n",
      "2510\n",
      "Cost : 2.18283\n",
      "Accuracy : 0.00000%\n",
      "2520\n",
      "Cost : 2.00599\n",
      "Accuracy : 40.00000%\n",
      "2530\n",
      "Cost : 2.04516\n",
      "Accuracy : 30.00000%\n",
      "2540\n",
      "Cost : 2.19285\n",
      "Accuracy : 0.00000%\n",
      "2550\n",
      "Cost : 2.16072\n",
      "Accuracy : 30.00000%\n",
      "2560\n",
      "Cost : 2.11643\n",
      "Accuracy : 20.00000%\n",
      "2570\n",
      "Cost : 2.12787\n",
      "Accuracy : 20.00000%\n",
      "2580\n",
      "Cost : 2.07535\n",
      "Accuracy : 30.00000%\n",
      "2590\n",
      "Cost : 2.06267\n",
      "Accuracy : 30.00000%\n",
      "2600\n",
      "Cost : 2.05550\n",
      "Accuracy : 30.00000%\n",
      "2610\n",
      "Cost : 2.04012\n",
      "Accuracy : 30.00000%\n",
      "2620\n",
      "Cost : 1.97736\n",
      "Accuracy : 40.00000%\n",
      "2630\n",
      "Cost : 2.17405\n",
      "Accuracy : 0.00000%\n",
      "2640\n",
      "Cost : 2.07982\n",
      "Accuracy : 30.00000%\n",
      "2650\n",
      "Cost : 2.08979\n",
      "Accuracy : 30.00000%\n",
      "2660\n",
      "Cost : 2.02282\n",
      "Accuracy : 40.00000%\n",
      "2670\n",
      "Cost : 2.22396\n",
      "Accuracy : 0.00000%\n",
      "2680\n",
      "Cost : 2.19824\n",
      "Accuracy : 10.00000%\n",
      "2690\n",
      "Cost : 2.10457\n",
      "Accuracy : 20.00000%\n",
      "2700\n",
      "Cost : 2.12748\n",
      "Accuracy : 10.00000%\n",
      "2710\n",
      "Cost : 2.08110\n",
      "Accuracy : 30.00000%\n",
      "2720\n",
      "Cost : 2.09533\n",
      "Accuracy : 20.00000%\n",
      "2730\n",
      "Cost : 2.03342\n",
      "Accuracy : 50.00000%\n",
      "2740\n",
      "Cost : 1.98169\n",
      "Accuracy : 50.00000%\n",
      "2750\n",
      "Cost : 2.09821\n",
      "Accuracy : 10.00000%\n",
      "2760\n",
      "Cost : 2.20195\n",
      "Accuracy : 10.00000%\n",
      "2770\n",
      "Cost : 2.12087\n",
      "Accuracy : 10.00000%\n",
      "2780\n",
      "Cost : 2.05831\n",
      "Accuracy : 20.00000%\n",
      "2790\n",
      "Cost : 2.14515\n",
      "Accuracy : 30.00000%\n",
      "2800\n",
      "Cost : 2.15300\n",
      "Accuracy : 10.00000%\n",
      "2810\n",
      "Cost : 2.14592\n",
      "Accuracy : 20.00000%\n",
      "2820\n",
      "Cost : 2.09422\n",
      "Accuracy : 20.00000%\n",
      "2830\n",
      "Cost : 2.11005\n",
      "Accuracy : 20.00000%\n",
      "2840\n",
      "Cost : 2.12602\n",
      "Accuracy : 20.00000%\n",
      "2850\n",
      "Cost : 2.06343\n",
      "Accuracy : 20.00000%\n",
      "2860\n",
      "Cost : 2.15660\n",
      "Accuracy : 0.00000%\n",
      "2870\n",
      "Cost : 1.99868\n",
      "Accuracy : 40.00000%\n",
      "2880\n",
      "Cost : 2.14862\n",
      "Accuracy : 20.00000%\n",
      "2890\n",
      "Cost : 2.07679\n",
      "Accuracy : 20.00000%\n",
      "2900\n",
      "Cost : 2.15486\n",
      "Accuracy : 10.00000%\n",
      "2910\n",
      "Cost : 2.02280\n",
      "Accuracy : 30.00000%\n",
      "2920\n",
      "Cost : 2.07286\n",
      "Accuracy : 20.00000%\n",
      "2930\n",
      "Cost : 1.95624\n",
      "Accuracy : 50.00000%\n",
      "2940\n",
      "Cost : 2.15119\n",
      "Accuracy : 10.00000%\n",
      "2950\n",
      "Cost : 2.19879\n",
      "Accuracy : 10.00000%\n",
      "2960\n",
      "Cost : 2.11031\n",
      "Accuracy : 20.00000%\n",
      "2970\n",
      "Cost : 2.07334\n",
      "Accuracy : 40.00000%\n",
      "2980\n",
      "Cost : 2.11737\n",
      "Accuracy : 10.00000%\n",
      "2990\n",
      "Cost : 2.08042\n",
      "Accuracy : 30.00000%\n",
      "3000\n",
      "Cost : 2.15538\n",
      "Accuracy : 20.00000%\n",
      "3010\n",
      "Cost : 2.04766\n",
      "Accuracy : 30.00000%\n",
      "3020\n",
      "Cost : 2.10680\n",
      "Accuracy : 30.00000%\n",
      "3030\n",
      "Cost : 2.14579\n",
      "Accuracy : 10.00000%\n",
      "3040\n",
      "Cost : 2.08546\n",
      "Accuracy : 30.00000%\n",
      "3050\n",
      "Cost : 2.11775\n",
      "Accuracy : 20.00000%\n",
      "3060\n",
      "Cost : 2.14122\n",
      "Accuracy : 30.00000%\n",
      "3070\n",
      "Cost : 2.10792\n",
      "Accuracy : 30.00000%\n",
      "3080\n",
      "Cost : 2.13069\n",
      "Accuracy : 10.00000%\n",
      "3090\n",
      "Cost : 2.15478\n",
      "Accuracy : 10.00000%\n",
      "3100\n",
      "Cost : 1.97274\n",
      "Accuracy : 20.00000%\n",
      "3110\n",
      "Cost : 2.04440\n",
      "Accuracy : 40.00000%\n",
      "3120\n",
      "Cost : 1.98706\n",
      "Accuracy : 40.00000%\n",
      "3130\n",
      "Cost : 1.87770\n",
      "Accuracy : 60.00000%\n",
      "3140\n",
      "Cost : 2.03464\n",
      "Accuracy : 30.00000%\n",
      "3150\n",
      "Cost : 2.09223\n",
      "Accuracy : 10.00000%\n",
      "3160\n",
      "Cost : 2.04622\n",
      "Accuracy : 30.00000%\n",
      "3170\n",
      "Cost : 1.98373\n",
      "Accuracy : 40.00000%\n",
      "3180\n",
      "Cost : 2.16758\n",
      "Accuracy : 10.00000%\n",
      "3190\n",
      "Cost : 1.97724\n",
      "Accuracy : 40.00000%\n",
      "3200\n",
      "Cost : 2.12054\n",
      "Accuracy : 40.00000%\n",
      "3210\n",
      "Cost : 2.13324\n",
      "Accuracy : 30.00000%\n",
      "3220\n",
      "Cost : 1.87507\n",
      "Accuracy : 60.00000%\n",
      "3230\n",
      "Cost : 1.94780\n",
      "Accuracy : 50.00000%\n",
      "3240\n",
      "Cost : 1.86315\n",
      "Accuracy : 40.00000%\n",
      "3250\n",
      "Cost : 2.03743\n",
      "Accuracy : 20.00000%\n",
      "3260\n",
      "Cost : 2.12161\n",
      "Accuracy : 10.00000%\n",
      "3270\n",
      "Cost : 2.15518\n",
      "Accuracy : 20.00000%\n",
      "3280\n",
      "Cost : 1.96728\n",
      "Accuracy : 40.00000%\n",
      "3290\n",
      "Cost : 2.05965\n",
      "Accuracy : 30.00000%\n",
      "3300\n",
      "Cost : 1.99668\n",
      "Accuracy : 30.00000%\n",
      "3310\n",
      "Cost : 2.08636\n",
      "Accuracy : 30.00000%\n",
      "3320\n",
      "Cost : 1.82844\n",
      "Accuracy : 60.00000%\n",
      "3330\n",
      "Cost : 1.87366\n",
      "Accuracy : 60.00000%\n",
      "3340\n",
      "Cost : 1.88446\n",
      "Accuracy : 50.00000%\n",
      "3350\n",
      "Cost : 2.19410\n",
      "Accuracy : 10.00000%\n",
      "3360\n",
      "Cost : 1.99235\n",
      "Accuracy : 20.00000%\n",
      "3370\n",
      "Cost : 1.96407\n",
      "Accuracy : 50.00000%\n",
      "3380\n",
      "Cost : 2.05665\n",
      "Accuracy : 30.00000%\n",
      "3390\n",
      "Cost : 1.88611\n",
      "Accuracy : 50.00000%\n",
      "3400\n",
      "Cost : 1.99431\n",
      "Accuracy : 30.00000%\n",
      "3410\n",
      "Cost : 1.86274\n",
      "Accuracy : 40.00000%\n",
      "3420\n",
      "Cost : 1.92345\n",
      "Accuracy : 50.00000%\n",
      "3430\n",
      "Cost : 2.02366\n",
      "Accuracy : 30.00000%\n",
      "3440\n",
      "Cost : 1.86685\n",
      "Accuracy : 60.00000%\n",
      "3450\n",
      "Cost : 1.84924\n",
      "Accuracy : 60.00000%\n",
      "3460\n",
      "Cost : 2.03490\n",
      "Accuracy : 40.00000%\n",
      "3470\n",
      "Cost : 1.88174\n",
      "Accuracy : 50.00000%\n",
      "3480\n",
      "Cost : 2.14311\n",
      "Accuracy : 20.00000%\n",
      "3490\n",
      "Cost : 2.03335\n",
      "Accuracy : 30.00000%\n",
      "3500\n",
      "Cost : 2.09331\n",
      "Accuracy : 20.00000%\n",
      "3510\n",
      "Cost : 1.98299\n",
      "Accuracy : 40.00000%\n",
      "3520\n",
      "Cost : 2.02538\n",
      "Accuracy : 30.00000%\n",
      "3530\n",
      "Cost : 2.03225\n",
      "Accuracy : 30.00000%\n",
      "3540\n",
      "Cost : 1.98243\n",
      "Accuracy : 50.00000%\n",
      "3550\n",
      "Cost : 1.96037\n",
      "Accuracy : 40.00000%\n",
      "3560\n",
      "Cost : 1.87067\n",
      "Accuracy : 60.00000%\n",
      "3570\n",
      "Cost : 1.94690\n",
      "Accuracy : 50.00000%\n",
      "3580\n",
      "Cost : 2.06490\n",
      "Accuracy : 20.00000%\n",
      "3590\n",
      "Cost : 1.97258\n",
      "Accuracy : 40.00000%\n",
      "3600\n",
      "Cost : 1.78249\n",
      "Accuracy : 50.00000%\n",
      "3610\n",
      "Cost : 1.92778\n",
      "Accuracy : 50.00000%\n",
      "3620\n",
      "Cost : 1.96530\n",
      "Accuracy : 30.00000%\n",
      "3630\n",
      "Cost : 2.01239\n",
      "Accuracy : 20.00000%\n",
      "3640\n",
      "Cost : 2.03014\n",
      "Accuracy : 30.00000%\n",
      "3650\n",
      "Cost : 2.00137\n",
      "Accuracy : 30.00000%\n",
      "3660\n",
      "Cost : 2.07974\n",
      "Accuracy : 20.00000%\n",
      "3670\n",
      "Cost : 2.01501\n",
      "Accuracy : 50.00000%\n",
      "3680\n",
      "Cost : 2.10781\n",
      "Accuracy : 10.00000%\n",
      "3690\n",
      "Cost : 2.00955\n",
      "Accuracy : 40.00000%\n",
      "3700\n",
      "Cost : 1.92295\n",
      "Accuracy : 50.00000%\n",
      "3710\n",
      "Cost : 2.07599\n",
      "Accuracy : 30.00000%\n",
      "3720\n",
      "Cost : 1.87882\n",
      "Accuracy : 60.00000%\n",
      "3730\n",
      "Cost : 1.97817\n",
      "Accuracy : 50.00000%\n",
      "3740\n",
      "Cost : 2.07019\n",
      "Accuracy : 30.00000%\n",
      "3750\n",
      "Cost : 1.96630\n",
      "Accuracy : 50.00000%\n",
      "3760\n",
      "Cost : 2.03071\n",
      "Accuracy : 20.00000%\n",
      "3770\n",
      "Cost : 1.92794\n",
      "Accuracy : 50.00000%\n",
      "3780\n",
      "Cost : 1.99794\n",
      "Accuracy : 30.00000%\n",
      "3790\n",
      "Cost : 1.98575\n",
      "Accuracy : 40.00000%\n",
      "3800\n",
      "Cost : 1.97797\n",
      "Accuracy : 40.00000%\n",
      "3810\n",
      "Cost : 2.11272\n",
      "Accuracy : 20.00000%\n",
      "3820\n",
      "Cost : 2.01542\n",
      "Accuracy : 40.00000%\n",
      "3830\n",
      "Cost : 2.01754\n",
      "Accuracy : 50.00000%\n",
      "3840\n",
      "Cost : 2.01576\n",
      "Accuracy : 30.00000%\n",
      "3850\n",
      "Cost : 1.88185\n",
      "Accuracy : 70.00000%\n",
      "3860\n",
      "Cost : 2.10348\n",
      "Accuracy : 20.00000%\n",
      "3870\n",
      "Cost : 2.06339\n",
      "Accuracy : 40.00000%\n",
      "3880\n",
      "Cost : 1.90230\n",
      "Accuracy : 60.00000%\n",
      "3890\n",
      "Cost : 1.84075\n",
      "Accuracy : 60.00000%\n",
      "3900\n",
      "Cost : 1.90126\n",
      "Accuracy : 50.00000%\n",
      "3910\n",
      "Cost : 1.91830\n",
      "Accuracy : 40.00000%\n",
      "3920\n",
      "Cost : 2.02270\n",
      "Accuracy : 30.00000%\n",
      "3930\n",
      "Cost : 1.97814\n",
      "Accuracy : 40.00000%\n",
      "3940\n",
      "Cost : 2.01169\n",
      "Accuracy : 40.00000%\n",
      "3950\n",
      "Cost : 1.92236\n",
      "Accuracy : 60.00000%\n",
      "3960\n",
      "Cost : 1.95390\n",
      "Accuracy : 40.00000%\n",
      "3970\n",
      "Cost : 2.08210\n",
      "Accuracy : 20.00000%\n",
      "3980\n",
      "Cost : 2.03403\n",
      "Accuracy : 30.00000%\n",
      "3990\n",
      "Cost : 1.82358\n",
      "Accuracy : 70.00000%\n",
      "4000\n",
      "Cost : 1.84164\n",
      "Accuracy : 70.00000%\n",
      "4010\n",
      "Cost : 1.83102\n",
      "Accuracy : 70.00000%\n",
      "4020\n",
      "Cost : 1.88621\n",
      "Accuracy : 50.00000%\n",
      "4030\n",
      "Cost : 1.86345\n",
      "Accuracy : 40.00000%\n",
      "4040\n",
      "Cost : 1.95399\n",
      "Accuracy : 40.00000%\n",
      "4050\n",
      "Cost : 1.97988\n",
      "Accuracy : 50.00000%\n",
      "4060\n",
      "Cost : 2.03809\n",
      "Accuracy : 40.00000%\n",
      "4070\n",
      "Cost : 1.90465\n",
      "Accuracy : 60.00000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080\n",
      "Cost : 1.94447\n",
      "Accuracy : 50.00000%\n",
      "4090\n",
      "Cost : 1.97848\n",
      "Accuracy : 40.00000%\n",
      "4100\n",
      "Cost : 1.94294\n",
      "Accuracy : 50.00000%\n",
      "4110\n",
      "Cost : 1.92979\n",
      "Accuracy : 60.00000%\n",
      "4120\n",
      "Cost : 1.80815\n",
      "Accuracy : 60.00000%\n",
      "4130\n",
      "Cost : 1.93089\n",
      "Accuracy : 40.00000%\n",
      "4140\n",
      "Cost : 1.96934\n",
      "Accuracy : 40.00000%\n",
      "4150\n",
      "Cost : 1.98872\n",
      "Accuracy : 40.00000%\n",
      "4160\n",
      "Cost : 1.98491\n",
      "Accuracy : 50.00000%\n",
      "4170\n",
      "Cost : 1.97648\n",
      "Accuracy : 50.00000%\n",
      "4180\n",
      "Cost : 1.94081\n",
      "Accuracy : 50.00000%\n",
      "4190\n",
      "Cost : 1.66156\n",
      "Accuracy : 70.00000%\n",
      "4200\n",
      "Cost : 2.04688\n",
      "Accuracy : 30.00000%\n",
      "4210\n",
      "Cost : 1.94435\n",
      "Accuracy : 50.00000%\n",
      "4220\n",
      "Cost : 1.89361\n",
      "Accuracy : 60.00000%\n",
      "4230\n",
      "Cost : 1.87728\n",
      "Accuracy : 50.00000%\n",
      "4240\n",
      "Cost : 1.91378\n",
      "Accuracy : 50.00000%\n",
      "4250\n",
      "Cost : 1.88725\n",
      "Accuracy : 60.00000%\n",
      "4260\n",
      "Cost : 1.90884\n",
      "Accuracy : 50.00000%\n",
      "4270\n",
      "Cost : 1.98763\n",
      "Accuracy : 40.00000%\n",
      "4280\n",
      "Cost : 1.73215\n",
      "Accuracy : 80.00000%\n",
      "4290\n",
      "Cost : 1.90522\n",
      "Accuracy : 50.00000%\n",
      "4300\n",
      "Cost : 2.08310\n",
      "Accuracy : 20.00000%\n",
      "4310\n",
      "Cost : 1.91760\n",
      "Accuracy : 50.00000%\n",
      "4320\n",
      "Cost : 2.01939\n",
      "Accuracy : 40.00000%\n",
      "4330\n",
      "Cost : 1.92750\n",
      "Accuracy : 50.00000%\n",
      "4340\n",
      "Cost : 1.91363\n",
      "Accuracy : 50.00000%\n",
      "4350\n",
      "Cost : 1.77178\n",
      "Accuracy : 70.00000%\n",
      "4360\n",
      "Cost : 1.86332\n",
      "Accuracy : 60.00000%\n",
      "4370\n",
      "Cost : 1.99077\n",
      "Accuracy : 30.00000%\n",
      "4380\n",
      "Cost : 1.90575\n",
      "Accuracy : 60.00000%\n",
      "4390\n",
      "Cost : 1.75045\n",
      "Accuracy : 70.00000%\n",
      "4400\n",
      "Cost : 1.89438\n",
      "Accuracy : 50.00000%\n",
      "4410\n",
      "Cost : 1.97238\n",
      "Accuracy : 50.00000%\n",
      "4420\n",
      "Cost : 1.85579\n",
      "Accuracy : 60.00000%\n",
      "4430\n",
      "Cost : 1.82338\n",
      "Accuracy : 60.00000%\n",
      "4440\n",
      "Cost : 1.87193\n",
      "Accuracy : 50.00000%\n",
      "4450\n",
      "Cost : 1.92567\n",
      "Accuracy : 50.00000%\n",
      "4460\n",
      "Cost : 2.07931\n",
      "Accuracy : 30.00000%\n",
      "4470\n",
      "Cost : 1.86092\n",
      "Accuracy : 40.00000%\n",
      "4480\n",
      "Cost : 2.07730\n",
      "Accuracy : 30.00000%\n",
      "4490\n",
      "Cost : 1.89682\n",
      "Accuracy : 50.00000%\n",
      "4500\n",
      "Cost : 2.00726\n",
      "Accuracy : 30.00000%\n",
      "4510\n",
      "Cost : 1.85774\n",
      "Accuracy : 40.00000%\n",
      "4520\n",
      "Cost : 1.99396\n",
      "Accuracy : 40.00000%\n",
      "4530\n",
      "Cost : 1.83575\n",
      "Accuracy : 80.00000%\n",
      "4540\n",
      "Cost : 1.94415\n",
      "Accuracy : 40.00000%\n",
      "4550\n",
      "Cost : 1.81528\n",
      "Accuracy : 60.00000%\n",
      "4560\n",
      "Cost : 1.82086\n",
      "Accuracy : 70.00000%\n",
      "4570\n",
      "Cost : 1.90294\n",
      "Accuracy : 60.00000%\n",
      "4580\n",
      "Cost : 1.89537\n",
      "Accuracy : 70.00000%\n",
      "4590\n",
      "Cost : 1.99674\n",
      "Accuracy : 40.00000%\n",
      "4600\n",
      "Cost : 1.96291\n",
      "Accuracy : 50.00000%\n",
      "4610\n",
      "Cost : 1.85309\n",
      "Accuracy : 70.00000%\n",
      "4620\n",
      "Cost : 1.86677\n",
      "Accuracy : 70.00000%\n",
      "4630\n",
      "Cost : 2.03668\n",
      "Accuracy : 40.00000%\n",
      "4640\n",
      "Cost : 1.91424\n",
      "Accuracy : 40.00000%\n",
      "4650\n",
      "Cost : 1.98625\n",
      "Accuracy : 50.00000%\n",
      "4660\n",
      "Cost : 2.09749\n",
      "Accuracy : 20.00000%\n",
      "4670\n",
      "Cost : 2.04132\n",
      "Accuracy : 30.00000%\n",
      "4680\n",
      "Cost : 2.01586\n",
      "Accuracy : 50.00000%\n",
      "4690\n",
      "Cost : 2.05960\n",
      "Accuracy : 30.00000%\n",
      "4700\n",
      "Cost : 1.99123\n",
      "Accuracy : 40.00000%\n",
      "4710\n",
      "Cost : 1.97263\n",
      "Accuracy : 40.00000%\n",
      "4720\n",
      "Cost : 1.96055\n",
      "Accuracy : 30.00000%\n",
      "4730\n",
      "Cost : 1.99405\n",
      "Accuracy : 40.00000%\n",
      "4740\n",
      "Cost : 2.03688\n",
      "Accuracy : 30.00000%\n",
      "4750\n",
      "Cost : 1.98954\n",
      "Accuracy : 40.00000%\n",
      "4760\n",
      "Cost : 2.08742\n",
      "Accuracy : 20.00000%\n",
      "4770\n",
      "Cost : 1.89105\n",
      "Accuracy : 60.00000%\n",
      "4780\n",
      "Cost : 2.03027\n",
      "Accuracy : 40.00000%\n",
      "4790\n",
      "Cost : 1.86121\n",
      "Accuracy : 70.00000%\n",
      "4800\n",
      "Cost : 1.98186\n",
      "Accuracy : 40.00000%\n",
      "4810\n",
      "Cost : 2.01947\n",
      "Accuracy : 30.00000%\n",
      "4820\n",
      "Cost : 1.98713\n",
      "Accuracy : 50.00000%\n",
      "4830\n",
      "Cost : 1.94858\n",
      "Accuracy : 50.00000%\n",
      "4840\n",
      "Cost : 2.02995\n",
      "Accuracy : 30.00000%\n",
      "4850\n",
      "Cost : 1.92062\n",
      "Accuracy : 60.00000%\n",
      "4860\n",
      "Cost : 1.97117\n",
      "Accuracy : 40.00000%\n",
      "4870\n",
      "Cost : 1.92712\n",
      "Accuracy : 60.00000%\n",
      "4880\n",
      "Cost : 1.84549\n",
      "Accuracy : 70.00000%\n",
      "4890\n",
      "Cost : 1.89555\n",
      "Accuracy : 60.00000%\n",
      "4900\n",
      "Cost : 1.94537\n",
      "Accuracy : 50.00000%\n",
      "4910\n",
      "Cost : 2.02929\n",
      "Accuracy : 40.00000%\n",
      "4920\n",
      "Cost : 1.82668\n",
      "Accuracy : 60.00000%\n",
      "4930\n",
      "Cost : 1.91389\n",
      "Accuracy : 70.00000%\n",
      "4940\n",
      "Cost : 1.98940\n",
      "Accuracy : 50.00000%\n",
      "4950\n",
      "Cost : 1.99561\n",
      "Accuracy : 50.00000%\n",
      "4960\n",
      "Cost : 2.05748\n",
      "Accuracy : 30.00000%\n",
      "4970\n",
      "Cost : 1.89175\n",
      "Accuracy : 50.00000%\n",
      "4980\n",
      "Cost : 2.09461\n",
      "Accuracy : 20.00000%\n",
      "4990\n",
      "Cost : 1.84343\n",
      "Accuracy : 60.00000%\n",
      "5000\n",
      "Cost : 1.86763\n",
      "Accuracy : 50.00000%\n",
      "5010\n",
      "Cost : 1.89579\n",
      "Accuracy : 50.00000%\n",
      "5020\n",
      "Cost : 1.82504\n",
      "Accuracy : 50.00000%\n",
      "5030\n",
      "Cost : 2.08237\n",
      "Accuracy : 30.00000%\n",
      "5040\n",
      "Cost : 1.92343\n",
      "Accuracy : 40.00000%\n",
      "5050\n",
      "Cost : 2.02009\n",
      "Accuracy : 40.00000%\n",
      "5060\n",
      "Cost : 2.13057\n",
      "Accuracy : 10.00000%\n",
      "5070\n",
      "Cost : 1.87145\n",
      "Accuracy : 60.00000%\n",
      "5080\n",
      "Cost : 2.06846\n",
      "Accuracy : 30.00000%\n",
      "5090\n",
      "Cost : 1.77792\n",
      "Accuracy : 70.00000%\n",
      "5100\n",
      "Cost : 1.93773\n",
      "Accuracy : 30.00000%\n",
      "5110\n",
      "Cost : 1.91343\n",
      "Accuracy : 40.00000%\n",
      "5120\n",
      "Cost : 1.82920\n",
      "Accuracy : 70.00000%\n",
      "5130\n",
      "Cost : 1.95368\n",
      "Accuracy : 40.00000%\n",
      "5140\n",
      "Cost : 1.90615\n",
      "Accuracy : 60.00000%\n",
      "5150\n",
      "Cost : 2.00724\n",
      "Accuracy : 20.00000%\n",
      "5160\n",
      "Cost : 2.08065\n",
      "Accuracy : 20.00000%\n",
      "5170\n",
      "Cost : 1.97345\n",
      "Accuracy : 30.00000%\n",
      "5180\n",
      "Cost : 1.91806\n",
      "Accuracy : 40.00000%\n",
      "5190\n",
      "Cost : 1.99956\n",
      "Accuracy : 40.00000%\n",
      "5200\n",
      "Cost : 1.92476\n",
      "Accuracy : 50.00000%\n",
      "5210\n",
      "Cost : 2.11497\n",
      "Accuracy : 20.00000%\n",
      "5220\n",
      "Cost : 1.96426\n",
      "Accuracy : 50.00000%\n",
      "5230\n",
      "Cost : 1.82266\n",
      "Accuracy : 60.00000%\n",
      "5240\n",
      "Cost : 2.01017\n",
      "Accuracy : 30.00000%\n",
      "5250\n",
      "Cost : 2.06434\n",
      "Accuracy : 0.00000%\n",
      "5260\n",
      "Cost : 1.73355\n",
      "Accuracy : 60.00000%\n",
      "5270\n",
      "Cost : 1.82373\n",
      "Accuracy : 60.00000%\n",
      "5280\n",
      "Cost : 1.92283\n",
      "Accuracy : 60.00000%\n",
      "5290\n",
      "Cost : 2.04214\n",
      "Accuracy : 40.00000%\n",
      "5300\n",
      "Cost : 2.09698\n",
      "Accuracy : 20.00000%\n",
      "5310\n",
      "Cost : 1.82999\n",
      "Accuracy : 60.00000%\n",
      "5320\n",
      "Cost : 1.88699\n",
      "Accuracy : 50.00000%\n",
      "5330\n",
      "Cost : 1.98541\n",
      "Accuracy : 30.00000%\n",
      "5340\n",
      "Cost : 1.89996\n",
      "Accuracy : 50.00000%\n",
      "5350\n",
      "Cost : 1.70288\n",
      "Accuracy : 80.00000%\n",
      "5360\n",
      "Cost : 1.96687\n",
      "Accuracy : 40.00000%\n",
      "5370\n",
      "Cost : 1.82269\n",
      "Accuracy : 60.00000%\n",
      "5380\n",
      "Cost : 1.88557\n",
      "Accuracy : 60.00000%\n",
      "5390\n",
      "Cost : 1.90493\n",
      "Accuracy : 50.00000%\n",
      "5400\n",
      "Cost : 1.91058\n",
      "Accuracy : 40.00000%\n",
      "5410\n",
      "Cost : 1.79848\n",
      "Accuracy : 60.00000%\n",
      "5420\n",
      "Cost : 2.03677\n",
      "Accuracy : 30.00000%\n",
      "5430\n",
      "Cost : 1.94265\n",
      "Accuracy : 30.00000%\n",
      "5440\n",
      "Cost : 1.88832\n",
      "Accuracy : 60.00000%\n",
      "5450\n",
      "Cost : 1.98946\n",
      "Accuracy : 30.00000%\n",
      "5460\n",
      "Cost : 2.00825\n",
      "Accuracy : 20.00000%\n",
      "5470\n",
      "Cost : 1.77898\n",
      "Accuracy : 60.00000%\n",
      "5480\n",
      "Cost : 1.80953\n",
      "Accuracy : 60.00000%\n",
      "5490\n",
      "Cost : 1.76482\n",
      "Accuracy : 70.00000%\n",
      "5500\n",
      "Cost : 1.80306\n",
      "Accuracy : 60.00000%\n",
      "5510\n",
      "Cost : 1.86234\n",
      "Accuracy : 60.00000%\n",
      "5520\n",
      "Cost : 1.83808\n",
      "Accuracy : 60.00000%\n",
      "5530\n",
      "Cost : 1.89522\n",
      "Accuracy : 50.00000%\n",
      "5540\n",
      "Cost : 1.80377\n",
      "Accuracy : 50.00000%\n",
      "5550\n",
      "Cost : 2.01059\n",
      "Accuracy : 30.00000%\n",
      "5560\n",
      "Cost : 1.77177\n",
      "Accuracy : 40.00000%\n",
      "5570\n",
      "Cost : 1.79538\n",
      "Accuracy : 60.00000%\n",
      "5580\n",
      "Cost : 1.68513\n",
      "Accuracy : 90.00000%\n",
      "5590\n",
      "Cost : 1.97576\n",
      "Accuracy : 20.00000%\n",
      "5600\n",
      "Cost : 1.77536\n",
      "Accuracy : 70.00000%\n",
      "5610\n",
      "Cost : 1.97631\n",
      "Accuracy : 30.00000%\n",
      "5620\n",
      "Cost : 1.97383\n",
      "Accuracy : 40.00000%\n",
      "5630\n",
      "Cost : 1.93620\n",
      "Accuracy : 50.00000%\n",
      "5640\n",
      "Cost : 1.85234\n",
      "Accuracy : 70.00000%\n",
      "5650\n",
      "Cost : 1.83228\n",
      "Accuracy : 70.00000%\n",
      "5660\n",
      "Cost : 1.93457\n",
      "Accuracy : 40.00000%\n",
      "5670\n",
      "Cost : 1.78384\n",
      "Accuracy : 90.00000%\n",
      "5680\n",
      "Cost : 1.89114\n",
      "Accuracy : 50.00000%\n",
      "5690\n",
      "Cost : 1.76806\n",
      "Accuracy : 70.00000%\n",
      "5700\n",
      "Cost : 1.89165\n",
      "Accuracy : 50.00000%\n",
      "5710\n",
      "Cost : 1.78162\n",
      "Accuracy : 60.00000%\n",
      "5720\n",
      "Cost : 1.90826\n",
      "Accuracy : 40.00000%\n",
      "5730\n",
      "Cost : 1.73508\n",
      "Accuracy : 60.00000%\n",
      "5740\n",
      "Cost : 1.96451\n",
      "Accuracy : 30.00000%\n",
      "5750\n",
      "Cost : 1.74863\n",
      "Accuracy : 80.00000%\n",
      "5760\n",
      "Cost : 1.83821\n",
      "Accuracy : 60.00000%\n",
      "5770\n",
      "Cost : 1.84960\n",
      "Accuracy : 50.00000%\n",
      "5780\n",
      "Cost : 1.82360\n",
      "Accuracy : 60.00000%\n",
      "5790\n",
      "Cost : 1.92840\n",
      "Accuracy : 50.00000%\n",
      "5800\n",
      "Cost : 1.87518\n",
      "Accuracy : 40.00000%\n",
      "5810\n",
      "Cost : 1.80121\n",
      "Accuracy : 50.00000%\n",
      "5820\n",
      "Cost : 1.75552\n",
      "Accuracy : 60.00000%\n",
      "5830\n",
      "Cost : 1.80068\n",
      "Accuracy : 60.00000%\n",
      "5840\n",
      "Cost : 1.73129\n",
      "Accuracy : 80.00000%\n",
      "5850\n",
      "Cost : 1.79097\n",
      "Accuracy : 70.00000%\n",
      "5860\n",
      "Cost : 1.96443\n",
      "Accuracy : 40.00000%\n",
      "5870\n",
      "Cost : 1.86241\n",
      "Accuracy : 50.00000%\n",
      "5880\n",
      "Cost : 1.71605\n",
      "Accuracy : 70.00000%\n",
      "5890\n",
      "Cost : 1.98534\n",
      "Accuracy : 30.00000%\n",
      "5900\n",
      "Cost : 1.91144\n",
      "Accuracy : 50.00000%\n",
      "5910\n",
      "Cost : 1.94720\n",
      "Accuracy : 20.00000%\n",
      "5920\n",
      "Cost : 1.88714\n",
      "Accuracy : 50.00000%\n",
      "5930\n",
      "Cost : 1.76328\n",
      "Accuracy : 70.00000%\n",
      "5940\n",
      "Cost : 1.87396\n",
      "Accuracy : 50.00000%\n",
      "5950\n",
      "Cost : 1.78111\n",
      "Accuracy : 70.00000%\n",
      "5960\n",
      "Cost : 1.74974\n",
      "Accuracy : 70.00000%\n",
      "5970\n",
      "Cost : 1.87150\n",
      "Accuracy : 60.00000%\n",
      "5980\n",
      "Cost : 1.76883\n",
      "Accuracy : 60.00000%\n",
      "5990\n",
      "Cost : 1.68015\n",
      "Accuracy : 80.00000%\n",
      "6000\n",
      "Cost : 1.80237\n",
      "Accuracy : 60.00000%\n",
      "6010\n",
      "Cost : 1.97202\n",
      "Accuracy : 20.00000%\n",
      "6020\n",
      "Cost : 1.71433\n",
      "Accuracy : 70.00000%\n",
      "6030\n",
      "Cost : 1.67159\n",
      "Accuracy : 80.00000%\n",
      "6040\n",
      "Cost : 1.83765\n",
      "Accuracy : 70.00000%\n",
      "6050\n",
      "Cost : 1.70953\n",
      "Accuracy : 60.00000%\n",
      "6060\n",
      "Cost : 1.81089\n",
      "Accuracy : 50.00000%\n",
      "6070\n",
      "Cost : 1.70945\n",
      "Accuracy : 70.00000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6080\n",
      "Cost : 1.66235\n",
      "Accuracy : 70.00000%\n",
      "6090\n",
      "Cost : 1.77788\n",
      "Accuracy : 60.00000%\n",
      "6100\n",
      "Cost : 1.95925\n",
      "Accuracy : 40.00000%\n",
      "6110\n",
      "Cost : 1.73988\n",
      "Accuracy : 60.00000%\n",
      "6120\n",
      "Cost : 1.77120\n",
      "Accuracy : 70.00000%\n",
      "6130\n",
      "Cost : 1.57836\n",
      "Accuracy : 90.00000%\n",
      "6140\n",
      "Cost : 1.58206\n",
      "Accuracy : 90.00000%\n",
      "6150\n",
      "Cost : 2.01788\n",
      "Accuracy : 30.00000%\n",
      "6160\n",
      "Cost : 1.84905\n",
      "Accuracy : 50.00000%\n",
      "6170\n",
      "Cost : 1.96824\n",
      "Accuracy : 50.00000%\n",
      "6180\n",
      "Cost : 1.81659\n",
      "Accuracy : 40.00000%\n",
      "6190\n",
      "Cost : 1.92179\n",
      "Accuracy : 40.00000%\n",
      "6200\n",
      "Cost : 1.75148\n",
      "Accuracy : 60.00000%\n",
      "6210\n",
      "Cost : 2.05157\n",
      "Accuracy : 20.00000%\n",
      "6220\n",
      "Cost : 1.75017\n",
      "Accuracy : 70.00000%\n",
      "6230\n",
      "Cost : 1.81722\n",
      "Accuracy : 50.00000%\n",
      "6240\n",
      "Cost : 1.66316\n",
      "Accuracy : 80.00000%\n",
      "6250\n",
      "Cost : 1.91976\n",
      "Accuracy : 50.00000%\n",
      "6260\n",
      "Cost : 1.85099\n",
      "Accuracy : 60.00000%\n",
      "6270\n",
      "Cost : 1.92643\n",
      "Accuracy : 50.00000%\n",
      "6280\n",
      "Cost : 1.76201\n",
      "Accuracy : 70.00000%\n",
      "6290\n",
      "Cost : 2.02098\n",
      "Accuracy : 30.00000%\n",
      "6300\n",
      "Cost : 1.95739\n",
      "Accuracy : 40.00000%\n",
      "6310\n",
      "Cost : 1.70783\n",
      "Accuracy : 60.00000%\n",
      "6320\n",
      "Cost : 1.90414\n",
      "Accuracy : 50.00000%\n",
      "6330\n",
      "Cost : 1.70268\n",
      "Accuracy : 60.00000%\n",
      "6340\n",
      "Cost : 1.92434\n",
      "Accuracy : 60.00000%\n",
      "6350\n",
      "Cost : 1.87531\n",
      "Accuracy : 50.00000%\n",
      "6360\n",
      "Cost : 1.84409\n",
      "Accuracy : 50.00000%\n",
      "6370\n",
      "Cost : 1.80123\n",
      "Accuracy : 60.00000%\n",
      "6380\n",
      "Cost : 1.85290\n",
      "Accuracy : 40.00000%\n",
      "6390\n",
      "Cost : 1.78267\n",
      "Accuracy : 60.00000%\n",
      "6400\n",
      "Cost : 1.69464\n",
      "Accuracy : 50.00000%\n",
      "6410\n",
      "Cost : 1.73315\n",
      "Accuracy : 60.00000%\n",
      "6420\n",
      "Cost : 1.77745\n",
      "Accuracy : 50.00000%\n",
      "6430\n",
      "Cost : 1.87632\n",
      "Accuracy : 60.00000%\n",
      "6440\n",
      "Cost : 1.59375\n",
      "Accuracy : 80.00000%\n",
      "6450\n",
      "Cost : 2.14628\n",
      "Accuracy : 0.00000%\n",
      "6460\n",
      "Cost : 1.82961\n",
      "Accuracy : 60.00000%\n",
      "6470\n",
      "Cost : 1.88119\n",
      "Accuracy : 60.00000%\n",
      "6480\n",
      "Cost : 1.65179\n",
      "Accuracy : 80.00000%\n",
      "6490\n",
      "Cost : 2.00413\n",
      "Accuracy : 40.00000%\n",
      "6500\n",
      "Cost : 1.86824\n",
      "Accuracy : 40.00000%\n",
      "6510\n",
      "Cost : 1.74403\n",
      "Accuracy : 70.00000%\n",
      "6520\n",
      "Cost : 1.75688\n",
      "Accuracy : 60.00000%\n",
      "6530\n",
      "Cost : 1.72128\n",
      "Accuracy : 50.00000%\n",
      "6540\n",
      "Cost : 1.71320\n",
      "Accuracy : 70.00000%\n",
      "6550\n",
      "Cost : 1.70529\n",
      "Accuracy : 70.00000%\n",
      "6560\n",
      "Cost : 1.74325\n",
      "Accuracy : 70.00000%\n",
      "6570\n",
      "Cost : 1.65378\n",
      "Accuracy : 70.00000%\n",
      "6580\n",
      "Cost : 1.82225\n",
      "Accuracy : 50.00000%\n",
      "6590\n",
      "Cost : 1.85046\n",
      "Accuracy : 50.00000%\n",
      "6600\n",
      "Cost : 1.86648\n",
      "Accuracy : 50.00000%\n",
      "6610\n",
      "Cost : 1.70326\n",
      "Accuracy : 80.00000%\n",
      "6620\n",
      "Cost : 1.72030\n",
      "Accuracy : 70.00000%\n",
      "6630\n",
      "Cost : 1.66398\n",
      "Accuracy : 80.00000%\n",
      "6640\n",
      "Cost : 1.85611\n",
      "Accuracy : 70.00000%\n",
      "6650\n",
      "Cost : 1.96726\n",
      "Accuracy : 50.00000%\n",
      "6660\n",
      "Cost : 1.91073\n",
      "Accuracy : 40.00000%\n",
      "6670\n",
      "Cost : 1.76784\n",
      "Accuracy : 70.00000%\n",
      "6680\n",
      "Cost : 1.79556\n",
      "Accuracy : 70.00000%\n",
      "6690\n",
      "Cost : 1.75908\n",
      "Accuracy : 60.00000%\n",
      "6700\n",
      "Cost : 1.76489\n",
      "Accuracy : 70.00000%\n",
      "6710\n",
      "Cost : 1.83008\n",
      "Accuracy : 60.00000%\n",
      "6720\n",
      "Cost : 1.73742\n",
      "Accuracy : 70.00000%\n",
      "6730\n",
      "Cost : 1.77457\n",
      "Accuracy : 70.00000%\n",
      "6740\n",
      "Cost : 1.88496\n",
      "Accuracy : 50.00000%\n",
      "6750\n",
      "Cost : 1.69014\n",
      "Accuracy : 80.00000%\n",
      "6760\n",
      "Cost : 1.92115\n",
      "Accuracy : 50.00000%\n",
      "6770\n",
      "Cost : 1.89306\n",
      "Accuracy : 50.00000%\n",
      "6780\n",
      "Cost : 1.96778\n",
      "Accuracy : 50.00000%\n",
      "6790\n",
      "Cost : 1.82958\n",
      "Accuracy : 70.00000%\n",
      "6800\n",
      "Cost : 1.84923\n",
      "Accuracy : 50.00000%\n",
      "6810\n",
      "Cost : 1.80901\n",
      "Accuracy : 60.00000%\n",
      "6820\n",
      "Cost : 1.84059\n",
      "Accuracy : 50.00000%\n",
      "6830\n",
      "Cost : 1.98991\n",
      "Accuracy : 40.00000%\n",
      "6840\n",
      "Cost : 1.97552\n",
      "Accuracy : 40.00000%\n",
      "6850\n",
      "Cost : 1.73683\n",
      "Accuracy : 60.00000%\n",
      "6860\n",
      "Cost : 1.91987\n",
      "Accuracy : 30.00000%\n",
      "6870\n",
      "Cost : 1.85390\n",
      "Accuracy : 50.00000%\n",
      "6880\n",
      "Cost : 1.93881\n",
      "Accuracy : 30.00000%\n",
      "6890\n",
      "Cost : 1.91306\n",
      "Accuracy : 60.00000%\n",
      "6900\n",
      "Cost : 1.78263\n",
      "Accuracy : 70.00000%\n",
      "6910\n",
      "Cost : 1.74996\n",
      "Accuracy : 80.00000%\n",
      "6920\n",
      "Cost : 1.92545\n",
      "Accuracy : 50.00000%\n",
      "6930\n",
      "Cost : 1.82542\n",
      "Accuracy : 70.00000%\n",
      "6940\n",
      "Cost : 2.05891\n",
      "Accuracy : 30.00000%\n",
      "6950\n",
      "Cost : 1.76807\n",
      "Accuracy : 70.00000%\n",
      "6960\n",
      "Cost : 1.77767\n",
      "Accuracy : 70.00000%\n",
      "6970\n",
      "Cost : 1.94798\n",
      "Accuracy : 40.00000%\n",
      "6980\n",
      "Cost : 1.95392\n",
      "Accuracy : 40.00000%\n",
      "6990\n",
      "Cost : 1.94859\n",
      "Accuracy : 30.00000%\n",
      "7000\n",
      "Cost : 1.81759\n",
      "Accuracy : 60.00000%\n",
      "7010\n",
      "Cost : 1.89860\n",
      "Accuracy : 50.00000%\n",
      "7020\n",
      "Cost : 1.89937\n",
      "Accuracy : 50.00000%\n",
      "7030\n",
      "Cost : 1.97224\n",
      "Accuracy : 40.00000%\n",
      "7040\n",
      "Cost : 1.95618\n",
      "Accuracy : 40.00000%\n",
      "7050\n",
      "Cost : 1.84647\n",
      "Accuracy : 60.00000%\n",
      "7060\n",
      "Cost : 1.84884\n",
      "Accuracy : 60.00000%\n",
      "7070\n",
      "Cost : 1.99858\n",
      "Accuracy : 20.00000%\n",
      "7080\n",
      "Cost : 1.80115\n",
      "Accuracy : 70.00000%\n",
      "7090\n",
      "Cost : 1.94872\n",
      "Accuracy : 40.00000%\n",
      "7100\n",
      "Cost : 1.88649\n",
      "Accuracy : 50.00000%\n",
      "7110\n",
      "Cost : 2.01378\n",
      "Accuracy : 40.00000%\n",
      "7120\n",
      "Cost : 2.07657\n",
      "Accuracy : 10.00000%\n",
      "7130\n",
      "Cost : 1.99543\n",
      "Accuracy : 60.00000%\n",
      "7140\n",
      "Cost : 2.03257\n",
      "Accuracy : 30.00000%\n",
      "7150\n",
      "Cost : 1.97861\n",
      "Accuracy : 30.00000%\n",
      "7160\n",
      "Cost : 1.97821\n",
      "Accuracy : 40.00000%\n",
      "7170\n",
      "Cost : 2.00267\n",
      "Accuracy : 30.00000%\n",
      "7180\n",
      "Cost : 1.87828\n",
      "Accuracy : 60.00000%\n",
      "7190\n",
      "Cost : 1.95991\n",
      "Accuracy : 40.00000%\n",
      "7200\n",
      "Cost : 1.88856\n",
      "Accuracy : 50.00000%\n",
      "7210\n",
      "Cost : 2.03927\n",
      "Accuracy : 30.00000%\n",
      "7220\n",
      "Cost : 1.90657\n",
      "Accuracy : 50.00000%\n",
      "7230\n",
      "Cost : 2.04020\n",
      "Accuracy : 30.00000%\n",
      "7240\n",
      "Cost : 2.09437\n",
      "Accuracy : 20.00000%\n",
      "7250\n",
      "Cost : 1.99238\n",
      "Accuracy : 40.00000%\n",
      "7260\n",
      "Cost : 1.90467\n",
      "Accuracy : 50.00000%\n",
      "7270\n",
      "Cost : 2.05362\n",
      "Accuracy : 40.00000%\n",
      "7280\n",
      "Cost : 2.08416\n",
      "Accuracy : 40.00000%\n",
      "7290\n",
      "Cost : 2.07524\n",
      "Accuracy : 30.00000%\n",
      "7300\n",
      "Cost : 2.09583\n",
      "Accuracy : 30.00000%\n",
      "7310\n",
      "Cost : 1.97520\n",
      "Accuracy : 50.00000%\n",
      "7320\n",
      "Cost : 1.99656\n",
      "Accuracy : 40.00000%\n",
      "7330\n",
      "Cost : 1.88806\n",
      "Accuracy : 60.00000%\n",
      "7340\n",
      "Cost : 1.87918\n",
      "Accuracy : 40.00000%\n",
      "7350\n",
      "Cost : 2.00213\n",
      "Accuracy : 50.00000%\n",
      "7360\n",
      "Cost : 1.86570\n",
      "Accuracy : 70.00000%\n",
      "7370\n",
      "Cost : 1.81524\n",
      "Accuracy : 70.00000%\n",
      "7380\n",
      "Cost : 2.08759\n",
      "Accuracy : 30.00000%\n",
      "7390\n",
      "Cost : 1.73168\n",
      "Accuracy : 80.00000%\n",
      "7400\n",
      "Cost : 1.88843\n",
      "Accuracy : 60.00000%\n",
      "7410\n",
      "Cost : 1.81866\n",
      "Accuracy : 70.00000%\n",
      "7420\n",
      "Cost : 1.81678\n",
      "Accuracy : 60.00000%\n",
      "7430\n",
      "Cost : 1.84122\n",
      "Accuracy : 50.00000%\n",
      "7440\n",
      "Cost : 2.02771\n",
      "Accuracy : 30.00000%\n",
      "7450\n",
      "Cost : 1.96572\n",
      "Accuracy : 40.00000%\n",
      "7460\n",
      "Cost : 1.83304\n",
      "Accuracy : 50.00000%\n",
      "7470\n",
      "Cost : 1.75090\n",
      "Accuracy : 70.00000%\n",
      "7480\n",
      "Cost : 1.92423\n",
      "Accuracy : 50.00000%\n",
      "7490\n",
      "Cost : 1.82211\n",
      "Accuracy : 60.00000%\n",
      "7500\n",
      "Cost : 1.76209\n",
      "Accuracy : 60.00000%\n",
      "7510\n",
      "Cost : 1.90363\n",
      "Accuracy : 50.00000%\n",
      "7520\n",
      "Cost : 1.71111\n",
      "Accuracy : 70.00000%\n",
      "7530\n",
      "Cost : 1.93901\n",
      "Accuracy : 50.00000%\n",
      "7540\n",
      "Cost : 1.91691\n",
      "Accuracy : 50.00000%\n",
      "7550\n",
      "Cost : 1.91827\n",
      "Accuracy : 50.00000%\n",
      "7560\n",
      "Cost : 1.67271\n",
      "Accuracy : 80.00000%\n",
      "7570\n",
      "Cost : 1.79066\n",
      "Accuracy : 80.00000%\n",
      "7580\n",
      "Cost : 1.82768\n",
      "Accuracy : 50.00000%\n",
      "7590\n",
      "Cost : 1.94861\n",
      "Accuracy : 40.00000%\n",
      "7600\n",
      "Cost : 1.79629\n",
      "Accuracy : 70.00000%\n",
      "7610\n",
      "Cost : 1.83172\n",
      "Accuracy : 70.00000%\n",
      "7620\n",
      "Cost : 1.93998\n",
      "Accuracy : 40.00000%\n",
      "7630\n",
      "Cost : 1.97850\n",
      "Accuracy : 50.00000%\n",
      "7640\n",
      "Cost : 2.07318\n",
      "Accuracy : 10.00000%\n",
      "7650\n",
      "Cost : 1.71882\n",
      "Accuracy : 80.00000%\n",
      "7660\n",
      "Cost : 1.90841\n",
      "Accuracy : 50.00000%\n",
      "7670\n",
      "Cost : 1.98486\n",
      "Accuracy : 30.00000%\n",
      "7680\n",
      "Cost : 1.85544\n",
      "Accuracy : 70.00000%\n",
      "7690\n",
      "Cost : 2.00058\n",
      "Accuracy : 50.00000%\n",
      "7700\n",
      "Cost : 1.74834\n",
      "Accuracy : 70.00000%\n",
      "7710\n",
      "Cost : 1.78839\n",
      "Accuracy : 60.00000%\n",
      "7720\n",
      "Cost : 1.87404\n",
      "Accuracy : 70.00000%\n",
      "7730\n",
      "Cost : 1.95635\n",
      "Accuracy : 50.00000%\n",
      "7740\n",
      "Cost : 1.83516\n",
      "Accuracy : 70.00000%\n",
      "7750\n",
      "Cost : 1.70393\n",
      "Accuracy : 80.00000%\n",
      "7760\n",
      "Cost : 1.99388\n",
      "Accuracy : 50.00000%\n",
      "7770\n",
      "Cost : 1.98326\n",
      "Accuracy : 40.00000%\n",
      "7780\n",
      "Cost : 1.84017\n",
      "Accuracy : 60.00000%\n",
      "7790\n",
      "Cost : 1.93213\n",
      "Accuracy : 40.00000%\n",
      "7800\n",
      "Cost : 1.90416\n",
      "Accuracy : 30.00000%\n",
      "7810\n",
      "Cost : 1.83398\n",
      "Accuracy : 50.00000%\n",
      "7820\n",
      "Cost : 1.73612\n",
      "Accuracy : 70.00000%\n",
      "7830\n",
      "Cost : 1.90185\n",
      "Accuracy : 50.00000%\n",
      "7840\n",
      "Cost : 1.84673\n",
      "Accuracy : 50.00000%\n",
      "7850\n",
      "Cost : 1.88720\n",
      "Accuracy : 60.00000%\n",
      "7860\n",
      "Cost : 1.87436\n",
      "Accuracy : 60.00000%\n",
      "7870\n",
      "Cost : 1.82361\n",
      "Accuracy : 60.00000%\n",
      "7880\n",
      "Cost : 1.94791\n",
      "Accuracy : 60.00000%\n",
      "7890\n",
      "Cost : 1.92396\n",
      "Accuracy : 40.00000%\n",
      "7900\n",
      "Cost : 2.10616\n",
      "Accuracy : 10.00000%\n",
      "7910\n",
      "Cost : 1.87839\n",
      "Accuracy : 70.00000%\n",
      "7920\n",
      "Cost : 2.01713\n",
      "Accuracy : 40.00000%\n",
      "7930\n",
      "Cost : 1.79415\n",
      "Accuracy : 60.00000%\n",
      "7940\n",
      "Cost : 1.79523\n",
      "Accuracy : 60.00000%\n",
      "7950\n",
      "Cost : 1.85433\n",
      "Accuracy : 60.00000%\n",
      "7960\n",
      "Cost : 1.98953\n",
      "Accuracy : 30.00000%\n",
      "7970\n",
      "Cost : 1.90926\n",
      "Accuracy : 60.00000%\n",
      "7980\n",
      "Cost : 1.96672\n",
      "Accuracy : 40.00000%\n",
      "7990\n",
      "Cost : 1.87316\n",
      "Accuracy : 60.00000%\n",
      "8000\n",
      "Cost : 1.76944\n",
      "Accuracy : 60.00000%\n",
      "8010\n",
      "Cost : 1.88917\n",
      "Accuracy : 60.00000%\n",
      "8020\n",
      "Cost : 1.97138\n",
      "Accuracy : 40.00000%\n",
      "8030\n",
      "Cost : 1.96693\n",
      "Accuracy : 40.00000%\n",
      "8040\n",
      "Cost : 1.84145\n",
      "Accuracy : 60.00000%\n",
      "8050\n",
      "Cost : 1.66729\n",
      "Accuracy : 80.00000%\n",
      "8060\n",
      "Cost : 1.84121\n",
      "Accuracy : 60.00000%\n",
      "8070\n",
      "Cost : 1.71955\n",
      "Accuracy : 80.00000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8080\n",
      "Cost : 1.66464\n",
      "Accuracy : 70.00000%\n",
      "8090\n",
      "Cost : 1.84533\n",
      "Accuracy : 60.00000%\n",
      "8100\n",
      "Cost : 1.82018\n",
      "Accuracy : 60.00000%\n",
      "8110\n",
      "Cost : 1.94585\n",
      "Accuracy : 60.00000%\n",
      "8120\n",
      "Cost : 2.02614\n",
      "Accuracy : 30.00000%\n",
      "8130\n",
      "Cost : 1.94124\n",
      "Accuracy : 40.00000%\n",
      "8140\n",
      "Cost : 1.74780\n",
      "Accuracy : 80.00000%\n",
      "8150\n",
      "Cost : 1.72098\n",
      "Accuracy : 60.00000%\n",
      "8160\n",
      "Cost : 1.65244\n",
      "Accuracy : 80.00000%\n",
      "8170\n",
      "Cost : 1.92124\n",
      "Accuracy : 50.00000%\n",
      "8180\n",
      "Cost : 1.83273\n",
      "Accuracy : 50.00000%\n",
      "8190\n",
      "Cost : 1.81219\n",
      "Accuracy : 60.00000%\n",
      "8200\n",
      "Cost : 1.88001\n",
      "Accuracy : 50.00000%\n",
      "8210\n",
      "Cost : 2.08271\n",
      "Accuracy : 20.00000%\n",
      "8220\n",
      "Cost : 1.91725\n",
      "Accuracy : 50.00000%\n",
      "8230\n",
      "Cost : 1.87667\n",
      "Accuracy : 50.00000%\n",
      "8240\n",
      "Cost : 1.70074\n",
      "Accuracy : 70.00000%\n",
      "8250\n",
      "Cost : 1.83336\n",
      "Accuracy : 60.00000%\n",
      "8260\n",
      "Cost : 1.93039\n",
      "Accuracy : 40.00000%\n",
      "8270\n",
      "Cost : 1.80827\n",
      "Accuracy : 70.00000%\n",
      "8280\n",
      "Cost : 1.99315\n",
      "Accuracy : 30.00000%\n",
      "8290\n",
      "Cost : 1.86336\n",
      "Accuracy : 40.00000%\n",
      "8300\n",
      "Cost : 1.90089\n",
      "Accuracy : 50.00000%\n",
      "8310\n",
      "Cost : 1.97424\n",
      "Accuracy : 30.00000%\n",
      "8320\n",
      "Cost : 1.86446\n",
      "Accuracy : 50.00000%\n",
      "8330\n",
      "Cost : 1.83360\n",
      "Accuracy : 60.00000%\n",
      "8340\n",
      "Cost : 2.01120\n",
      "Accuracy : 40.00000%\n",
      "8350\n",
      "Cost : 1.80048\n",
      "Accuracy : 60.00000%\n",
      "8360\n",
      "Cost : 1.71189\n",
      "Accuracy : 70.00000%\n",
      "8370\n",
      "Cost : 1.72722\n",
      "Accuracy : 50.00000%\n",
      "8380\n",
      "Cost : 1.76799\n",
      "Accuracy : 70.00000%\n",
      "8390\n",
      "Cost : 1.87295\n",
      "Accuracy : 60.00000%\n",
      "8400\n",
      "Cost : 1.91414\n",
      "Accuracy : 40.00000%\n",
      "8410\n",
      "Cost : 1.85991\n",
      "Accuracy : 50.00000%\n",
      "8420\n",
      "Cost : 1.92723\n",
      "Accuracy : 50.00000%\n",
      "8430\n",
      "Cost : 1.84223\n",
      "Accuracy : 60.00000%\n",
      "8440\n",
      "Cost : 1.98181\n",
      "Accuracy : 40.00000%\n",
      "8450\n",
      "Cost : 1.90850\n",
      "Accuracy : 40.00000%\n",
      "8460\n",
      "Cost : 1.75825\n",
      "Accuracy : 70.00000%\n",
      "8470\n",
      "Cost : 1.73732\n",
      "Accuracy : 70.00000%\n",
      "8480\n",
      "Cost : 1.84811\n",
      "Accuracy : 50.00000%\n",
      "8490\n",
      "Cost : 1.77089\n",
      "Accuracy : 70.00000%\n",
      "8500\n",
      "Cost : 1.80972\n",
      "Accuracy : 60.00000%\n",
      "8510\n",
      "Cost : 1.55038\n",
      "Accuracy : 100.00000%\n",
      "8520\n",
      "Cost : 1.83926\n",
      "Accuracy : 70.00000%\n",
      "8530\n",
      "Cost : 1.71637\n",
      "Accuracy : 70.00000%\n",
      "8540\n",
      "Cost : 1.64883\n",
      "Accuracy : 80.00000%\n",
      "8550\n",
      "Cost : 1.86367\n",
      "Accuracy : 40.00000%\n",
      "8560\n",
      "Cost : 1.72269\n",
      "Accuracy : 80.00000%\n",
      "8570\n",
      "Cost : 1.48814\n",
      "Accuracy : 100.00000%\n",
      "8580\n",
      "Cost : 1.93519\n",
      "Accuracy : 30.00000%\n",
      "8590\n",
      "Cost : 1.61211\n",
      "Accuracy : 100.00000%\n",
      "8600\n",
      "Cost : 1.76704\n",
      "Accuracy : 60.00000%\n",
      "8610\n",
      "Cost : 1.64735\n",
      "Accuracy : 80.00000%\n",
      "8620\n",
      "Cost : 1.74942\n",
      "Accuracy : 80.00000%\n",
      "8630\n",
      "Cost : 1.85848\n",
      "Accuracy : 40.00000%\n",
      "8640\n",
      "Cost : 1.81989\n",
      "Accuracy : 60.00000%\n",
      "8650\n",
      "Cost : 1.78285\n",
      "Accuracy : 70.00000%\n",
      "8660\n",
      "Cost : 1.83059\n",
      "Accuracy : 60.00000%\n",
      "8670\n",
      "Cost : 1.91941\n",
      "Accuracy : 50.00000%\n",
      "8680\n",
      "Cost : 1.67872\n",
      "Accuracy : 80.00000%\n",
      "8690\n",
      "Cost : 2.03911\n",
      "Accuracy : 30.00000%\n",
      "8700\n",
      "Cost : 2.05639\n",
      "Accuracy : 30.00000%\n",
      "8710\n",
      "Cost : 2.03644\n",
      "Accuracy : 20.00000%\n",
      "8720\n",
      "Cost : 2.03517\n",
      "Accuracy : 30.00000%\n",
      "8730\n",
      "Cost : 1.93435\n",
      "Accuracy : 40.00000%\n",
      "8740\n",
      "Cost : 2.00387\n",
      "Accuracy : 40.00000%\n",
      "8750\n",
      "Cost : 1.97008\n",
      "Accuracy : 40.00000%\n",
      "8760\n",
      "Cost : 1.86381\n",
      "Accuracy : 50.00000%\n",
      "8770\n",
      "Cost : 1.96511\n",
      "Accuracy : 40.00000%\n",
      "8780\n",
      "Cost : 1.94938\n",
      "Accuracy : 40.00000%\n",
      "8790\n",
      "Cost : 1.99911\n",
      "Accuracy : 50.00000%\n",
      "8800\n",
      "Cost : 1.78387\n",
      "Accuracy : 60.00000%\n",
      "8810\n",
      "Cost : 1.84095\n",
      "Accuracy : 50.00000%\n",
      "8820\n",
      "Cost : 1.78555\n",
      "Accuracy : 80.00000%\n",
      "8830\n",
      "Cost : 1.91799\n",
      "Accuracy : 40.00000%\n",
      "8840\n",
      "Cost : 1.94322\n",
      "Accuracy : 40.00000%\n",
      "8850\n",
      "Cost : 2.15635\n",
      "Accuracy : 10.00000%\n",
      "8860\n",
      "Cost : 2.18906\n",
      "Accuracy : 10.00000%\n",
      "8870\n",
      "Cost : 1.90191\n",
      "Accuracy : 60.00000%\n",
      "8880\n",
      "Cost : 1.95258\n",
      "Accuracy : 50.00000%\n",
      "8890\n",
      "Cost : 1.92593\n",
      "Accuracy : 50.00000%\n",
      "8900\n",
      "Cost : 1.85388\n",
      "Accuracy : 50.00000%\n",
      "8910\n",
      "Cost : 1.72876\n",
      "Accuracy : 80.00000%\n",
      "8920\n",
      "Cost : 1.93167\n",
      "Accuracy : 50.00000%\n",
      "8930\n",
      "Cost : 1.79814\n",
      "Accuracy : 70.00000%\n",
      "8940\n",
      "Cost : 1.71832\n",
      "Accuracy : 70.00000%\n",
      "8950\n",
      "Cost : 1.79379\n",
      "Accuracy : 70.00000%\n",
      "8960\n",
      "Cost : 1.87306\n",
      "Accuracy : 70.00000%\n",
      "8970\n",
      "Cost : 1.78795\n",
      "Accuracy : 70.00000%\n",
      "8980\n",
      "Cost : 1.80537\n",
      "Accuracy : 80.00000%\n",
      "8990\n",
      "Cost : 1.77499\n",
      "Accuracy : 70.00000%\n",
      "9000\n",
      "Cost : 1.92302\n",
      "Accuracy : 60.00000%\n",
      "9010\n",
      "Cost : 1.85435\n",
      "Accuracy : 60.00000%\n",
      "9020\n",
      "Cost : 1.83785\n",
      "Accuracy : 60.00000%\n",
      "9030\n",
      "Cost : 1.87587\n",
      "Accuracy : 40.00000%\n",
      "9040\n",
      "Cost : 1.86522\n",
      "Accuracy : 60.00000%\n",
      "9050\n",
      "Cost : 1.84511\n",
      "Accuracy : 50.00000%\n",
      "9060\n",
      "Cost : 1.79991\n",
      "Accuracy : 70.00000%\n",
      "9070\n",
      "Cost : 1.94878\n",
      "Accuracy : 30.00000%\n",
      "9080\n",
      "Cost : 1.83387\n",
      "Accuracy : 70.00000%\n",
      "9090\n",
      "Cost : 1.98560\n",
      "Accuracy : 30.00000%\n",
      "9100\n",
      "Cost : 1.72671\n",
      "Accuracy : 80.00000%\n",
      "9110\n",
      "Cost : 1.81628\n",
      "Accuracy : 50.00000%\n",
      "9120\n",
      "Cost : 1.81420\n",
      "Accuracy : 80.00000%\n",
      "9130\n",
      "Cost : 1.80149\n",
      "Accuracy : 60.00000%\n",
      "9140\n",
      "Cost : 1.97593\n",
      "Accuracy : 30.00000%\n",
      "9150\n",
      "Cost : 1.74142\n",
      "Accuracy : 70.00000%\n",
      "9160\n",
      "Cost : 1.72437\n",
      "Accuracy : 80.00000%\n",
      "9170\n",
      "Cost : 1.73333\n",
      "Accuracy : 70.00000%\n",
      "9180\n",
      "Cost : 1.76493\n",
      "Accuracy : 50.00000%\n",
      "9190\n",
      "Cost : 1.65139\n",
      "Accuracy : 80.00000%\n",
      "9200\n",
      "Cost : 1.69797\n",
      "Accuracy : 80.00000%\n",
      "9210\n",
      "Cost : 1.79429\n",
      "Accuracy : 50.00000%\n",
      "9220\n",
      "Cost : 1.98202\n",
      "Accuracy : 50.00000%\n",
      "9230\n",
      "Cost : 1.75912\n",
      "Accuracy : 60.00000%\n",
      "9240\n",
      "Cost : 1.79160\n",
      "Accuracy : 60.00000%\n",
      "9250\n",
      "Cost : 1.81786\n",
      "Accuracy : 60.00000%\n",
      "9260\n",
      "Cost : 1.74228\n",
      "Accuracy : 70.00000%\n",
      "9270\n",
      "Cost : 1.59178\n",
      "Accuracy : 100.00000%\n",
      "9280\n",
      "Cost : 1.70475\n",
      "Accuracy : 70.00000%\n",
      "9290\n",
      "Cost : 1.81193\n",
      "Accuracy : 60.00000%\n",
      "9300\n",
      "Cost : 1.83564\n",
      "Accuracy : 60.00000%\n",
      "9310\n",
      "Cost : 1.58568\n",
      "Accuracy : 90.00000%\n",
      "9320\n",
      "Cost : 1.76575\n",
      "Accuracy : 60.00000%\n",
      "9330\n",
      "Cost : 1.69037\n",
      "Accuracy : 80.00000%\n",
      "9340\n",
      "Cost : 1.91963\n",
      "Accuracy : 50.00000%\n",
      "9350\n",
      "Cost : 1.67291\n",
      "Accuracy : 80.00000%\n",
      "9360\n",
      "Cost : 1.64028\n",
      "Accuracy : 70.00000%\n",
      "9370\n",
      "Cost : 1.83834\n",
      "Accuracy : 50.00000%\n",
      "9380\n",
      "Cost : 1.74784\n",
      "Accuracy : 60.00000%\n",
      "9390\n",
      "Cost : 1.71195\n",
      "Accuracy : 70.00000%\n",
      "9400\n",
      "Cost : 1.75311\n",
      "Accuracy : 60.00000%\n",
      "9410\n",
      "Cost : 1.94114\n",
      "Accuracy : 30.00000%\n",
      "9420\n",
      "Cost : 1.64739\n",
      "Accuracy : 80.00000%\n",
      "9430\n",
      "Cost : 1.81463\n",
      "Accuracy : 60.00000%\n",
      "9440\n",
      "Cost : 1.88685\n",
      "Accuracy : 40.00000%\n",
      "9450\n",
      "Cost : 1.61502\n",
      "Accuracy : 90.00000%\n",
      "9460\n",
      "Cost : 1.78941\n",
      "Accuracy : 70.00000%\n",
      "9470\n",
      "Cost : 1.65356\n",
      "Accuracy : 80.00000%\n",
      "9480\n",
      "Cost : 1.62546\n",
      "Accuracy : 80.00000%\n",
      "9490\n",
      "Cost : 1.75059\n",
      "Accuracy : 60.00000%\n",
      "9500\n",
      "Cost : 1.81801\n",
      "Accuracy : 70.00000%\n",
      "9510\n",
      "Cost : 1.93736\n",
      "Accuracy : 40.00000%\n",
      "9520\n",
      "Cost : 1.69656\n",
      "Accuracy : 70.00000%\n",
      "9530\n",
      "Cost : 1.94674\n",
      "Accuracy : 50.00000%\n",
      "9540\n",
      "Cost : 1.71065\n",
      "Accuracy : 80.00000%\n",
      "9550\n",
      "Cost : 2.18862\n",
      "Accuracy : 10.00000%\n",
      "9560\n",
      "Cost : 1.92854\n",
      "Accuracy : 40.00000%\n",
      "9570\n",
      "Cost : 1.84137\n",
      "Accuracy : 60.00000%\n",
      "9580\n",
      "Cost : 1.79760\n",
      "Accuracy : 60.00000%\n",
      "9590\n",
      "Cost : 1.84153\n",
      "Accuracy : 60.00000%\n",
      "9600\n",
      "Cost : 1.88948\n",
      "Accuracy : 60.00000%\n",
      "9610\n",
      "Cost : 1.76908\n",
      "Accuracy : 70.00000%\n",
      "9620\n",
      "Cost : 1.82186\n",
      "Accuracy : 60.00000%\n",
      "9630\n",
      "Cost : 1.92646\n",
      "Accuracy : 40.00000%\n",
      "9640\n",
      "Cost : 1.95689\n",
      "Accuracy : 40.00000%\n",
      "9650\n",
      "Cost : 2.00635\n",
      "Accuracy : 40.00000%\n",
      "9660\n",
      "Cost : 1.75565\n",
      "Accuracy : 80.00000%\n",
      "9670\n",
      "Cost : 1.76797\n",
      "Accuracy : 80.00000%\n",
      "9680\n",
      "Cost : 1.71070\n",
      "Accuracy : 90.00000%\n",
      "9690\n",
      "Cost : 1.80104\n",
      "Accuracy : 70.00000%\n",
      "9700\n",
      "Cost : 1.72967\n",
      "Accuracy : 80.00000%\n",
      "9710\n",
      "Cost : 1.79045\n",
      "Accuracy : 80.00000%\n",
      "9720\n",
      "Cost : 1.59719\n",
      "Accuracy : 90.00000%\n",
      "9730\n",
      "Cost : 1.79644\n",
      "Accuracy : 70.00000%\n",
      "9740\n",
      "Cost : 1.75113\n",
      "Accuracy : 70.00000%\n",
      "9750\n",
      "Cost : 1.74965\n",
      "Accuracy : 70.00000%\n",
      "9760\n",
      "Cost : 1.88014\n",
      "Accuracy : 60.00000%\n",
      "9770\n",
      "Cost : 1.85388\n",
      "Accuracy : 60.00000%\n",
      "9780\n",
      "Cost : 1.75694\n",
      "Accuracy : 60.00000%\n",
      "9790\n",
      "Cost : 1.87437\n",
      "Accuracy : 50.00000%\n",
      "9800\n",
      "Cost : 1.76647\n",
      "Accuracy : 80.00000%\n",
      "9810\n",
      "Cost : 1.84501\n",
      "Accuracy : 70.00000%\n",
      "9820\n",
      "Cost : 1.69965\n",
      "Accuracy : 70.00000%\n",
      "9830\n",
      "Cost : 1.79150\n",
      "Accuracy : 70.00000%\n",
      "9840\n",
      "Cost : 1.79350\n",
      "Accuracy : 70.00000%\n",
      "9850\n",
      "Cost : 1.85303\n",
      "Accuracy : 70.00000%\n",
      "9860\n",
      "Cost : 1.84749\n",
      "Accuracy : 60.00000%\n",
      "9870\n",
      "Cost : 1.75097\n",
      "Accuracy : 70.00000%\n",
      "9880\n",
      "Cost : 1.71019\n",
      "Accuracy : 80.00000%\n",
      "9890\n",
      "Cost : 1.79071\n",
      "Accuracy : 80.00000%\n",
      "9900\n",
      "Cost : 1.66138\n",
      "Accuracy : 90.00000%\n",
      "9910\n",
      "Cost : 1.67885\n",
      "Accuracy : 80.00000%\n",
      "9920\n",
      "Cost : 1.76384\n",
      "Accuracy : 80.00000%\n",
      "9930\n",
      "Cost : 1.74496\n",
      "Accuracy : 60.00000%\n",
      "9940\n",
      "Cost : 1.76404\n",
      "Accuracy : 70.00000%\n",
      "9950\n",
      "Cost : 1.73166\n",
      "Accuracy : 70.00000%\n",
      "9960\n",
      "Cost : 1.71420\n",
      "Accuracy : 70.00000%\n",
      "9970\n",
      "Cost : 1.78468\n",
      "Accuracy : 70.00000%\n",
      "9980\n",
      "Cost : 1.74628\n",
      "Accuracy : 60.00000%\n",
      "9990\n",
      "Cost : 1.73567\n",
      "Accuracy : 70.00000%\n",
      "10000\n",
      "Cost : 1.59403\n",
      "Accuracy : 90.00000%\n",
      "10010\n",
      "Cost : 1.80387\n",
      "Accuracy : 60.00000%\n",
      "10020\n",
      "Cost : 1.89870\n",
      "Accuracy : 60.00000%\n",
      "10030\n",
      "Cost : 1.75845\n",
      "Accuracy : 60.00000%\n",
      "10040\n",
      "Cost : 1.84906\n",
      "Accuracy : 50.00000%\n",
      "10050\n",
      "Cost : 1.71038\n",
      "Accuracy : 70.00000%\n",
      "10060\n",
      "Cost : 1.89792\n",
      "Accuracy : 60.00000%\n",
      "10070\n",
      "Cost : 1.79267\n",
      "Accuracy : 50.00000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10080\n",
      "Cost : 1.60489\n",
      "Accuracy : 90.00000%\n",
      "10090\n",
      "Cost : 1.83558\n",
      "Accuracy : 60.00000%\n",
      "10100\n",
      "Cost : 1.74217\n",
      "Accuracy : 60.00000%\n",
      "10110\n",
      "Cost : 1.83956\n",
      "Accuracy : 60.00000%\n",
      "10120\n",
      "Cost : 1.66612\n",
      "Accuracy : 70.00000%\n",
      "10130\n",
      "Cost : 1.67374\n",
      "Accuracy : 70.00000%\n",
      "10140\n",
      "Cost : 1.69360\n",
      "Accuracy : 70.00000%\n",
      "10150\n",
      "Cost : 1.82857\n",
      "Accuracy : 60.00000%\n",
      "10160\n",
      "Cost : 1.81944\n",
      "Accuracy : 50.00000%\n",
      "10170\n",
      "Cost : 1.66210\n",
      "Accuracy : 50.00000%\n",
      "10180\n",
      "Cost : 1.80178\n",
      "Accuracy : 60.00000%\n",
      "10190\n",
      "Cost : 1.98987\n",
      "Accuracy : 30.00000%\n",
      "10200\n",
      "Cost : 1.85595\n",
      "Accuracy : 50.00000%\n",
      "10210\n",
      "Cost : 2.02905\n",
      "Accuracy : 20.00000%\n",
      "10220\n",
      "Cost : 1.84903\n",
      "Accuracy : 50.00000%\n",
      "10230\n",
      "Cost : 1.66110\n",
      "Accuracy : 90.00000%\n",
      "10240\n",
      "Cost : 2.01817\n",
      "Accuracy : 30.00000%\n",
      "10250\n",
      "Cost : 1.80418\n",
      "Accuracy : 50.00000%\n",
      "10260\n",
      "Cost : 1.83977\n",
      "Accuracy : 60.00000%\n",
      "10270\n",
      "Cost : 1.83599\n",
      "Accuracy : 70.00000%\n",
      "10280\n",
      "Cost : 1.95053\n",
      "Accuracy : 50.00000%\n",
      "10290\n",
      "Cost : 1.70197\n",
      "Accuracy : 80.00000%\n",
      "10300\n",
      "Cost : 1.69726\n",
      "Accuracy : 90.00000%\n",
      "10310\n",
      "Cost : 1.88752\n",
      "Accuracy : 50.00000%\n",
      "10320\n",
      "Cost : 1.73790\n",
      "Accuracy : 70.00000%\n",
      "10330\n",
      "Cost : 1.74541\n",
      "Accuracy : 90.00000%\n",
      "10340\n",
      "Cost : 1.77359\n",
      "Accuracy : 50.00000%\n",
      "10350\n",
      "Cost : 1.81698\n",
      "Accuracy : 60.00000%\n",
      "10360\n",
      "Cost : 1.76226\n",
      "Accuracy : 60.00000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-f05065ecce51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_ARCHITECTURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.05 stable LR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-fd6174900e12>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate, verbose, callback)\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mget_accuracy_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                 \u001b[0mconv_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_backward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcashe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mdf1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-61d1904879c8>\u001b[0m in \u001b[0;36mfull_backward_propagation\u001b[1;34m(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mconv_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mdconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutionBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_s\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mdconv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-6d7435cddcf8>\u001b[0m in \u001b[0;36mconvolutionBackward\u001b[1;34m(dconv_prev, conv_in, filt, s)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m#each entry of the dconv_prev will try to affect the idxs from which was made of.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mdfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mdconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#, axis =1) ## AXIS?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdconv_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfilt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, NN_ARCHITECTURE, 2, 0.05) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
