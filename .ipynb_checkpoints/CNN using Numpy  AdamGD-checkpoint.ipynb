{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron, MNIST\n",
    "---\n",
    "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data\n",
    "2. Define a neural network\n",
    "3. Train the model\n",
    "4. Evaluate the performance of our trained model on a test dataset!\n",
    "\n",
    "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Architecture ######\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 800, \"output_dim\": 512, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 512, \"output_dim\": 10, \"activation\": \"sigmoid\"} #Or relu again like the original example\n",
    "]#No Dropout...yet\n",
    "\n",
    "\n",
    "######  Init Layers  ######\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "\n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(image, params, s): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] #5x5\n",
    "    f_num = f.shape[0]\n",
    "    \n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] * f[z, :, :, :]) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "\n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X,filter_params ,params_values, nn_architecture, dropout):\n",
    "    \n",
    "    ######################## Forward Propagation Convolution Part  ##########################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    \n",
    "    params = [f1, b1]\n",
    "    conv1 = conv(X, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "    conv1[conv1<=0] = 0 #Relu\n",
    "    \n",
    "    params = [f2, b2]\n",
    "    conv2 = conv(conv1, params, 1)\n",
    "    conv2[conv2<=0] = 0 #Relu\n",
    "    \n",
    "    pl = maxpool(conv2, 2, 2) #pool_f = 2 , pool_s = 2\n",
    "    \n",
    "    #packet\n",
    "    conv_mem = [X, conv1, conv2, pl]\n",
    "    \n",
    "    num_c, f_dim, _ = pl.shape\n",
    "    fc1 = pl.reshape(num_c*f_dim*f_dim, 1) #Flattening\n",
    "\n",
    "    \n",
    "    ######################## Forward Propagation FC Part  ##########################\n",
    "    \n",
    "    \n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = fc1\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        if ((layer_idx == 1)and(dropout)):\n",
    "                ## Dropout ##\n",
    "                d = (np.random.rand(W_curr.shape[0],W_curr.shape[1])<0.5)\n",
    "                d = d*1 #Bool --> int(0s and 1s)\n",
    "                W_curr = d*W_curr\n",
    "                #############\n",
    "            \n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, conv_mem, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = []\n",
    "    #print(len(targets))\n",
    "    for i in range(len(targets)):\n",
    "        out.append(logs[i][targets[i]])\n",
    "    out = np.array(out)\n",
    "    \n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  BACK PROPAGATION  #######\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # calculation of the activation function derivative\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    \n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr \n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture):\n",
    "    \n",
    "    \n",
    "    ################# Backwardpropagation for FC Part  #######################\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    #print(Y.shape)\n",
    "    m = Y.shape[1]     # 1 sample each time\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = Y_hat - Y#- (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        \n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    ################# Backwardpropagation for Conv Part  #######################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    [image_in, conv1, conv2, pl]= conv_mem\n",
    "    #dA_prev\n",
    "    #Find dimensions of pooled image\n",
    "    #dim = int(np.sqrt(dA_prev.shape(0)/f2.shape(0))) #sqrt(800/8)=10 ==> 8*10*10\n",
    "    dpool = dA_prev.reshape(pl.shape) #, 1) \n",
    "    dconv2 = maxpoolBackward(dpool, conv2)  # , pool_f, pool_s)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    conv_s = 1\n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) #\n",
    "    dconv1[conv1<=0] = 0\n",
    "    \n",
    "    _, df1, db1 = convolutionBackward(dconv1, image_in, f1, conv_s)\n",
    "    \n",
    "    conv_grads = [df1, df2, db1, db2] \n",
    "    \n",
    "    return conv_grads, grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE ######\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    \n",
    "    ####### Building index labels from One-Hot matrix #######\n",
    "    temp = []\n",
    "    #train_labels = train_labels.sum(1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if(Y[i][j]==1):\n",
    "                temp.append(j)\n",
    "    #########################################################\n",
    "    \n",
    "    ##filter params\n",
    "    num_f1 = 8\n",
    "    num_f2 = 8\n",
    "    f_dim = 5\n",
    "    f1 = (num_f1, 1, f_dim, f_dim )\n",
    "    f2 = (num_f2, num_f1, f_dim, f_dim )\n",
    "    #To make for a smoother training process, we initialize each filter with a mean of 0 and a standard deviation of 1\n",
    "    scale = 1.0\n",
    "    #stddev = scale/np.sqrt(np.prod(f1))\n",
    "    trim = 0.1\n",
    "    f1 = np.random.randn( num_f1, 1, f_dim, f_dim) *trim\n",
    "    stddev = scale/np.sqrt(np.prod(f2))\n",
    "    f2 = np.random.randn(num_f2, num_f1, f_dim, f_dim ) *trim\n",
    "    b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "    b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "    #Packing Conv params\n",
    "    filter_params = []\n",
    "    filter_params = (f1, f2, b1, b2)\n",
    "\n",
    "    \n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 12\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            \n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.95\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            \n",
    "            df1 = np.zeros(f1.shape)\n",
    "            df2 = np.zeros(f2.shape)\n",
    "            dW1 = np.zeros(params_values['W1'].shape)\n",
    "            dW2 = np.zeros(params_values['W2'].shape)          \n",
    "            db1 = np.zeros(b1.shape)\n",
    "            db2 = np.zeros(b2.shape)\n",
    "            dB1 = np.zeros(params_values['b1'].shape)\n",
    "            dB2 = np.zeros(params_values['b2'].shape)\n",
    "            \n",
    "            v1  = np.zeros(f1.shape)\n",
    "            v2  = np.zeros(f2.shape)\n",
    "            v3  = np.zeros(params_values['W1'].shape)\n",
    "            v4  = np.zeros(params_values['W2'].shape)\n",
    "            bv1 = np.zeros(b1.shape)\n",
    "            bv2 = np.zeros(b2.shape)\n",
    "            bv3 = np.zeros(params_values['b1'].shape)\n",
    "            bv4 = np.zeros(params_values['b2'].shape)\n",
    "    \n",
    "            s1 = np.zeros(f1.shape)\n",
    "            s2 = np.zeros(f2.shape)\n",
    "            s3 = np.zeros(params_values['W1'].shape)\n",
    "            s4 = np.zeros(params_values['W2'].shape)\n",
    "            bs1 = np.zeros(b1.shape)\n",
    "            bs2 = np.zeros(b2.shape)\n",
    "            bs3 = np.zeros(params_values['b1'].shape)\n",
    "            bs4 = np.zeros(params_values['b2'].shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            Ys  = np.array(temp[c:(c + batch)]).reshape(batch,1) #shape (m,1), NOT one-hot\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                \n",
    "                # 1 image per time...for now\n",
    "                Y_hat, conv_mem, cashe = full_forward_propagation(X_t[b], filter_params, params_values, nn_architecture, dropout)\n",
    "            \n",
    "            \n",
    "                       \n",
    "                Yh = np.array(Y_hat.T)\n",
    "\n",
    "                ############### LogSoftMax  #################\n",
    "                #x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "                num = np.exp(Yh)\n",
    "                den = np.sum(np.exp(Yh), axis = 1)\n",
    "\n",
    "                for i in range(Yh.shape[0]): #1  , just 1 per time\n",
    "                    #for j in range(Yh.shape[1]): #10\n",
    "                    Yh[i][:] = np.log(num[i][:] / den[i])  \n",
    "                #############################################\n",
    "            \n",
    "                #print(Yh.shape)\n",
    "                #print(len(Ys))                \n",
    "                cost += NLLLoss(Yh, np.array(Ys[b]))  #(we need to send image(samples,classes), labels(NOT in one-hot) - Future optimizations needed)\n",
    "                #print(\"Cost: {%f}\" %(cost))\n",
    "                \n",
    "                \n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b].reshape(10,1))\n",
    "                \n",
    "                conv_grads, grads_values = full_backward_propagation(Y_hat, Y_t[b].reshape(Y_t[b].shape[0],1), conv_mem, cashe, filter_params, params_values, nn_architecture)\n",
    "                [df1_, df2_, db1_, db2_] = conv_grads\n",
    "\n",
    "                df1 += df1_\n",
    "                df2 += df2_\n",
    "                db1 += db1_\n",
    "                db2 += db2_\n",
    "\n",
    "                dW1 +=  grads_values['dW1']\n",
    "                dW2 +=  grads_values['dW2']\n",
    "                dB1 += grads_values['db1']\n",
    "                dB2 += grads_values['db2']\n",
    "            \n",
    "            w1 = params_values[\"W1\"]\n",
    "            w2 = params_values[\"W2\"]\n",
    "            B1 = params_values[\"b1\"]\n",
    "            B2 = params_values[\"b2\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            v1 = beta1*v1 + (1-beta1)*df1/batch_size # momentum update\n",
    "            s1 = beta2*s1 + (1-beta2)*(df1/batch_size)**2 # RMSProp update\n",
    "            f1 -= lr * v1/np.sqrt(s1+1e-7) # combine momentum and RMSProp to perform update with Adam\n",
    "\n",
    "            bv1 = beta1*bv1 + (1-beta1)*db1/batch_size\n",
    "            bs1 = beta2*bs1 + (1-beta2)*(db1/batch_size)**2\n",
    "            b1 -= lr * bv1/np.sqrt(bs1+1e-7)\n",
    "\n",
    "            v2 = beta1*v2 + (1-beta1)*df2/batch_size\n",
    "            s2 = beta2*s2 + (1-beta2)*(df2/batch_size)**2\n",
    "            f2 -= lr * v2/np.sqrt(s2+1e-7)\n",
    "\n",
    "            bv2 = beta1*bv2 + (1-beta1) * db2/batch_size\n",
    "            bs2 = beta2*bs2 + (1-beta2)*(db2/batch_size)**2\n",
    "            b2 -= lr * bv2/np.sqrt(bs2+1e-7)\n",
    "\n",
    "            v3 = beta1*v3 + (1-beta1) * dW1/batch_size\n",
    "            s3 = beta2*s3 + (1-beta2)*(dW1/batch_size)**2\n",
    "            w1 -= lr * v3/np.sqrt(s3+1e-7)\n",
    "\n",
    "            bv3 = beta1*bv3 + (1-beta1) * dB1/batch_size\n",
    "            bs3 = beta2*bs3 + (1-beta2)*(dB1/batch_size)**2\n",
    "            B1 -= lr * bv3/np.sqrt(bs3+1e-7)\n",
    "\n",
    "            v4 = beta1*v4 + (1-beta1) * dW2/batch_size\n",
    "            s4 = beta2*s4 + (1-beta2)*(dW2/batch_size)**2\n",
    "            w2 -= lr * v4 / np.sqrt(s4+1e-7)\n",
    "\n",
    "            bv4 = beta1*bv4 + (1-beta1)*dB2/batch_size\n",
    "            bs4 = beta2*bs4 + (1-beta2)*(dB2/batch_size)**2\n",
    "            B2 -= lr * bv4 / np.sqrt(bs4+1e-7)\n",
    "            \n",
    "            params_values[\"W1\"] = w1 \n",
    "            params_values[\"W2\"] = w2 \n",
    "            params_values[\"b1\"] = B1 \n",
    "            params_values[\"b2\"] = B2 \n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            print(c)\n",
    "            \n",
    "            '''\n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "0\n",
      "Cost : 2.33194\n",
      "Accuracy : 0.00000%\n",
      "12\n",
      "Cost : 2.30260\n",
      "Accuracy : 0.00000%\n",
      "24\n",
      "Cost : 2.30258\n",
      "Accuracy : 0.00000%\n",
      "36\n",
      "Cost : 2.30255\n",
      "Accuracy : 0.00000%\n",
      "48\n",
      "Cost : 2.30256\n",
      "Accuracy : 0.00000%\n",
      "60\n",
      "Cost : 2.30258\n",
      "Accuracy : 0.00000%\n",
      "72\n",
      "Cost : 2.30259\n",
      "Accuracy : 0.00000%\n",
      "84\n",
      "Cost : 2.30256\n",
      "Accuracy : 0.00000%\n",
      "96\n",
      "Cost : 2.30261\n",
      "Accuracy : 0.00000%\n",
      "108\n",
      "Cost : 2.30260\n",
      "Accuracy : 0.00000%\n",
      "120\n",
      "Cost : 2.30262\n",
      "Accuracy : 0.00000%\n",
      "132\n",
      "Cost : 2.30259\n",
      "Accuracy : 0.00000%\n",
      "144\n",
      "Cost : 2.30268\n",
      "Accuracy : 0.00000%\n",
      "156\n",
      "Cost : 2.30231\n",
      "Accuracy : 0.00000%\n",
      "168\n",
      "Cost : 2.30287\n",
      "Accuracy : 0.00000%\n",
      "180\n",
      "Cost : 2.30286\n",
      "Accuracy : 0.00000%\n",
      "192\n",
      "Cost : 2.29079\n",
      "Accuracy : 0.00000%\n",
      "204\n",
      "Cost : 2.29998\n",
      "Accuracy : 0.00000%\n",
      "216\n",
      "Cost : 2.30125\n",
      "Accuracy : 0.00000%\n",
      "228\n",
      "Cost : 2.32021\n",
      "Accuracy : 8.33333%\n",
      "240\n",
      "Cost : 2.30208\n",
      "Accuracy : 0.00000%\n",
      "252\n",
      "Cost : 2.30281\n",
      "Accuracy : 0.00000%\n",
      "264\n",
      "Cost : 2.31243\n",
      "Accuracy : 0.00000%\n",
      "276\n",
      "Cost : 2.30220\n",
      "Accuracy : 0.00000%\n",
      "288\n",
      "Cost : 2.28348\n",
      "Accuracy : 0.00000%\n",
      "300\n",
      "Cost : 2.32758\n",
      "Accuracy : 0.00000%\n",
      "312\n",
      "Cost : 2.30026\n",
      "Accuracy : 0.00000%\n",
      "324\n",
      "Cost : 2.31152\n",
      "Accuracy : 0.00000%\n",
      "336\n",
      "Cost : 2.30729\n",
      "Accuracy : 0.00000%\n",
      "348\n",
      "Cost : 2.29978\n",
      "Accuracy : 0.00000%\n",
      "360\n",
      "Cost : 2.28441\n",
      "Accuracy : 0.00000%\n",
      "372\n",
      "Cost : 2.27410\n",
      "Accuracy : 0.00000%\n",
      "384\n",
      "Cost : 2.31726\n",
      "Accuracy : 0.00000%\n",
      "396\n",
      "Cost : 2.30735\n",
      "Accuracy : 0.00000%\n",
      "408\n",
      "Cost : 2.27855\n",
      "Accuracy : 0.00000%\n",
      "420\n",
      "Cost : 2.31936\n",
      "Accuracy : 0.00000%\n",
      "432\n",
      "Cost : 2.29263\n",
      "Accuracy : 0.00000%\n",
      "444\n",
      "Cost : 2.30301\n",
      "Accuracy : 0.00000%\n",
      "456\n",
      "Cost : 2.29169\n",
      "Accuracy : 0.00000%\n",
      "468\n",
      "Cost : 2.29173\n",
      "Accuracy : 0.00000%\n",
      "480\n",
      "Cost : 2.29597\n",
      "Accuracy : 0.00000%\n",
      "492\n",
      "Cost : 2.30016\n",
      "Accuracy : 0.00000%\n",
      "504\n",
      "Cost : 2.28165\n",
      "Accuracy : 0.00000%\n",
      "516\n",
      "Cost : 2.29060\n",
      "Accuracy : 0.00000%\n",
      "528\n",
      "Cost : 2.31687\n",
      "Accuracy : 0.00000%\n",
      "540\n",
      "Cost : 2.29470\n",
      "Accuracy : 0.00000%\n",
      "552\n",
      "Cost : 2.27829\n",
      "Accuracy : 0.00000%\n",
      "564\n",
      "Cost : 2.25179\n",
      "Accuracy : 0.00000%\n",
      "576\n",
      "Cost : 2.28683\n",
      "Accuracy : 0.00000%\n",
      "588\n",
      "Cost : 2.27289\n",
      "Accuracy : 8.33333%\n",
      "600\n",
      "Cost : 2.24939\n",
      "Accuracy : 0.00000%\n",
      "612\n",
      "Cost : 2.25928\n",
      "Accuracy : 0.00000%\n",
      "624\n",
      "Cost : 2.25046\n",
      "Accuracy : 0.00000%\n",
      "636\n",
      "Cost : 2.21095\n",
      "Accuracy : 8.33333%\n",
      "648\n",
      "Cost : 2.16718\n",
      "Accuracy : 25.00000%\n",
      "660\n",
      "Cost : 2.24224\n",
      "Accuracy : 8.33333%\n",
      "672\n",
      "Cost : 2.27124\n",
      "Accuracy : 8.33333%\n",
      "684\n",
      "Cost : 2.24399\n",
      "Accuracy : 8.33333%\n",
      "696\n",
      "Cost : 2.25343\n",
      "Accuracy : 8.33333%\n",
      "708\n",
      "Cost : 2.16007\n",
      "Accuracy : 8.33333%\n",
      "720\n",
      "Cost : 2.32939\n",
      "Accuracy : 0.00000%\n",
      "732\n",
      "Cost : 2.23845\n",
      "Accuracy : 0.00000%\n",
      "744\n",
      "Cost : 2.23244\n",
      "Accuracy : 16.66667%\n",
      "756\n",
      "Cost : 2.24845\n",
      "Accuracy : 0.00000%\n",
      "768\n",
      "Cost : 2.23437\n",
      "Accuracy : 8.33333%\n",
      "780\n",
      "Cost : 2.08305\n",
      "Accuracy : 16.66667%\n",
      "792\n",
      "Cost : 2.14429\n",
      "Accuracy : 8.33333%\n",
      "804\n",
      "Cost : 2.22437\n",
      "Accuracy : 8.33333%\n",
      "816\n",
      "Cost : 2.12319\n",
      "Accuracy : 8.33333%\n",
      "828\n",
      "Cost : 2.09467\n",
      "Accuracy : 33.33333%\n",
      "840\n",
      "Cost : 2.22820\n",
      "Accuracy : 0.00000%\n",
      "852\n",
      "Cost : 2.09037\n",
      "Accuracy : 33.33333%\n",
      "864\n",
      "Cost : 1.98015\n",
      "Accuracy : 41.66667%\n",
      "876\n",
      "Cost : 2.16287\n",
      "Accuracy : 16.66667%\n",
      "888\n",
      "Cost : 2.14598\n",
      "Accuracy : 8.33333%\n",
      "900\n",
      "Cost : 1.94090\n",
      "Accuracy : 41.66667%\n",
      "912\n",
      "Cost : 2.12127\n",
      "Accuracy : 25.00000%\n",
      "924\n",
      "Cost : 2.10711\n",
      "Accuracy : 25.00000%\n",
      "936\n",
      "Cost : 2.11696\n",
      "Accuracy : 25.00000%\n",
      "948\n",
      "Cost : 1.91863\n",
      "Accuracy : 41.66667%\n",
      "960\n",
      "Cost : 1.90665\n",
      "Accuracy : 50.00000%\n",
      "972\n",
      "Cost : 1.97610\n",
      "Accuracy : 25.00000%\n",
      "984\n",
      "Cost : 2.07565\n",
      "Accuracy : 25.00000%\n",
      "996\n",
      "Cost : 2.13112\n",
      "Accuracy : 33.33333%\n",
      "1008\n",
      "Cost : 2.14555\n",
      "Accuracy : 25.00000%\n",
      "1020\n",
      "Cost : 2.07277\n",
      "Accuracy : 25.00000%\n",
      "1032\n",
      "Cost : 2.17934\n",
      "Accuracy : 25.00000%\n",
      "1044\n",
      "Cost : 2.21452\n",
      "Accuracy : 0.00000%\n",
      "1056\n",
      "Cost : 2.25084\n",
      "Accuracy : 16.66667%\n",
      "1068\n",
      "Cost : 2.27392\n",
      "Accuracy : 0.00000%\n",
      "1080\n",
      "Cost : 2.24777\n",
      "Accuracy : 0.00000%\n",
      "1092\n",
      "Cost : 2.30348\n",
      "Accuracy : 8.33333%\n",
      "1104\n",
      "Cost : 2.18663\n",
      "Accuracy : 8.33333%\n",
      "1116\n",
      "Cost : 2.19023\n",
      "Accuracy : 8.33333%\n",
      "1128\n",
      "Cost : 2.25800\n",
      "Accuracy : 16.66667%\n",
      "1140\n",
      "Cost : 2.21124\n",
      "Accuracy : 0.00000%\n",
      "1152\n",
      "Cost : 2.23020\n",
      "Accuracy : 8.33333%\n",
      "1164\n",
      "Cost : 2.12097\n",
      "Accuracy : 8.33333%\n",
      "1176\n",
      "Cost : 2.14478\n",
      "Accuracy : 0.00000%\n",
      "1188\n",
      "Cost : 2.17810\n",
      "Accuracy : 25.00000%\n",
      "1200\n",
      "Cost : 2.07611\n",
      "Accuracy : 16.66667%\n",
      "1212\n",
      "Cost : 2.15010\n",
      "Accuracy : 16.66667%\n",
      "1224\n",
      "Cost : 2.15114\n",
      "Accuracy : 25.00000%\n",
      "1236\n",
      "Cost : 2.15349\n",
      "Accuracy : 25.00000%\n",
      "1248\n",
      "Cost : 2.06211\n",
      "Accuracy : 25.00000%\n",
      "1260\n",
      "Cost : 2.15463\n",
      "Accuracy : 16.66667%\n",
      "1272\n",
      "Cost : 2.09840\n",
      "Accuracy : 33.33333%\n",
      "1284\n",
      "Cost : 2.12463\n",
      "Accuracy : 8.33333%\n",
      "1296\n",
      "Cost : 2.27049\n",
      "Accuracy : 8.33333%\n",
      "1308\n",
      "Cost : 2.23717\n",
      "Accuracy : 8.33333%\n",
      "1320\n",
      "Cost : 2.12568\n",
      "Accuracy : 25.00000%\n",
      "1332\n",
      "Cost : 2.04463\n",
      "Accuracy : 33.33333%\n",
      "1344\n",
      "Cost : 2.16741\n",
      "Accuracy : 25.00000%\n",
      "1356\n",
      "Cost : 2.27395\n",
      "Accuracy : 0.00000%\n",
      "1368\n",
      "Cost : 2.17799\n",
      "Accuracy : 0.00000%\n",
      "1380\n",
      "Cost : 2.24887\n",
      "Accuracy : 8.33333%\n",
      "1392\n",
      "Cost : 2.15543\n",
      "Accuracy : 16.66667%\n",
      "1404\n",
      "Cost : 2.24656\n",
      "Accuracy : 8.33333%\n",
      "1416\n",
      "Cost : 2.17227\n",
      "Accuracy : 25.00000%\n",
      "1428\n",
      "Cost : 2.29433\n",
      "Accuracy : 0.00000%\n",
      "1440\n",
      "Cost : 2.27392\n",
      "Accuracy : 16.66667%\n",
      "1452\n",
      "Cost : 2.28618\n",
      "Accuracy : 0.00000%\n",
      "1464\n",
      "Cost : 2.26904\n",
      "Accuracy : 8.33333%\n",
      "1476\n",
      "Cost : 2.31085\n",
      "Accuracy : 0.00000%\n",
      "1488\n",
      "Cost : 2.22988\n",
      "Accuracy : 8.33333%\n",
      "1500\n",
      "Cost : 2.14433\n",
      "Accuracy : 16.66667%\n",
      "1512\n",
      "Cost : 2.17158\n",
      "Accuracy : 16.66667%\n",
      "1524\n",
      "Cost : 2.24149\n",
      "Accuracy : 8.33333%\n",
      "1536\n",
      "Cost : 2.23087\n",
      "Accuracy : 8.33333%\n",
      "1548\n",
      "Cost : 2.17999\n",
      "Accuracy : 16.66667%\n",
      "1560\n",
      "Cost : 2.16897\n",
      "Accuracy : 16.66667%\n",
      "1572\n",
      "Cost : 2.17639\n",
      "Accuracy : 25.00000%\n",
      "1584\n",
      "Cost : 2.20589\n",
      "Accuracy : 8.33333%\n",
      "1596\n",
      "Cost : 2.04986\n",
      "Accuracy : 25.00000%\n",
      "1608\n",
      "Cost : 2.08586\n",
      "Accuracy : 33.33333%\n",
      "1620\n",
      "Cost : 2.08518\n",
      "Accuracy : 16.66667%\n",
      "1632\n",
      "Cost : 2.04481\n",
      "Accuracy : 25.00000%\n",
      "1644\n",
      "Cost : 2.06237\n",
      "Accuracy : 33.33333%\n",
      "1656\n",
      "Cost : 2.07658\n",
      "Accuracy : 25.00000%\n",
      "1668\n",
      "Cost : 1.93888\n",
      "Accuracy : 41.66667%\n",
      "1680\n",
      "Cost : 2.04740\n",
      "Accuracy : 33.33333%\n",
      "1692\n",
      "Cost : 1.79845\n",
      "Accuracy : 83.33333%\n",
      "1704\n",
      "Cost : 1.99986\n",
      "Accuracy : 33.33333%\n",
      "1716\n",
      "Cost : 1.87296\n",
      "Accuracy : 41.66667%\n",
      "1728\n",
      "Cost : 1.92012\n",
      "Accuracy : 41.66667%\n",
      "1740\n",
      "Cost : 1.92308\n",
      "Accuracy : 50.00000%\n",
      "1752\n",
      "Cost : 1.91120\n",
      "Accuracy : 50.00000%\n",
      "1764\n",
      "Cost : 2.30589\n",
      "Accuracy : 8.33333%\n",
      "1776\n",
      "Cost : 2.08347\n",
      "Accuracy : 25.00000%\n",
      "1788\n",
      "Cost : 1.94503\n",
      "Accuracy : 50.00000%\n",
      "1800\n",
      "Cost : 1.97084\n",
      "Accuracy : 41.66667%\n",
      "1812\n",
      "Cost : 2.16990\n",
      "Accuracy : 16.66667%\n",
      "1824\n",
      "Cost : 2.02867\n",
      "Accuracy : 25.00000%\n",
      "1836\n",
      "Cost : 1.91103\n",
      "Accuracy : 25.00000%\n",
      "1848\n",
      "Cost : 2.06455\n",
      "Accuracy : 25.00000%\n",
      "1860\n",
      "Cost : 2.02487\n",
      "Accuracy : 33.33333%\n",
      "1872\n",
      "Cost : 2.14373\n",
      "Accuracy : 25.00000%\n",
      "1884\n",
      "Cost : 2.24039\n",
      "Accuracy : 8.33333%\n",
      "1896\n",
      "Cost : 2.08776\n",
      "Accuracy : 25.00000%\n",
      "1908\n",
      "Cost : 2.04671\n",
      "Accuracy : 33.33333%\n",
      "1920\n",
      "Cost : 2.12076\n",
      "Accuracy : 25.00000%\n",
      "1932\n",
      "Cost : 2.28760\n",
      "Accuracy : 0.00000%\n",
      "1944\n",
      "Cost : 2.33089\n",
      "Accuracy : 8.33333%\n",
      "1956\n",
      "Cost : 2.30568\n",
      "Accuracy : 0.00000%\n",
      "1968\n",
      "Cost : 2.28682\n",
      "Accuracy : 8.33333%\n",
      "1980\n",
      "Cost : 2.14560\n",
      "Accuracy : 16.66667%\n",
      "1992\n",
      "Cost : 2.21603\n",
      "Accuracy : 0.00000%\n",
      "2004\n",
      "Cost : 2.11552\n",
      "Accuracy : 25.00000%\n",
      "2016\n",
      "Cost : 2.13128\n",
      "Accuracy : 25.00000%\n",
      "2028\n",
      "Cost : 2.25557\n",
      "Accuracy : 16.66667%\n",
      "2040\n",
      "Cost : 2.07362\n",
      "Accuracy : 33.33333%\n",
      "2052\n",
      "Cost : 2.09521\n",
      "Accuracy : 25.00000%\n",
      "2064\n",
      "Cost : 2.30387\n",
      "Accuracy : 0.00000%\n",
      "2076\n",
      "Cost : 2.30255\n",
      "Accuracy : 0.00000%\n",
      "2088\n",
      "Cost : 2.16531\n",
      "Accuracy : 16.66667%\n",
      "2100\n",
      "Cost : 2.23200\n",
      "Accuracy : 8.33333%\n",
      "2112\n",
      "Cost : 2.14594\n",
      "Accuracy : 16.66667%\n",
      "2124\n",
      "Cost : 2.26378\n",
      "Accuracy : 8.33333%\n",
      "2136\n",
      "Cost : 2.17556\n",
      "Accuracy : 16.66667%\n",
      "2148\n",
      "Cost : 1.86211\n",
      "Accuracy : 50.00000%\n",
      "2160\n",
      "Cost : 2.05959\n",
      "Accuracy : 33.33333%\n",
      "2172\n",
      "Cost : 2.17390\n",
      "Accuracy : 16.66667%\n",
      "2184\n",
      "Cost : 2.09802\n",
      "Accuracy : 25.00000%\n",
      "2196\n",
      "Cost : 2.13580\n",
      "Accuracy : 16.66667%\n",
      "2208\n",
      "Cost : 2.10782\n",
      "Accuracy : 25.00000%\n",
      "2220\n",
      "Cost : 1.96266\n",
      "Accuracy : 41.66667%\n",
      "2232\n",
      "Cost : 1.91236\n",
      "Accuracy : 50.00000%\n",
      "2244\n",
      "Cost : 1.92271\n",
      "Accuracy : 41.66667%\n",
      "2256\n",
      "Cost : 2.03373\n",
      "Accuracy : 41.66667%\n",
      "2268\n",
      "Cost : 1.94966\n",
      "Accuracy : 41.66667%\n",
      "2280\n",
      "Cost : 2.03718\n",
      "Accuracy : 25.00000%\n",
      "2292\n",
      "Cost : 1.84980\n",
      "Accuracy : 58.33333%\n",
      "2304\n",
      "Cost : 1.93125\n",
      "Accuracy : 33.33333%\n",
      "2316\n",
      "Cost : 2.09035\n",
      "Accuracy : 25.00000%\n",
      "2328\n",
      "Cost : 2.09515\n",
      "Accuracy : 25.00000%\n",
      "2340\n",
      "Cost : 2.14066\n",
      "Accuracy : 25.00000%\n",
      "2352\n",
      "Cost : 1.99931\n",
      "Accuracy : 25.00000%\n",
      "2364\n",
      "Cost : 2.05995\n",
      "Accuracy : 16.66667%\n",
      "2376\n",
      "Cost : 2.17509\n",
      "Accuracy : 8.33333%\n",
      "2388\n",
      "Cost : 2.08433\n",
      "Accuracy : 25.00000%\n",
      "2400\n",
      "Cost : 2.04266\n",
      "Accuracy : 25.00000%\n",
      "2412\n",
      "Cost : 2.09226\n",
      "Accuracy : 33.33333%\n",
      "2424\n",
      "Cost : 2.06533\n",
      "Accuracy : 16.66667%\n",
      "2436\n",
      "Cost : 2.16744\n",
      "Accuracy : 16.66667%\n",
      "2448\n",
      "Cost : 2.18553\n",
      "Accuracy : 8.33333%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2460\n",
      "Cost : 2.13447\n",
      "Accuracy : 8.33333%\n",
      "2472\n",
      "Cost : 1.89953\n",
      "Accuracy : 50.00000%\n",
      "2484\n",
      "Cost : 2.04527\n",
      "Accuracy : 33.33333%\n",
      "2496\n",
      "Cost : 1.97349\n",
      "Accuracy : 41.66667%\n",
      "2508\n",
      "Cost : 1.87428\n",
      "Accuracy : 58.33333%\n",
      "2520\n",
      "Cost : 2.11772\n",
      "Accuracy : 16.66667%\n",
      "2532\n",
      "Cost : 2.09419\n",
      "Accuracy : 25.00000%\n",
      "2544\n",
      "Cost : 2.33473\n",
      "Accuracy : 0.00000%\n",
      "2556\n",
      "Cost : 2.27270\n",
      "Accuracy : 0.00000%\n",
      "2568\n",
      "Cost : 2.32330\n",
      "Accuracy : 0.00000%\n",
      "2580\n",
      "Cost : 2.30278\n",
      "Accuracy : 0.00000%\n",
      "2592\n",
      "Cost : 2.30691\n",
      "Accuracy : 0.00000%\n",
      "2604\n",
      "Cost : 2.30755\n",
      "Accuracy : 0.00000%\n",
      "2616\n",
      "Cost : 2.29244\n",
      "Accuracy : 0.00000%\n",
      "2628\n",
      "Cost : 2.31780\n",
      "Accuracy : 0.00000%\n",
      "2640\n",
      "Cost : 2.31016\n",
      "Accuracy : 0.00000%\n",
      "2652\n",
      "Cost : 2.30722\n",
      "Accuracy : 0.00000%\n",
      "2664\n",
      "Cost : 2.31377\n",
      "Accuracy : 0.00000%\n",
      "2676\n",
      "Cost : 2.32432\n",
      "Accuracy : 0.00000%\n",
      "2688\n",
      "Cost : 2.30231\n",
      "Accuracy : 0.00000%\n",
      "2700\n",
      "Cost : 2.30458\n",
      "Accuracy : 0.00000%\n",
      "2712\n",
      "Cost : 2.31840\n",
      "Accuracy : 0.00000%\n",
      "2724\n",
      "Cost : 2.28814\n",
      "Accuracy : 0.00000%\n",
      "2736\n",
      "Cost : 2.31371\n",
      "Accuracy : 0.00000%\n",
      "2748\n",
      "Cost : 2.30587\n",
      "Accuracy : 0.00000%\n",
      "2760\n",
      "Cost : 2.32415\n",
      "Accuracy : 0.00000%\n",
      "2772\n",
      "Cost : 2.30618\n",
      "Accuracy : 0.00000%\n",
      "2784\n",
      "Cost : 2.30148\n",
      "Accuracy : 0.00000%\n",
      "2796\n",
      "Cost : 2.29316\n",
      "Accuracy : 0.00000%\n",
      "2808\n",
      "Cost : 2.29219\n",
      "Accuracy : 0.00000%\n",
      "2820\n",
      "Cost : 2.29515\n",
      "Accuracy : 0.00000%\n",
      "2832\n",
      "Cost : 2.30442\n",
      "Accuracy : 0.00000%\n",
      "2844\n",
      "Cost : 2.27862\n",
      "Accuracy : 0.00000%\n",
      "2856\n",
      "Cost : 2.31020\n",
      "Accuracy : 0.00000%\n",
      "2868\n",
      "Cost : 2.31203\n",
      "Accuracy : 0.00000%\n",
      "2880\n",
      "Cost : 2.30552\n",
      "Accuracy : 0.00000%\n",
      "2892\n",
      "Cost : 2.28637\n",
      "Accuracy : 0.00000%\n",
      "2904\n",
      "Cost : 2.31649\n",
      "Accuracy : 0.00000%\n",
      "2916\n",
      "Cost : 2.29216\n",
      "Accuracy : 0.00000%\n",
      "2928\n",
      "Cost : 2.28338\n",
      "Accuracy : 0.00000%\n",
      "2940\n",
      "Cost : 2.32255\n",
      "Accuracy : 0.00000%\n",
      "2952\n",
      "Cost : 2.33057\n",
      "Accuracy : 0.00000%\n",
      "2964\n",
      "Cost : 2.30215\n",
      "Accuracy : 0.00000%\n",
      "2976\n",
      "Cost : 2.33565\n",
      "Accuracy : 0.00000%\n",
      "2988\n",
      "Cost : 2.30589\n",
      "Accuracy : 0.00000%\n",
      "3000\n",
      "Cost : 2.30607\n",
      "Accuracy : 0.00000%\n",
      "3012\n",
      "Cost : 2.30873\n",
      "Accuracy : 0.00000%\n",
      "3024\n",
      "Cost : 2.29374\n",
      "Accuracy : 0.00000%\n",
      "3036\n",
      "Cost : 2.32294\n",
      "Accuracy : 0.00000%\n",
      "3048\n",
      "Cost : 2.29524\n",
      "Accuracy : 0.00000%\n",
      "3060\n",
      "Cost : 2.32352\n",
      "Accuracy : 0.00000%\n",
      "3072\n",
      "Cost : 2.28523\n",
      "Accuracy : 0.00000%\n",
      "3084\n",
      "Cost : 2.27177\n",
      "Accuracy : 0.00000%\n",
      "3096\n",
      "Cost : 2.29146\n",
      "Accuracy : 0.00000%\n",
      "3108\n",
      "Cost : 2.29978\n",
      "Accuracy : 0.00000%\n",
      "3120\n",
      "Cost : 2.29972\n",
      "Accuracy : 0.00000%\n",
      "3132\n",
      "Cost : 2.30046\n",
      "Accuracy : 0.00000%\n",
      "3144\n",
      "Cost : 2.33426\n",
      "Accuracy : 0.00000%\n",
      "3156\n",
      "Cost : 2.29626\n",
      "Accuracy : 0.00000%\n",
      "3168\n",
      "Cost : 2.29301\n",
      "Accuracy : 0.00000%\n",
      "3180\n",
      "Cost : 2.30657\n",
      "Accuracy : 0.00000%\n",
      "3192\n",
      "Cost : 2.29757\n",
      "Accuracy : 0.00000%\n",
      "3204\n",
      "Cost : 2.32956\n",
      "Accuracy : 0.00000%\n",
      "3216\n",
      "Cost : 2.30720\n",
      "Accuracy : 0.00000%\n",
      "3228\n",
      "Cost : 2.30884\n",
      "Accuracy : 0.00000%\n",
      "3240\n",
      "Cost : 2.30874\n",
      "Accuracy : 0.00000%\n",
      "3252\n",
      "Cost : 2.26481\n",
      "Accuracy : 0.00000%\n",
      "3264\n",
      "Cost : 2.31191\n",
      "Accuracy : 0.00000%\n",
      "3276\n",
      "Cost : 2.27992\n",
      "Accuracy : 0.00000%\n",
      "3288\n",
      "Cost : 2.30232\n",
      "Accuracy : 0.00000%\n",
      "3300\n",
      "Cost : 2.31311\n",
      "Accuracy : 0.00000%\n",
      "3312\n",
      "Cost : 2.29387\n",
      "Accuracy : 0.00000%\n",
      "3324\n",
      "Cost : 2.30475\n",
      "Accuracy : 0.00000%\n",
      "3336\n",
      "Cost : 2.28851\n",
      "Accuracy : 0.00000%\n",
      "3348\n",
      "Cost : 2.32010\n",
      "Accuracy : 0.00000%\n",
      "3360\n",
      "Cost : 2.30346\n",
      "Accuracy : 0.00000%\n",
      "3372\n",
      "Cost : 2.31792\n",
      "Accuracy : 0.00000%\n",
      "3384\n",
      "Cost : 2.30270\n",
      "Accuracy : 0.00000%\n",
      "3396\n",
      "Cost : 2.31718\n",
      "Accuracy : 0.00000%\n",
      "3408\n",
      "Cost : 2.30294\n",
      "Accuracy : 0.00000%\n",
      "3420\n",
      "Cost : 2.30517\n",
      "Accuracy : 0.00000%\n",
      "3432\n",
      "Cost : 2.31150\n",
      "Accuracy : 0.00000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-74fe5753bead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_ARCHITECTURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.05 stable LR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-6eb285a36e97>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose, callback)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mget_accuracy_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mconv_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_backward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcashe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mdf1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-61d1904879c8>\u001b[0m in \u001b[0;36mfull_backward_propagation\u001b[1;34m(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mconv_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mdconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutionBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_s\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mdconv1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6d7435cddcf8>\u001b[0m in \u001b[0;36mconvolutionBackward\u001b[1;34m(dconv_prev, conv_in, filt, s)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m#each entry of the dconv_prev will try to affect the idxs from which was made of.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mdfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mdconv_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdconv_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#, axis =1) ## AXIS?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdconv_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfilt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, NN_ARCHITECTURE, 2, 0.01, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
